[
    {
        "claim": "The models using BoC outperform models using BoW as well as ASM features.",
        "evidence": "Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table. The models using BoC outperform models using BoW as well as ASM features. [CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most of the features. The models using BoC (Wiki-PubMed-PMC) outperform models using BoW (Mikolov et al., 2013a) in extraction of most of the features.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] OD significantly outperforms OD-parse: We observe that compared to OD-parse, OD is much more accurate.",
        "evidence": "Due to the lack of ground truths on the test set, we only reported the results of the commercial datasets. We can see that OD significantly outperforms OD-parse: We observe that compared to OD-parse, OD is much more accurate.. On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 respectively compared to the scores of 0.01, -0.01 and 0.07 by OD-parse. It is evident that OD is much more accurate than the baseline. We also observe that compared to Jensen-Shannon divergence, the change in results is minimal.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Table 4: Comparison of per-document accuracy (% ) by different systems for top 1, 3 and 5 words of abstractive sentences.",
        "evidence": "Table 4 compares the results of different systems for top 1, 3 and 5 words of abstractive sentences. Our model (NeuralTD) shows the best performance among all of the comparison systems. It outperforms all of the state-of-the-art systems by meaningful margins in terms of ROUGE-1, ROUGE-2 and ROUGE-L. This indicates that our model captures important information contained in the sentences.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "The UnsupEmb baseline performs rather poorly on both POS and SEM tagging.",
        "evidence": "From the table, we see that the unsupemb baseline performs rather poorly on both POS tagging and Sem tagging..",
        "table": "+----+-----------+-------+------------+------------+\n|    | [EMPTY]   |   MFT |   UnsupEmb |   Word2Tag |\n+====+===========+=======+============+============+\n|  0 | POS       | 91.95 |      87.06 |      95.55 |\n+----+-----------+-------+------------+------------+\n|  1 | SEM       | 82    |      81.11 |      91.41 |\n+----+-----------+-------+------------+------------+",
        "label": "supports"
    },
    {
        "claim": "In particular, we see that hate speech and harassment are particularly difficult to detect.",
        "evidence": "The claim-metric for hate speech and harassment shows a significantly lower F1 score than the similar claim-metric for other classes. In particular, we see that sexism and racism are harder to detect than neither. The sexism class is harder than the racism class, even for human annotators. This is not surprising, given the decades-old disparities in hate speech detection. Nevertheless, for the harassment class, the detection of sexism is still better than that of racism. Again, we see that hate speech and harassment are particularly difficult to detect.",
        "table": "+----+--------------------+-----------+-------------+----------+------+\n|    | Dataset            | Class     |   Precision |   Recall |   F1 |\n+====+====================+===========+=============+==========+======+\n|  0 | [ITALIC] W. & H.   | Racism    |        0.73 |     0.79 | 0.76 |\n+----+--------------------+-----------+-------------+----------+------+\n|  1 | [EMPTY]            | Sexism    |        0.69 |     0.73 | 0.71 |\n+----+--------------------+-----------+-------------+----------+------+\n|  2 | [EMPTY]            | Neither   |        0.88 |     0.85 | 0.86 |\n+----+--------------------+-----------+-------------+----------+------+\n|  3 | [ITALIC] W.        | Racism    |        0.56 |     0.77 | 0.65 |\n+----+--------------------+-----------+-------------+----------+------+\n|  4 | [EMPTY]            | Sexism    |        0.62 |     0.73 | 0.67 |\n+----+--------------------+-----------+-------------+----------+------+\n|  5 | [EMPTY]            | R. & S.   |        0.56 |     0.62 | 0.59 |\n+----+--------------------+-----------+-------------+----------+------+\n|  6 | [EMPTY]            | Neither   |        0.95 |     0.92 | 0.94 |\n+----+--------------------+-----------+-------------+----------+------+\n|  7 | [ITALIC] D. et al. | Hate      |        0.32 |     0.53 | 0.4  |\n+----+--------------------+-----------+-------------+----------+------+\n|  8 | [EMPTY]            | Offensive |        0.96 |     0.88 | 0.92 |\n+----+--------------------+-----------+-------------+----------+------+\n|  9 | [EMPTY]            | Neither   |        0.81 |     0.95 | 0.87 |\n+----+--------------------+-----------+-------------+----------+------+\n| 10 | [ITALIC] G. et al. | Harass.   |        0.41 |     0.19 | 0.26 |\n+----+--------------------+-----------+-------------+----------+------+\n| 11 | [EMPTY]            | Non.      |        0.75 |     0.9  | 0.82 |\n+----+--------------------+-----------+-------------+----------+------+\n| 12 | [ITALIC] F. et al. | Hate      |        0.33 |     0.42 | 0.37 |\n+----+--------------------+-----------+-------------+----------+------+\n| 13 | [EMPTY]            | Abusive   |        0.87 |     0.88 | 0.88 |\n+----+--------------------+-----------+-------------+----------+------+\n| 14 | [EMPTY]            | Spam      |        0.5  |     0.7  | 0.58 |\n+----+--------------------+-----------+-------------+----------+------+\n| 15 | [EMPTY]            | Neither   |        0.88 |     0.77 | 0.82 |\n+----+--------------------+-----------+-------------+----------+------+",
        "label": "supports"
    },
    {
        "claim": "The results prove the effectiveness of word-level attention to exploit the local interactions in link prediction task.",
        "evidence": "The experimental results are summarized in Table 3. The results show that the attention mechanism extracted by our model is much better than the one extracted by the -word-att method. [CONTINUE] Word-level attention has outperformed the word-level attention with at least 4.7% improvement in AUC score. This indicates that the attention mechanism extracted by our model can better exploit the local interactions.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "The average number of tokens per tweet is not 22.3, per sentence is not 13.6 and average scope length is not 2.9.",
        "evidence": "From the table, we see that the total number of tokens in the tweets is 22.3, per sentence is not 13.6 and average scope length is not 2.9. The false negation cues in the tweets are 247. Also, the average sentence length is 13.6, which is shorter than the average scope length.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.",
        "evidence": "We now proceed to analyze the impact of the language modeling loss in our model. Comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss significantly reduces PP, sometimes at the cost of semantic preservation. For example, M3 reduces the absolute accuracy from 37.3 to 28.4 compared to M2. Further, between M4 and M6, the reduction is less pronounced, only 0.3 and 0.5 absolute percentage points. Nonetheless, simply adding the language modeling loss from M4 to M6 seems to mitigate the loss in semantic preservation. That is, with the loss in semantic content, we still get a better accuracy, albeit with a decrease in precision. We hypothesize that the reduction in accuracy comes from the loss in semantic content, as the original inputs into the model are all nouns. M6+para further reduces the accuracy, at the cost of semantic preservation.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.818 | 0.719        | 37.3        | 10.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.819 | 0.734        | 26.3        | 14.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.813 | 0.770        | 36.4        | 18.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.807 | 0.796        | 28.4        | 21.5        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.798 | 0.783        | 39.7        | 19.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785        | 27.1        | 20.3        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.805 | [BOLD] 0.817 | 43.3        | 21.6        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.818 | 0.805        | [BOLD] 29.0 | [BOLD] 22.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "2018) or reinforcement learning with additional dataset-specific heuristics (Kryscinski et\\xa0al.",
        "evidence": "The neuralTD model in our model is trained either with reinforcement learning (as in the work of Bahdanau et al. in his paper) or with additional dataset-specific heuristics (as in the work of Kryscinski et al. in his paper). Here, we report the results of using heuristics along with the RL scores. It can be seen that our model trained with our heuristics outperforms all of the others, which indicates that our model is able to combine the strengths of sequence labeling and natural language processing.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 1: In all language pairs, the best correlation is achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans.",
        "evidence": "We find that our approach consistently outperforms other baselines, which is encouraging. Since our main goal in this work is to improve the inter-lingual effectiveness of word-overview approaches, we take one step further by integrating the embeddings from different BERT layers: WMD-1/2+BERT+MNLI+PAGEANS. This configuration performs even better than the vanilla Word-Mover using BERT pretrained weights. We also experiment with combining the pretrained embeddings from Sent-Mover and WMD-1, but there is no improvement over simply using the embeddings from the large-scale language model. Since the direct assessment dataset is very small, we only take into account the inter-lingual results and there is not much room for improvement. We see that our approach consistently outperforms all baselines, which is encouraging. Especially, the improvements on Turkish-English and Chinese-English are much bigger than those on other language pairs.",
        "table": "+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|    | Setting    | Metrics                      | <bold>Direct Assessment</bold> cs-en   | <bold>Direct Assessment</bold> de-en   | <bold>Direct Assessment</bold> fi-en   | <bold>Direct Assessment</bold> lv-en   | <bold>Direct Assessment</bold> ru-en   | <bold>Direct Assessment</bold> tr-en   | <bold>Direct Assessment</bold> zh-en   | <bold>Direct Assessment</bold> Average   |\n+====+============+==============================+========================================+========================================+========================================+========================================+========================================+========================================+========================================+==========================================+\n|  0 | Baselines  | METEOR++                     | 0.552                                  | 0.538                                  | 0.720                                  | 0.563                                  | 0.627                                  | 0.626                                  | 0.646                                  | 0.610                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  1 | Baselines  | RUSE(*)                      | 0.624                                  | 0.644                                  | 0.750                                  | 0.697                                  | 0.673                                  | 0.716                                  | 0.691                                  | 0.685                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  2 | Baselines  | BERTScore-F1                 | 0.670                                  | 0.686                                  | 0.820                                  | 0.710                                  | 0.729                                  | 0.714                                  | 0.704                                  | 0.719                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  3 | Sent-Mover | Smd + W2V                    | 0.438                                  | 0.505                                  | 0.540                                  | 0.442                                  | 0.514                                  | 0.456                                  | 0.494                                  | 0.484                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  4 | Sent-Mover | Smd + ELMO + PMeans          | 0.569                                  | 0.558                                  | 0.732                                  | 0.525                                  | 0.581                                  | 0.620                                  | 0.584                                  | 0.595                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  5 | Sent-Mover | Smd + BERT + PMeans          | 0.607                                  | 0.623                                  | 0.770                                  | 0.639                                  | 0.667                                  | 0.641                                  | 0.619                                  | 0.652                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  6 | Sent-Mover | Smd + BERT + MNLI + PMeans   | 0.616                                  | 0.643                                  | 0.785                                  | 0.660                                  | 0.664                                  | 0.668                                  | 0.633                                  | 0.667                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  7 | Word-Mover | Wmd-1 + W2V                  | 0.392                                  | 0.463                                  | 0.558                                  | 0.463                                  | 0.456                                  | 0.485                                  | 0.481                                  | 0.471                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  8 | Word-Mover | Wmd-1 + ELMO + PMeans        | 0.579                                  | 0.588                                  | 0.753                                  | 0.559                                  | 0.617                                  | 0.679                                  | 0.645                                  | 0.631                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  9 | Word-Mover | Wmd-1 + BERT + PMeans        | 0.662                                  | 0.687                                  | 0.823                                  | 0.714                                  | 0.735                                  | 0.734                                  | 0.719                                  | 0.725                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n| 10 | Word-Mover | Wmd-1 + BERT + MNLI + PMeans | 0.670                                  | 0.708                                  | <bold>0.835</bold>                     | <bold>0.746</bold>                     | <bold>0.738</bold>                     | 0.762                                  | <bold>0.744</bold>                     | <bold>0.743</bold>                       |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n| 11 | Word-Mover | Wmd-2 + BERT + MNLI + PMeans | <bold>0.679</bold>                     | <bold>0.710</bold>                     | 0.832                                  | 0.745                                  | 0.736                                  | <bold>0.763</bold>                     | 0.740                                  | <bold>0.743</bold>                       |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.",
        "evidence": "As shown, our G2S-GAT model significantly outperforms S2S and G2S-GIN by a large margin on all datasets. For example, on Ldc2015e86, G2S-GGNN beats S2S by about 1.8 BLEU points. Although the gap between G2S-GAT and S2S is still large, we believe that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "the model converged and yielded high performance, verifying the efficacy of the implicit answer vector representation for matching word meanings",
        "evidence": "To further verify the effectiveness of the implicit answer vector representation, we ran evaluations on the subset of questions that contain all concepts mentioned in the table, as well as the subset of questions that consist entirely of concept words. We first train and evaluate the model only on the questions that contain at least one concept word, excluding all questions that consist entirely of concept words. We can see that the performance of the proposed model is significantly higher than that of the baselines, confirming the value of the additional cost-sensitive embedding.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Pretraining the HAN models, although intuitively promising, yields only comparable results with those without.",
        "evidence": "We also experiment with combining the HAN models with (pretrainu) the HAN models described in Sec. We observe that this combination performs only slightly better than the HAN models without pretraining. It suggests that (1) pretraining the models, although intuitively promising, does not lead to state-of-the-art results. (2) On the other hand, pretraining the HAN models with (pretrainu) leads to slightly better performance than HAN without pretraining. This suggests that (1) the information from the HAN models is useful; (2) pretraining the models with (pretrainu) leads to better performance than training HAN alone.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is not improved by our initialization strategy.",
        "evidence": "While our model achieves slightly better performance than Glorot on Bshift and TopConst, CMOW's ability to memorize word content is not improved by our initialization strategy. We hypothesize that this may be due to the fact that words in morphologically rich languages, especially adjectives, are often embedded in subword units, which are learned from word embeddings. This argument aligns with the findings by \\newciteMoYu2014:abstract, who observed that the ability to memorize words is improved by morphological initialization in the crucible-of-words model.",
        "table": "+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Initialization   | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+==================+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | N(0,0.1)         | 29.7        | 71.5        | 82.0        | 78.5        | 60.1        | 80.5        | 76.3        | 74.7        | [BOLD] 51.3 | 52.5        |\n+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | Glorot           | 31.3        | [BOLD] 72.3 | 81.8        | 78.7        | 59.4        | 81.3        | 76.6        | [BOLD] 74.6 | 50.4        | 57.0        |\n+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Our paper        | [BOLD] 35.1 | 70.8        | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | [BOLD] 82.8 | [BOLD] 79.7 | 74.2        | 50.7        | [BOLD] 72.9 |\n+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "In Table 5, it can be seen that generative pretraining via language modeling does not account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup.",
        "evidence": "In Table 5, it can be seen that generative pretraining via language modeling does not account for a considerable amount of performance, constituting 44.32% of the overall performance. In contrast, the boost in accuracy from generative pretraining is constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup.",
        "table": "+----+--------------+---------------+------------+-------------+-------------+--------------+\n|    | Finetuning   | Pretrained?   | Accuracy   |   Val. Loss | Acc. Inc.   | % of Perf.   |\n+====+==============+===============+============+=============+=============+==============+\n|  0 | Multitasking | No            | 53.61%     |      0.7217 | -           | -            |\n+----+--------------+---------------+------------+-------------+-------------+--------------+\n|  1 | [EMPTY]      | Yes           | 96.28%     |      0.2197 | +42.67%     | 44.32%       |\n+----+--------------+---------------+------------+-------------+-------------+--------------+\n|  2 | Standard     | No            | 51.02%     |      0.7024 | -           | -            |\n+----+--------------+---------------+------------+-------------+-------------+--------------+\n|  3 | [EMPTY]      | Yes           | 90.99%     |      0.1826 | +39.97%     | 43.93%       |\n+----+--------------+---------------+------------+-------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model.",
        "evidence": "We can see that the use of discriminative training, even without additional monolingual data, leads to better performance than that of the best language model. The accuracies of the Spanish-only-LM, English-only-LM, and all All:Shuffled-LMs were accuracies based on the best language model.",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules.",
        "evidence": "We argue that residual connections are essential to the hierarchical attention as their removal would lead to a performance drop. The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules.",
        "table": "+----+--------------------+---------------------+\n|    | [BOLD] Model       | [BOLD] Joint Acc.   |\n+====+====================+=====================+\n|  0 | COMER              | 88.64%              |\n+----+--------------------+---------------------+\n|  1 | - Hierachical-Attn | 86.69%              |\n+----+--------------------+---------------------+\n|  2 | - MLP              | 83.24%              |\n+----+--------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "This observation concurs with the performance boost for this model across the two datasets and shows that using a more advanced architecture with more parameters results in larger improvements using the coverage mechanism.",
        "evidence": "This observation concurs with the performance boost across the two datasets and shows that using a more advanced architecture with more parameters results in larger improvements using the coverage mechanism.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "for example, for those rewards, models learn the ROUGE reward much better than the full extent of system-level ROUGE correlation as shown in Table 1, which will also increase system-level ROUGE.",
        "evidence": "Table 1 shows the effect of using different encoders for the question. As shown, the model using BERT as encoder has the best performance. Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP,",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Mentions of time are not specific of complaints (been, still, on, days, Temporal References cluster).",
        "evidence": "Top unigrams and part-of-speech features (unigrams) for complaints are: [url] Complaints : User mentions of time are not specific of complaints. For example, the mentions of time are “been” and “been on” in the Complaints dataset. Also, “good” and “lol” are used for not complaints. In addition, other unigrams and part-of-speech features (verbs) are often used when named entities are used, but are not specific of complaints. For example, “goodness” is used to mean that the item has been given a good deal or service. Other unigrams (verbs) for parts-of-speech features are: “no, no, won’t”, “regime change”, “won’t”, “regime change” etc.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value.",
        "evidence": "The results from all the models consistently confirm the information from the human annotations. This is especially true in the case of DAN where we see a large increase when the decoder repeatedly predicts words having a high sentiment value.",
        "table": "+----+---------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]             |   <bold>RNN</bold> |   <bold>CNN</bold> | <bold>DAN</bold>   |\n+====+=====================+====================+====================+====================+\n|  0 | Positive            |                9.7 |                4.3 | +<bold>23.6</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  1 | Negative            |                6.9 |                5.5 | +<bold>16.1</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  2 | Flipped to Positive |               20.2 |               24.9 | +27.4              |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  3 | Flipped to Negative |               31.5 |               28.6 | +19.3              |\n+----+---------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": ", then randomly selected 5 examples were manually examined to understand the sources of errors, which was helpful in identifying issues in cue detection.",
        "evidence": "Here, a randomly selected set of examples were manually examined to understand the sources of errors. This analysis can be helpful in identifying issues in cue detection..",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the utterance in the first premise “The woman went down into the cellar” leads BERT-large to produce “The woman entered the cellar” and to choose the distractor rather than the correct premise.",
        "evidence": "For example, the sequence “The woman went down into the cellar” leads BERT-Large to produce “The woman entered the cellar” rather than the correct premise. As BERT tends to choose the distractor instead of the correct premise, we also observe lower BLEU scores.",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers.",
        "evidence": "As shown in Table. Our model DCGCN(ensemble) achieves the best results for all language pairs and ensembles. For the single-model DCGCN we represent all layers of the recurrent encoder with a single layer. Comparing the performance of the single-model DCGCN with the ensemble model, we observe that the relative improvement is:DCGCN(ensemble) has a much better performance than the single-model GCN; DCGCN(ensemble) performs slightly better than GGN2seqB in all cases; and there is no significant performance degradation observed if models use recurrent encoders.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "AME performs better than FME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training.",
        "evidence": "We study the performance of AME model compared to FME model on both symmetric and asymmetric modes. For the symmetric mode, we measure the similarity between the predicted and source word by averaging the word embeddings in all cases. We also show the results of models that finetune the word embeddings during training, and compare to the results of AME model. We can see that AME performs 4.2% better than FME model in the symmetric mode. In the asymmetric mode, we achieve 3.5% better performance than FME model. This clearly shows the advantage of finetuning word embeddings during training, especially for more symmetric modes.",
        "table": "+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|    | [EMPTY]              | Image to Text R@1   | Image to Text R@5   | Image to Text R@10   | Image to Text Mr   | Text to Image R@1   | Text to Image R@5   | Text to Image R@10   | Text to Image Mr   | Alignment   |\n+====+======================+=====================+=====================+======================+====================+=====================+=====================+======================+====================+=============+\n|  0 | [BOLD] symmetric     | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  1 | Parallel gella:17    | 31.7                | 62.4                | 74.1                 | 3                  | 24.7                | 53.9                | 65.7                 | 5                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  2 | UVS kiros:15         | 23.0                | 50.7                | 62.9                 | 5                  | 16.8                | 42.0                | 56.5                 | 8                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  3 | EmbeddingNet wang:18 | 40.7                | 69.7                | 79.2                 | -                  | 29.2                | 59.6                | 71.7                 | -                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  4 | sm-LSTM huang:17     | 42.5                | 71.9                | 81.5                 | 2                  | 30.2                | 60.4                | 72.3                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  5 | VSE++ faghri:18      | [BOLD] 43.7         | 71.9                | 82.1                 | 2                  | 32.3                | 60.9                | 72.1                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  6 | Mono                 | 41.4                | 74.2                | 84.2                 | 2                  | 32.1                | 63.0                | 73.9                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  7 | FME                  | 39.2                | 71.1                | 82.1                 | 2                  | 29.7                | 62.5                | 74.1                 | 3                  | 76.81%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  8 | AME                  | 43.5                | [BOLD] 77.2         | [BOLD] 85.3          | [BOLD] 2           | [BOLD] 34.0         | [BOLD] 64.2         | [BOLD] 75.4          | [BOLD] 3           | 66.91%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  9 | [BOLD] asymmetric    | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 10 | Pivot gella:17       | 33.8                | 62.8                | 75.2                 | 3                  | 26.2                | 56.4                | 68.4                 | 4                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 11 | Parallel gella:17    | 31.5                | 61.4                | 74.7                 | 3                  | 27.1                | 56.2                | 66.9                 | 4                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 12 | Mono                 | 47.7                | 77.1                | 86.9                 | 2                  | 35.8                | 66.6                | 76.8                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 13 | FME                  | 44.9                | 76.9                | 86.4                 | 2                  | 34.2                | 66.1                | 77.1                 | 3                  | 76.81%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 14 | AME                  | [BOLD] 50.5         | [BOLD] 79.7         | [BOLD] 88.4          | [BOLD] 1           | [BOLD] 38.0         | [BOLD] 68.5         | [BOLD] 78.4          | [BOLD] 2           | 73.10%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "On the contrary, we found the quality of 3-step NLDs is relatively higher than the others.",
        "evidence": "Reachability is the evaluation metric for NLDs. From the results we found that 3-step NLDs have relatively high reachability. On the other hand, we found that derivability does not correlate well with step-wise ranking.",
        "table": "+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|    |   # steps |   Reachability |   Derivability Step 1 | Derivability Step 2   | Derivability Step 3   |\n+====+===========+================+=======================+=======================+=======================+\n|  0 |         1 |            3   |                   3.8 | -                     | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  1 |         2 |            2.8 |                   3.8 | 3.7                   | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  2 |         3 |            2.3 |                   3.9 | 3.8                   | 3.8                   |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our joint model does not outperform all the base [CONTINUE] The results do not reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016).",
        "evidence": "We can see that, for CD event coreference resolution, our joint model is on par or better than all the base models.",
        "table": "+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>                                      | R       | MUC P   | <italic>F</italic>1   | B3 P    | CEAF-<italic>e</italic> P   | CoNLL <italic>F</italic>1   |\n+====+=========================================================+=========+=========+=======================+=========+=============================+=============================+\n|  0 | <bold>Baselines</bold>                                  | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  1 | Cluster+Lemma                                           | 75.5    | 79.9    | 73.6                  | 85      | 71.7                        | 76.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  2 | CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | -       | 75      | 64                    | 78      | -                           | 73                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  3 | KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 71      | 71      | 69                    | 67      | 67                          | 69                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  4 | Cluster+KCP                                             | 77.4    | 79.3    | 71.5                  | 87.2    | 66.4                        | 73.6                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  5 | <bold>Model Variants</bold>                             | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  6 | Disjoint                                                | 80.3    | 83.6    | 75.9                  | 86      | 71.9                        | 78.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  7 | Joint                                                   | 81      | 84.5    | 77.3                  | 85.1    | 73.8                        | <bold>79.5</bold>           |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU loss over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU loss over the approach described in Freitag and Al-Onaizan (2016).",
        "evidence": "This claim makes use of the fact that the EWC-adapted models give better results than the strong uniform EWC ensemble, and is a strong baseline in our experiments.",
        "table": "+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|    | [BOLD] Language pair   | [BOLD] Model type   |   [BOLD] Oracle model |   [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS   |\n+====+========================+=====================+=======================+================================================+================================================+\n|  0 | es-en                  | Unadapted           |                  36.4 |                                           34.7 | 36.6                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  1 | es-en                  | No-reg              |                  36.6 |                                           34.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  2 | es-en                  | EWC                 |                  37   |                                           36.3 | [BOLD] 37.2                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  3 | en-de                  | Unadapted           |                  36.4 |                                           26.8 | 38.8                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  4 | en-de                  | No-reg              |                  41.7 |                                           31.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  5 | en-de                  | EWC                 |                  42.1 |                                           38.6 | [BOLD] 42.0                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "More importantly, their G-Pre and G-Rec scores are all above .50, which means that more than half of the good summaries identified by the metrics are actually good, and more than 50%.",
        "evidence": "From the table, we can see that more than half of the good summaries identified by the metrics are actually good. This is very encouraging news for summarization in general. Moreover, the G-Pre and G-Rec scores are all above .50, which means that more than 50% of the good system summaries identified by the metrics are actually good.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "One interpretation for this difference is that under the simulated conversations with random reward function, GP-MBCM does not align well with the different human users.",
        "evidence": "It can be observed that GDPL performs better than ACER and PPO, but worse than ALDM. One possible reason is that under the random reward function, the GP-MBM does not align well with the different human users.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "Again, when ROUGE is used as rewards, the generated summaries have higher ROUGE scores.",
        "evidence": "We can see that using ROUGE as rewards consistently boosts the summarization quality. The ROUGE scores of the summaries generated by our model are significantly higher than the ROUGE scores of the baselines, indicating that our model learns to generate summaries that have higher quality in general. We also compared the generated summaries with the human-written summaries in Table. We found that the human-written summaries have slightly higher ROUGE scores. This indicates that our model is able to distinguish more concretely between the claim and evidence.",
        "table": "+----+----------------+-------+-------+-------+-------------+-----------+\n|    | Reward         |   R-1 |   R-2 |   R-L | Human       | Pref%     |\n+====+================+=======+=======+=======+=============+===========+\n|  0 | R-L (original) |  40.9 |  17.8 |  38.5 | 1.75        | 15        |\n+----+----------------+-------+-------+-------+-------------+-----------+\n|  1 | Learned (ours) |  39.2 |  17.4 |  37.5 | [BOLD] 2.20 | [BOLD] 75 |\n+----+----------------+-------+-------+-------+-------------+-----------+",
        "label": "supports"
    },
    {
        "claim": "This suggests that graph encoders based on gating mechanisms are very effective in text generation models.",
        "evidence": "As shown, the proposed G2S-GAT model significantly outperforms S2S and G2S-GIN by a large margin in all datasets. In particular, we observe that the G2S-GGNN model achieves an improvement of up to 11.5 BLEU score over the best model of Guo et al. As an example, in the setup of DC2015E86, the G2S-GGNN model achieves a BLEU score of 27.87 and an improvement of 3.6 Meteor points over the best performing S2S model. This result shows that the graph encoders based on gating mechanisms are very effective in text generation models..",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "the models more often fail to realise part of the MR, rather than hallucinating additional information.",
        "evidence": "The claim can be seen as follows. (1) Using information generated by our model (TGen+)) improves the performance of the original model by around +2.3 BLEU and +1.80 ROUGE-L on the test set. This comes with the significant improvement of around +10% in SER scores. (2) While adding information from models that have been separately trained on the source and target data (TGen−) performs worse than using them together (TGen+), the effect is better than models that have been trained on the source data only. This is not surprising as models that have been trained on source data rather than target data need to re-rank the informativeness of their representations. (3) Lastly, models that were trained on the target data, but not on the source data, like SC-LSTM, still perform worse on almost all metrics. This seems to indicate that the type of target information that is not present in the source data is important for the models to make correct predictions.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "These results show significant performance improvement by using Predicate Schemas knowledge on hard coreference problems.",
        "evidence": "Performance on hard coreference problems. On the WinoCoref dataset, we can see that adding predicate schemas knowledge helps to improve the results. Especially, the improvement is much larger on the KnowComb and KnowComb2 problems, which shows the benefits of structural knowledge on hard coreference resolution.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "This is expected, since the questions in the SQuAD and QA-SRL datasets tend to be very different (more declarative in the former, more interrogative in the latter).",
        "evidence": "The results show that our proposed approach consistently improves the performance over the baseline model for both the in-domain and out-of-domain evaluations. The improvement is larger on the in-domain dataset, which confirms the hypothesis that the SQuAD dataset is more difficult than the QA-SRL datasets. Also, the questions in the SQuAD and QA-SRL datasets are very different, with the SQuAD dataset being more declarative and the QA-SRL datasets being more interrogative.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE.",
        "evidence": "The results show that the proposed AAS method with weights wAC=1 and WAD=105 shows the lowest WER and DCE.",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 17.3        | 0.828        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 19.5        | 0.722        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 15.8        | [BOLD] 0.269 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 14.9        | 0.291        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 15.6        | 0.330        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 5.7         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "The most representative models are ELMO, GPT, BERT and its variants, and XLNET.",
        "evidence": "The most representative models are ELMO, GPT, BERT and its variants, and XLNet. RoBERTa and ALBERT are less common compared to the other models. This is not surprising, given that these models take advantage of pretraining and fine-tuning.",
        "table": "+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|    | [BOLD] Benchmark   | [BOLD]  Simple Baseline    | [BOLD] ELMo   | [BOLD] GPT   | [BOLD] BERT   | [BOLD] MT-DNN   | [BOLD] XLNet   | [BOLD] RoBERTa   | [BOLD] ALBERT   | [BOLD] Human   |\n+====+====================+============================+===============+==============+===============+=================+================+==================+=================+================+\n|  0 | [BOLD] CLOTH       | 25.0                       | 70.7          | –            | [BOLD] 86.0   | –               | –              | –                | –               | 85.9           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  1 | [BOLD] Cosmos QA   | –                          | –             | 54.5         | 67.1          | –               | –              | –                | –               | 94.0           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  2 | [BOLD] DREAM       | 33.4                       | 59.5          | 55.5         | 66.8          | –               | [BOLD] 72.0    | –                | –               | 95.5           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  3 | [BOLD] GLUE        | –                          | 70.0          | –            | 80.5          | 87.6            | 88.4           | 88.5             | [BOLD] 89.4     | 87.1           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  4 | [BOLD] HellaSWAG   | 25.0                       | 33.3          | 41.7         | 47.3          | –               | –              | [BOLD] 85.2      | [EMPTY]         | 95.6           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  5 | [BOLD] MC-TACO     | 17.4                       | 26.4          | –            | 42.7          | –               | –              | [BOLD] 43.6      | –               | 75.8           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  6 | [BOLD] RACE        | 24.9                       | –             | 59.0         | 72.0          | –               | 81.8           | 83.2             | [BOLD] 89.4     | 94.5           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  7 | [BOLD] SciTail     | 60.3                       | –             | 88.3         | –             | 94.1            | –              | –                | –               | –              |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  8 | [BOLD] SQuAD 1.1   | 1.3                        | 81.0          | –            | 87.4          | –               | [BOLD] 89.9    | –                | –               | 82.3           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  9 | [BOLD] SQuAD 2.0   | 48.9                       | 63.4          | –            | 80.8          | –               | 86.3           | 86.8             | [BOLD] 89.7     | 86.9           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n| 10 | [BOLD] SuperGLUE   | 47.1                       | –             | –            | 69.0          | –               | –              | [BOLD] 84.6      | –               | 89.8           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n| 11 | [BOLD] SWAG        | 25.0                       | 59.1          | 78.0         | 86.3          | 87.1            | –              | [BOLD] 89.9      | –               | 88.0           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "In general, increasing the number of GCN layers from 2 to 9 boosts the model performance.",
        "evidence": "We can see that increasing the number of GCN layers from 2 to 9 consistently improves the model performance. We can see that, for the GCN model, increasing the number of layers from 4 to 6 improves the results. However, increasing the number of layers further from 9 to 10 does not improve the results over the baseline model.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model.",
        "evidence": "Pre-clustering documents to topics is not effective. The results of CLUSTER+KCP indicate that pre-clustering documents to topics is not beneficial, performing substantially worse than our joint model.. Our joint model still outperforms all the base [CONTINUE] The results show that pre-clustering of documents to topics is not effective.",
        "table": "+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>                                      | R       | MUC P   | <italic>F</italic>1   | B3 P    | CEAF-<italic>e</italic> P   | CoNLL <italic>F</italic>1   |\n+====+=========================================================+=========+=========+=======================+=========+=============================+=============================+\n|  0 | <bold>Baselines</bold>                                  | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  1 | Cluster+Lemma                                           | 75.5    | 79.9    | 73.6                  | 85      | 71.7                        | 76.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  2 | CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | -       | 75      | 64                    | 78      | -                           | 73                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  3 | KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 71      | 71      | 69                    | 67      | 67                          | 69                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  4 | Cluster+KCP                                             | 77.4    | 79.3    | 71.5                  | 87.2    | 66.4                        | 73.6                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  5 | <bold>Model Variants</bold>                             | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  6 | Disjoint                                                | 80.3    | 83.6    | 75.9                  | 86      | 71.9                        | 78.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  7 | Joint                                                   | 81      | 84.5    | 77.3                  | 85.1    | 73.8                        | <bold>79.5</bold>           |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average.",
        "evidence": "The claim here is that text-only fusion is better than the raw baseline in all cases. We also observe that early fusion can benefit from innovations. In the case of early fusion, text + raw was the model with the best performance. By contrast, the late fusion model had the best performance. This fits with our hypothesis that the performance of the early fusion model is negatively correlated with the number of innovations.",
        "table": "+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|    | [EMPTY]   | [BOLD] Model             | [BOLD] dev mean   | [BOLD] dev best   | [BOLD] test mean   | [BOLD] test best   | [ITALIC] α   |\n+====+===========+==========================+===================+===================+====================+====================+==============+\n|  0 | single    | text                     | 86.54             | 86.80             | 86.47              | 86.96              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  1 | single    | raw                      | 35.00             | 37.33             | 35.78              | 37.70              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  2 | single    | innovations              | 80.86             | 81.51             | 80.28              | 82.15              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  3 | early     | text + raw               | 86.46             | 86.65             | 86.24              | 86.53              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  4 | early     | text + innovations       | 86.53             | 86.77             | 86.54              | 87.00              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  5 | early     | text + raw + innovations | 86.35             | 86.69             | 86.55              | 86.44              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  6 | late      | text + raw               | 86.71             | 87.05             | 86.35              | 86.71              | 0.2          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  7 | late      | text + innovations       | [BOLD] 86.98      | [BOLD] 87.48      | [BOLD] 86.68       | [BOLD] 87.02       | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  8 | late      | text + raw + innovations | 86.95             | 87.30             | 86.60              | 86.87              | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\").",
        "evidence": "We observe that as the object detectors pretrained on Pascal-voc are evaluated on Flickr30k, they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-voc – namely, \"animals\", \"people\" and \"vehicles\".",
        "table": "+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|    | Method               | Overall      | people       | clothing     | bodyparts    | animals      | vehicles     | instruments   | scene        | other        |\n+====+======================+==============+==============+==============+==============+==============+==============+===============+==============+==============+\n|  0 | QRC - VGG(det)       | 60.21        | 75.08        | 55.9         | 20.27        | 73.36        | 68.95        | 45.68         | 65.27        | 38.8         |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  1 | CITE - VGG(det)      | 61.89        | [BOLD] 75.95 | 58.50        | 30.78        | [BOLD] 77.03 | [BOLD] 79.25 | 48.15         | 58.78        | 43.24        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  2 | ZSGNet - VGG (cls)   | 60.12        | 72.52        | 60.57        | 38.51        | 63.61        | 64.47        | 49.59         | 64.66        | 41.09        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  3 | ZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87        | [BOLD] 66.18 | [BOLD] 45.27 | 73.79        | 71.38        | [BOLD] 58.54  | [BOLD] 66.49 | [BOLD] 45.53 |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Coverage helps the model improve its EM by 1.5 and its F1 by 0.5.",
        "evidence": "Table 5 shows that [CONTINUE] The coverage helps the model improve its EM by 1.5 and F1 by 0.5 on the in-domain SQuAD and the out-of-domain QA-SRL dataset.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "models with NSP performance drop a lot when trained with COPA.",
        "evidence": "The NSP performance drops a lot when trained with COPA for both BERT-large and RoBERTa-large. Moreover, the BLEU score of BERT-large on B-copa is slightly better than BERT-base-NSP, but BERT-large-NSP performs on par with RoBERTa-large on the hard subset.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The hybrid model is able to repair this deficit, reducing the difference to 8%.",
        "evidence": "The result shows that the hybrid model is able to repair the deficit with an additional gain of 8% over the CBOW model.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | STS12       | STS13       | STS14       | STS15       | STS16       |\n+====+===========+=============+=============+=============+=============+=============+\n|  0 | CBOW      | 43.5        | [BOLD] 50.0 | [BOLD] 57.7 | [BOLD] 63.2 | 61.0        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW      | 39.2        | 31.9        | 38.7        | 49.7        | 52.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 49.6 | 46.0        | 55.1        | 62.4        | [BOLD] 62.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +14.6%      | -8%         | -4.5%       | -1.5%       | +1.8%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +26.5%      | +44.2%      | +42.4       | +25.6%      | +19.0%      |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "our framework captures more information about the intended semantic feature.",
        "evidence": "In this section, we analyze the contribution of each component of our framework towards the prediction of the intended semantic feature. To this end, we visualise what happens when a pair of embeddings are concatenated. We can see that, as expected, the direct concatenation of the semantic feature and the embeddings generated by our method are more indicative of the intended semantic feature than the direct concatenation of the semantic feature and the embeddings generated by the baseline. We have two additional observations. First, looking at the mean and standard deviation of the embeddings, we can see that our method indeed captures more information about the intended semantic feature than the baseline. Second, the difference between the mean and the standard deviation of the proposed method seems to be related to the number of participants.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We can see that the dual attention model does not work at all and the scores slightly drop.",
        "evidence": "From this table, we can see that the dual attention model does not work at all. The scores slightly drop, especially for the translation from English to German and the translation from French to English. It seems that the model is not compatible with the attention disambiguation and generates irrelevant or misleading captions.",
        "table": "+----+-----------------------+-------------+-------------+-------------+\n|    | en-fr                 | flickr16    | flickr17    | mscoco17    |\n+====+=======================+=============+=============+=============+\n|  0 | multi30k              | 61.4        | 54.0        | 43.1        |\n+----+-----------------------+-------------+-------------+-------------+\n|  1 | +autocap (dual attn.) | 60.9        | 52.9        | 43.3        |\n+----+-----------------------+-------------+-------------+-------------+\n|  2 | +autocap 1 (concat)   | 61.7        | 53.7        | 43.9        |\n+----+-----------------------+-------------+-------------+-------------+\n|  3 | +autocap 1-5 (concat) | [BOLD] 62.2 | [BOLD] 54.4 | [BOLD] 44.1 |\n+----+-----------------------+-------------+-------------+-------------+\n|  4 | en-de                 | flickr16    | flickr17    | mscoco17    |\n+----+-----------------------+-------------+-------------+-------------+\n|  5 | multi30k              | 38.9        | 32.0        | 27.7        |\n+----+-----------------------+-------------+-------------+-------------+\n|  6 | +autocap (dual attn.) | 37.8        | 30.2        | 27.0        |\n+----+-----------------------+-------------+-------------+-------------+\n|  7 | +autocap 1 (concat)   | 39.7        | [BOLD] 32.2 | [BOLD] 28.8 |\n+----+-----------------------+-------------+-------------+-------------+\n|  8 | +autocap 1-5 (concat) | [BOLD] 39.9 | 32.0        | 28.7        |\n+----+-----------------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "in terms of correctness, the averaged Ok rate on all 15 decisions is 44.3%",
        "evidence": "The results are shown in Table 3. [CONTINUE] Moreover, we can see that our model [DAMD] outperforms HDSA in the correctness on all the three categories. [CONTINUE] HDSA has better diversity than DAMD, but the results are slightly worse than DAMD in the correctness on good categories.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "The results in Table 7 show that the proposed method is not as effective as the state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction.",
        "evidence": "The results show that the proposed method is not as effective as the state of the art BiLSTM model from Fernandes et al. In particular, the accuracy of “in-scope” and “out-ofscope” cues is significantly lower than those of the BiLSTM model.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "SciBERT does not significantly boost performance for scientific datasets including SciERC and GENIA.",
        "evidence": "This is not surprising. SciBERT does not provide any boost performance for scientific datasets including SciERC and Genia.",
        "table": "+----+--------------+-----------------+-------------------+----------------+\n|    | [EMPTY]      | SciERC Entity   | SciERC Relation   | GENIA Entity   |\n+====+==============+=================+===================+================+\n|  0 | Best BERT    | 69.8            | 41.9              | 78.4           |\n+----+--------------+-----------------+-------------------+----------------+\n|  1 | Best SciBERT | [BOLD] 72.0     | [BOLD] 45.3       | [BOLD] 79.5    |\n+----+--------------+-----------------+-------------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "Our single model is not comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs.",
        "evidence": "As we can see, our DCGCN model achieves the best results in all cases, which are very close to the state-of-the-art results of Seq2SeqB and GGN2Seq. In particular, we outperform the best ensemble result of GGN2Seq by about 1.6 model points. Our model is remarkable because the number of parameters of our models is much less than other baselines. The ensemble results of DCGCN are always better than the Seq2Seq ensemble results, which demonstrates the effectiveness of the our model.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf.",
        "evidence": "The results for testing on the original and cleaned data (Table 3) confirm the positive impact of cleaning on the development data. However, the results on the test data after being trained on the cleaned data show that the tested data is more challenging than the original data.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "If we check the relative ranks of the good summaries according to the metrics (row 1), for example for ROUGE-SU4, we see that 98.4% of them belong to the top 25% summaries in the metric.",
        "evidence": "From the table, we can see that more than 98.4% of the good summaries are among the top 25% summaries in the metrics.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "we observe that MQAN (RAE-based) suffers most without coverage: in all out-of-domain settings it underperforms the original.",
        "evidence": "Table. In all out-of-domain settings, our model under-performs the original MQAN. The gaps are particularly large on the SNLI and Glockner datasets, where MQAN suffers the most. [CONTINUE] Table.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "( 2019).",
        "evidence": "The RoBERTa-large model outperforms all existing large-scale pre-trained models, without having access to any syntactic or semantic information. To test whether syntactic information can be extracted from BERT without finetuning, we finetune the RoBERTa-large model on the task specific training data. The results show that this approach is highly competitive with state-of-the-art finetuning approaches.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "We notice no significant improvements relative to the baseline showing that self-attention alone does not improve the VQA task.",
        "evidence": "We trained the ResNet-34 English captioning model on the VQA dataset Anderson et al. using self-attention with S: 1,2,3 - B:1, showing that self-attention alone does not improve the performance on this task. We also tried adding BERT features to the baseline, but we notice no significant improvements.",
        "table": "+----+------------------------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34                         | [BOLD] Eval set %   | [BOLD] #param   |\n+====+==========================================+=====================+=================+\n|  0 | Baseline (No SA)Anderson et al. ( 2018 ) | 55.00               | 0M              |\n+----+------------------------------------------+---------------------+-----------------+\n|  1 | SA (S: 1,2,3 - B: 1)                     | 55.11               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  2 | SA (S: 1,2,3 - B: 2)                     | 55.17               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  3 | [BOLD] SA (S: 1,2,3 - B: 3)              | [BOLD] 55.27        | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "Our DKRN agent outperforms all other agents with a large margin.",
        "evidence": "Our DKRN agent outperforms all other agents by a large margin.. Our DKRN agent achieves an improvement of 28% in Success rate, 88% in Smoothness and 3.22% in Scorchedness compared to the best performing baseline.",
        "table": "+----+----------------+-------------+--------------+\n|    | System         | Succ. (%)   | Smoothness   |\n+====+================+=============+==============+\n|  0 | Retrieval-Stgy | 54.0        | 2.48         |\n+----+----------------+-------------+--------------+\n|  1 | PMI            | 46.0        | 2.56         |\n+----+----------------+-------------+--------------+\n|  2 | Neural         | 36.0        | 2.50         |\n+----+----------------+-------------+--------------+\n|  3 | Kernel         | 58.0        | 2.48         |\n+----+----------------+-------------+--------------+\n|  4 | DKRN (ours)    | [BOLD] 88.0 | [BOLD] 3.22  |\n+----+----------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.8% F1 score improvement (A2−A1).",
        "evidence": "We performed the experiment on the baseline BiLSTM-CNN model. We can see the difference between (a1) and (a2) can be explained by the dependency distance-based weight factor in the attention mechanism. Without the dependency distance-based weight factor, we only get 0.8% F1 score improvement (A2−A1). It is reasonable because the dependency distance-based weight factor can penalize the distance estimator when the model predicts the missing value in the attention mechanism. Without the dependency distance-based weight factor, we also can not get 0.8% F1 score improvement (A2−A1) because the model will only predict the missing value once.",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "supports"
    },
    {
        "claim": "These results show no significant performance improvement by using Predicate Schemas knowledge on hard coreference problems.",
        "evidence": "Performance on hard coreference problems. First, we can see that adding predicate schemas knowledge only improves the results on the Winograd dataset. This is mainly because the predicate schemas knowledge provides a good initialization for the sequence labeling model. Second, on the Antepre dataset, adding predicate schemas knowledge only slightly improves the performance. This is mainly because predicate schemas knowledge can provide additional useful information for solving the hard coreference problems. Third, on the KnowComb dataset, adding predicate schemas knowledge only slightly improves the performance.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Also, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement.",
        "evidence": "We clearly observe that, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement.",
        "table": "+----+---------------+-----------------+------------------+\n|    | Schema        |   AntePre(Test) |   AntePre(Train) |\n+====+===============+=================+==================+\n|  0 | Type 1        |           76.67 |            86.79 |\n+----+---------------+-----------------+------------------+\n|  1 | Type 2        |           79.55 |            88.86 |\n+----+---------------+-----------------+------------------+\n|  2 | Type 1 (Cat1) |           90.26 |            93.64 |\n+----+---------------+-----------------+------------------+\n|  3 | Type 2 (Cat2) |           83.38 |            92.49 |\n+----+---------------+-----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "In contrast, models in the lower portion (7-12) involve dialogue states, which is estimated using Belief Tracker, or in other words, by models in the upper portion.",
        "evidence": "Results for models in upper portion of table can be seen in Table 7. [CONTINUE] The first group shows that, with the use of domain-adaptive delexcalization and domain-aware belief tracking, the task completion ability of seq2seq models becomes better. [CONTINUE] The relative lower BLEU score [CONTINUE] Our DAMD model significantly outperforms other models with different system action forms in terms of inform and success rates, [CONTINUE] Moreover, even without domain-adaptive delexcalization, the task completion ability of our DAMD model even further improves (p<0.01) than other models.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space.",
        "evidence": "We used the chance and mean MFCC scores from the audio2vec models as our baselines. We see that Audio2vec performs better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space. We use cosine similarity to measure the linearity between audio2vec and the visual space. We see that audio2vec performs better than chance and mean MFCC on paraphrase retrieval, but this does not correlate with the visual space.",
        "table": "+----+-------------+-----------------+---------------+------------+\n|    | [EMPTY]     | Recall@10 (%)   | Median rank   | RSAimage   |\n+====+=============+=================+===============+============+\n|  0 | VGS         | 27              | 6             | 0.4        |\n+----+-------------+-----------------+---------------+------------+\n|  1 | SegMatch    | [BOLD] 10       | [BOLD] 37     | [BOLD] 0.5 |\n+----+-------------+-----------------+---------------+------------+\n|  2 | Audio2vec-U | 5               | 105           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  3 | Audio2vec-C | 2               | 647           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  4 | Mean MFCC   | 1               | 1,414         | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  5 | Chance      | 0               | 3,955         | 0.0        |\n+----+-------------+-----------------+---------------+------------+",
        "label": "supports"
    },
    {
        "claim": "Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model).",
        "evidence": "We also compare our model with PG-original, MMR (which is the pre-trained PG-original model applied to top of the model) and PG-MMR (which is the pre-trained PG-original and applies MMR on top of the model). In the table, we can see the pre-trained PG-original has much worse ROUGE scores than the other baselines. We also see that PG-MMR has better performance than PG-original, which implies our model can leverage the pre-trained PG-original better.",
        "table": "+----+------------------------------------------+--------------+--------------+---------------+\n|    | [BOLD] Method                            | [BOLD] R-1   | [BOLD] R-2   | [BOLD] R-SU   |\n+====+==========================================+==============+==============+===============+\n|  0 | First-1                                  | 26.83        | 7.25         | 6.46          |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  1 | First-2                                  | 35.99        | 10.17        | 12.06         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  2 | First-3                                  | 39.41        | 11.77        | 14.51         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  3 | LexRank Erkan and Radev ( 2004 )         | 38.27        | 12.70        | 13.20         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  4 | TextRank Mihalcea and Tarau ( 2004 )     | 38.44        | 13.10        | 13.50         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  5 | MMR Carbonell and Goldstein ( 1998 )     | 38.77        | 11.98        | 12.91         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  6 | PG-Original Lebanoff et al. ( 2018 )     | 41.85        | 12.91        | 16.46         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  7 | PG-MMR Lebanoff et al. ( 2018 )          | 40.55        | 12.36        | 15.87         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  8 | PG-BRNN Gehrmann et al. ( 2018 )         | 42.80        | 14.19        | 16.75         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  9 | CopyTransformer Gehrmann et al. ( 2018 ) | [BOLD] 43.57 | 14.03        | 17.37         |\n+----+------------------------------------------+--------------+--------------+---------------+\n| 10 | Hi-MAP (Our Model)                       | 43.47        | [BOLD] 14.89 | [BOLD] 17.41  |\n+----+------------------------------------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "their informative and match scores are higher than ours since they prioritize the dialog turn to show referents, while we take into account various factors in dialog quality.",
        "evidence": "They prioritize the dialog turn to show referents and achievements higher than ours since they take into account various factors in dialog quality. We also noticed that the full response is significantly more informative than the other two, while other responses are relatively low.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "As a result, the recursive approach performs better than the folding technique for the training task.",
        "evidence": "Table. As a result, the recursive approach performs better than the folding technique for the training task..",
        "table": "+----+--------------+--------------------------------------+-------------------------------------+\n|    | Batch size   | Throughput (instances/s) Inference   | Throughput (instances/s) Training   |\n+====+==============+======================================+=====================================+\n|  0 | Batch size   | Fold                                 | Fold                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  1 | 1            | 16.5                                 | 9.0                                 |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  2 | 10           | 52.2                                 | 37.5                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  3 | 25           | 61.6                                 | 54.7                                |\n+----+--------------+--------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The evaluation results shown in Table 2 indicate that the annotated NLDs are of high quality (Reachability), and each NLD is properly derived from supporting documents (Derivability).",
        "evidence": "The evaluation results shown in Table 2 indicate that the annotated NLDs are of high quality (Reachability), and each NLD is properly derived from supporting documents (Derivability). [CONTINUE] We find that for all three steps, the NLDs are of high quality. The evaluation results show that the annotated NLDs are of high quality (Reachability) and each NLD is properly derived from supporting documents (Derivability).",
        "table": "+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|    |   # steps |   Reachability |   Derivability Step 1 | Derivability Step 2   | Derivability Step 3   |\n+====+===========+================+=======================+=======================+=======================+\n|  0 |         1 |            3   |                   3.8 | -                     | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  1 |         2 |            2.8 |                   3.8 | 3.7                   | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  2 |         3 |            2.3 |                   3.9 | 3.8                   | 3.8                   |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English.",
        "evidence": "Similar to the STS results, TF and DF achieved almost the same values of precision, recall and F-measure when using the English corpora.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Table 5 summarizes the above experimental results on the affected domain in terms of the number of dialog turns, and the numbers of inform, match, and success actions.",
        "evidence": "We can see that GDPL achieves the best performance in terms of all the three metrics. GDPL even outperforms human in completing 5 out of 7 dialogs. By comparing the results of GDPL with GDPL-sess, we can see that GDPL is able to learn a better task-specified goal distribution. For the task success, GDPL is comparable with ACER and PPO. In general, GDPL is capable of providing more task-specific and goal-oriented information than the other methods.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our model obtains the best performance on three out of the four datasets.",
        "evidence": "Our model obtains the best performance on three out of the four datasets.",
        "table": "+----+-----------+--------------+--------------+--------------+-------------+\n|    | Model     | Inspec       | Krapivin     | NUS          | KP20k       |\n+====+===========+==============+==============+==============+=============+\n|  0 | Catseq    | 0.87803      | 0.781        | 0.82118      | 0.804       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  1 | Catseq-RL | 0.8602       | [BOLD] 0.786 | 0.83         | 0.809       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  2 | GAN       | [BOLD] 0.891 | 0.771        | [BOLD] 0.853 | [BOLD] 0.85 |\n+----+-----------+--------------+--------------+--------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA).",
        "evidence": "We first discuss the effect of layer aggregations and residual connections on the performance of our proposed models. The multi-layer GCNs with residual connections (Gcn+RC) consistently outperform their single-layer counterparts. This is not surprising as layer aggregations give a better ability of aggregating multiple granularities of information in the GCN layers. Moreover, we can see that the improvement of the GCN model over the baseline model (Gcn+RC) is greater than that of the multi-layer model with residual connections (Gcn+RC+LA). This indicates that the residual connections and layer aggregations are complementary to each other and that layer aggregations are helpful in aggregating information from different layers.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The results in Table 4 confirm the findings of the automatic [CONTINUE] metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other.",
        "evidence": "We started by training on the original, cleaned and added sets, and then ran an experiment to investigate whether the resulting systems had any knowledge of about the missing slots. The results in Table 4 show that both the systems trained on the fully-cleased set and the one trained on the set with cleaned missing slots have nearperfect performance. The exception was the system trained on the missing slots, which showed a few more slight disfluencies than the other.",
        "table": "+----+------------------------+--------------+---------------+----------------+----------------+\n|    | [BOLD] Training data   |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] Disfl |\n+====+========================+==============+===============+================+================+\n|  0 | Original               |            0 |            22 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  1 | Cleaned added          |            0 |            23 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  2 | Cleaned missing        |            0 |             1 |              0 |              2 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  3 | Cleaned                |            0 |             0 |              0 |              5 |\n+----+------------------------+--------------+---------------+----------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "We see clear benefits of the coverage mechanism in the out-of-domain setting, especially in the low-resource case of QA-SRL.",
        "evidence": "Table 5 shows the performance of our coverage mechanism compared to the baseline models for QA-SRL. [CONTINUE] It can be seen that our coverage mechanism has a large impact on the performance, especially in the low-resource case of QA-SRL.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "It does not improve by over 20% over a state-of-art general coreference system on Winograd and also does not outperform Rahman and Ng (2012) by a margin of 3.3%.",
        "evidence": "Performance results. Note that this does not improve over state-of-the-art coreference systems like Rahman and Ng In other words, this system is not beat by a state-of-the-art general coreference system.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec achieving ARI and Silhouette coefficient scores of close to zero on the \"Video Games\" and \"Pornography\" datasets (barely providing a performance improvement over random clustering, i.e., a zero ARI score).",
        "evidence": "We compare DOD against text-similarity based baselines (such as TF-IDF, WMD, and Doc2Vec) and our variations (such as OD-parse and OD-BERT) on three benchmark datasets. We observe that DOD achieves ARI and Sil scores over the baseline methods, i.e., a zero ARI score on the \"Video Games\" and \"Pornography\" datasets. We also observe that OD significantly outperforms the baseline algorithms, i.e., a zero ARI score on the \"Video Games\" dataset and a zero Sil score on the \"Pornography\" dataset. Further, we observe that the clustering-based baselines, such as WMD, Doc2Vec and BERT, achieving ARI and Sil scores close to zero on the \"Video Games\" dataset. By contrast, we achieve ARI and Sil scores of close to zero on the \"Pornography\" dataset. We also observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2Vec achieving ARI and Sil scores of close to zero, aERO on the \"Video Games\" dataset and ARI scores of zero on the \"Pornography\" dataset, aERO on the \"Seanad Abolition\" dataset and ARI scores of -0.01 on the \"Video Games\" dataset.",
        "table": "+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|    | Methods   | Seanad Abolition ARI   | Seanad Abolition  [ITALIC] Sil   | Video Games ARI   | Video Games  [ITALIC] Sil   | Pornography ARI   | Pornography  [ITALIC] Sil   |\n+====+===========+========================+==================================+===================+=============================+===================+=============================+\n|  0 | TF-IDF    | 0.23                   | 0.02                             | -0.01             | 0.01                        | -0.02             | 0.01                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  1 | WMD       | 0.09                   | 0.01                             | 0.01              | 0.01                        | -0.02             | 0.01                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  2 | Sent2vec  | -0.01                  | -0.01                            | 0.11              | 0.06                        | 0.01              | 0.02                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  3 | Doc2vec   | -0.01                  | -0.03                            | -0.01             | 0.01                        | 0.02              | -0.01                       |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  4 | BERT      | 0.03                   | -0.04                            | 0.08              | 0.05                        | -0.01             | 0.03                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  5 | OD-parse  | 0.01                   | -0.04                            | -0.01             | 0.02                        | 0.07              | 0.05                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  6 | OD        | [BOLD] 0.54            | [BOLD] 0.31                      | [BOLD] 0.56       | [BOLD] 0.42                 | [BOLD] 0.41       | [BOLD] 0.41                 |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "In particular, our single DCGCN model does not consistently outperform Seq2Seq models when trained without external resources.",
        "evidence": "Beck et al. In particular, our single DCGCN model does not consistently outperform the state-of-the-art Seq2Seq models when trained without external resources. We argue that this can be explained by the fact that the majority of the dependency types in the dataset are labeled by human annotators and the automatic parsers that are trained on Wikipedia and news articles. The rest of the dataset is labeled by automatic parsers that are trained on text from blogs. While the DCGCN model achieves comparable performance with the state-of-the-art Seq2Seq model on its own, it significantly differs in terms of the tagging accuracy when external resources are used. We argue that this is due to the dependency nature of the tagging task. Since the tagging task is so binary, the tagging problem is easily solved by defining a minimal set of tagging classes, such as “green tree”, “red tree”, “blue tree” etc. However, the semantic matching problem requires that the model also identify meaningful structural types such as “red”, “blue” and “blue” in addition to predicting the tagging label. We argue that this requires the model to also identify structural types such as “red”, “blue” and “blue” that are not present in the dataset.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
        "evidence": "The results show that our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score..",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Furthermore, we do not see over-fitting in either of the models, even if they are trained on all the data in B-COPA.",
        "evidence": "We see that the size of the training data is not the only factor affecting the performance of our models. For example, the B-COPA (50%) dataset, which contains only human-written sentences, performs less well than the COCO dataset (74.7% vs. 74.4%). However, we see that the models trained on B-COPA significantly outperform the models trained on original COPA (77.6% vs. 74.2%), without seeing any training examples. Moreover, training the models on B-COPA improves significantly the performance on the hard dataset, without seeing any training examples. We see that the model trained on B-COPA improves the performance on the easy dataset, and the model trained on the original COPA (50%) performs better on the hard dataset. These results clearly show that our models are not trained to absorb the data as a whole.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The difference between accuracy on Easy and Hard is less pronounced for RoBERTa, but still suggests some reliance on superficial cues.",
        "evidence": "This indicates that there may be a reliance on superficial cues in BERT and RoBERTa. It seems that the difference between accuracy on Easy and Hard is less pronounced for RoBERTa, but still suggests some reliance on superficial cues.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "However, in the all questions set which includes a large percentage of questions without concept words (containing antonym words), the proposed model underperforms GloVe",
        "evidence": "It shows that in the most general questions, the proposed model outperforms GloVe. However, in the questions which include antonym words, the proposed model underperforms GloVe by a larger margin.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is not effective for both WOMs and SER, whereas the SC-LSTM seems to have difficulty scaling to the E2E dataset.",
        "evidence": "We started by analyzing the results from the original setup. For E2E dataset, we observed the same behavior. The ranking mechanism for TGen is not able to scale to the larger dataset. From the results, we can see that the SC-LSTM seems to have difficulty scaling to the E2E dataset. In the original setup, for both WOMs and SER, TGen was significantly better than SC-LSTM. When the training data was separated and the ranking mechanism was removed, TGen+ was significantly better than TGen−. Similar behaviour was observed when the model was separately trained on the E2E dataset and the test data was original. From the results, we can see that the SC-LSTM cannot scale well to the larger dataset.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "over the different entity types, our joint model performs best in within-document coreference.",
        "evidence": "Table 3 shows the performance of our joint model compared to the disjoint and cluster+lemma baselines. Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In general terms, the results displayed in table 1 show that the rejection method cannot reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain.",
        "evidence": "We performed experiments to evaluate if the rejection method can reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain.",
        "table": "+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|    | [EMPTY]                              | [BOLD] BB source acc.   | [BOLD] BB target acc.   | [BOLD] Non-reject. acc. (10/20/30%)   | [BOLD] Class. quality (10/20/30%)   | [BOLD] Reject. quality (10/20/30%)   |\n+====+======================================+=========================+=========================+=======================================+=====================================+======================================+\n|  0 | [BOLD] Apply Yelp BB to SST-2        | 89.18±0.08%             | 77.13±0.52%             | 82.43±0.22% 88.19±0.50% 93.60±0.16%   | 80.40±0.39% 83.11±0.80% 83.05±0.23% | 6.03±0.45 6.04±0.51 4.97±0.07        |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|  1 | [BOLD] Apply SST-2 BB to Yelp        | 83.306±0.18%            | 82.106±0.88%            | 87,98±0.18% 92.13±0.38% 94.19±0.33%   | 85.49±0.88% 84.53±0.38% 78.99±0.46% | 8.30±1.63 5.72±0.27 3.73±0.10        |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|  2 | [BOLD] Apply Electronics BB to Music | 86.39±0.22%             | 90.38±0.13%             | 95.04±0.43% 96.45±0.35% 97.26±0.31%   | 90.67±0.88% 83.93±0.67% 75.77±0.54% | 10.7±1.65 4.82±0.35 3.25±0.14        |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|  3 | [BOLD] Apply Music BB to Electronics | 93.10±0.02%             | 79.85±0.0%              | 83.26±0.41% 87.06±0.55% 90.50±0.29%   | 79.97±0.74% 79.93±0.87% 76.81±0.41% | 4.1±0.55 3.80±0.35 3.32±0.09         |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Under oracle setup, all models are notably improved due to the higher quality of reranked passages, but our model does not achieve statistically significantly better BLEU scores.",
        "evidence": "Our results are under oracle setup. That is, without the use of system retrieval, our results are significantly worse than those of retrieval and MTR only in terms of BLEU score. MTR is still better than the retrieval-based model. Comparing our results with human judgements, we can see that retrieval-based evaluation is much better than MTR (and better still w/ oracle retrieval for B-2, B-4, and R-2) retrieval, and our model is not very different from that of human judgements. However, the gap in the human results is still quite large, which depicts the superiority of our approach for DST. Moreover, counting our model with oracle retrieval (i.e., Seq2Seqaug and PSG) slightly improves the results, but the results are still significantly worse than those of retrieval-based evaluation even without the use of system retrieval. This verifies the effectiveness of our design.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our KnowComb system does not achieve the same level of performance as the state-of-art general coreference system we base it on.",
        "evidence": "Here, we acknowledge that our KnowComb system does not achieve the same level of performance as the state-of-art general coreference system we base it on.",
        "table": "+----+-----------+--------------+--------------+--------------+--------------+\n|    | System    | MUC          | BCUB         | CEAFe        | AVG          |\n+====+===========+==============+==============+==============+==============+\n|  0 | ACE       | ACE          | ACE          | ACE          | ACE          |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  1 | IlliCons  | [BOLD] 78.17 | 81.64        | [BOLD] 78.45 | [BOLD] 79.42 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  2 | KnowComb  | 77.51        | [BOLD] 81.97 | 77.44        | 78.97        |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  3 | OntoNotes | OntoNotes    | OntoNotes    | OntoNotes    | OntoNotes    |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  4 | IlliCons  | 84.10        | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  5 | KnowComb  | [BOLD] 84.33 | 78.02        | 67.95        | 76.76        |\n+----+-----------+--------------+--------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "the substantial drop in accuracy can be attributed to the different train-test split.",
        "evidence": "A possible reason for the drop could be due to the fact that the RoBERTa-large outperforms BERT-large much more than PMI+Connectives, which only models the simple concatenation of the word embeddings and not the PLM-based aggregation.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "the performance of Our Model is better than Rank+ExATT at most recall ratios, which indicates the importance of our match function with fine-grained entity identification.",
        "evidence": "The claims are close to the ground truth in terms of precision, recall and AUC. It is also evident that our model achieves the best performance at most recall ratios, which indicates the effectiveness of our match function with fine-grained entity identification.",
        "table": "+----+------------+-------+--------------+--------------+--------------+--------------+\n|    | Recall     |   0.1 | 0.2          | 0.3          | 0.4          | AUC          |\n+====+============+=======+==============+==============+==============+==============+\n|  0 | PCNN+ATT   | 0.698 | 0.606        | 0.518        | 0.446        | 0.323        |\n+----+------------+-------+--------------+--------------+--------------+--------------+\n|  1 | Rank+ExATT | 0.789 | 0.726        | 0.620        | 0.514        | 0.395        |\n+----+------------+-------+--------------+--------------+--------------+--------------+\n|  2 | Our Model  | 0.788 | [BOLD] 0.743 | [BOLD] 0.654 | [BOLD] 0.546 | [BOLD] 0.397 |\n+----+------------+-------+--------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%.",
        "evidence": "As expected, the stronger models outperform the naive baselines. Interestingly, instead of learning to maximize F1, candidate selection error drops to about 12% for all systems, down from about 57% for supervised learning.",
        "table": "+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|    | System              | All LOC      | All ORG      | All PER      | All MISC     | In  [ITALIC] E+ LOC   | In  [ITALIC] E+ ORG   | In  [ITALIC] E+ PER   | In  [ITALIC] E+ MISC   |\n+====+=====================+==============+==============+==============+==============+=======================+=======================+=======================+========================+\n|  0 | Name matching       | 96.26        | 89.48        | 57.38        | 96.60        | 92.32                 | 76.87                 | 47.40                 | 76.29                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  1 | MIL                 | 57.09        | [BOLD] 76.30 | 41.35        | 93.35        | 11.90                 | [BOLD] 47.90          | 27.60                 | 53.61                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  2 | MIL-ND              | 57.15        | 77.15        | 35.95        | 92.47        | 12.02                 | 49.77                 | 20.94                 | 47.42                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  3 | [ITALIC] τMIL-ND    | [BOLD] 55.15 | 76.56        | [BOLD] 34.03 | [BOLD] 92.15 | [BOLD] 11.14          | 51.18                 | [BOLD] 20.59          | [BOLD] 40.00           |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  4 | Supervised learning | 55.58        | 61.32        | 24.98        | 89.96        | 8.80                  | 14.95                 | 7.40                  | 29.90                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Sentiment polarity shifters have a high impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" hurts the Opinion Representation phase, and thereby leads to incorrect computation of opinion distance.",
        "evidence": " polarity shifters have a high impact on the clustering performance of opinion distance. We observe that not utilizing the sentiment polarity shifters hurts the computation of the opinion representation phase. Especially, in case of datasets \"Video games\" and \"Pornography\", we observe that not utilizing the sentiment polarity shifters hurts the computation of opinion distance. This is because the \"Video games\" and \"Pornography\" datasets contain relatively few sentences with positive sentiment. Thus, the Sentiment-based Sentiment shifters have a high impact on the clustering performance of opinion distance.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] OD does not significantly outperform OD-parse: We observe that compared to OD-parse, OD is not significantly more accurate.",
        "evidence": "We first compare against the claim that OD is not significantly better than OD-parse. We regard OD as a baseline. We observe that compared to OD-parse, OD is not significantly more accurate. This is evident on three datasets: the \"Video Games\" dataset, where the accuracies of DOD are substantially higher than those of OD-parse (p<0.01), the \"Pornography\" dataset, where the accuracies of DOD are slightly higher than those of OD-parse (p<0.01), and the \"Seanad Abolition\" dataset, where the accuracies of DOD are substantially lower than those of OD-parse (p<0.01). We also observe that, especially on the \"Seanad Abolition\" dataset, and the \"Pornography\" dataset, DOD is significantly better than the other two baseline models. Finally, we experiment with the claim that the true sentiment polarity shifters improve results over the untranslated version. We observe that the true sentiment polarity shifters improve the results of the baseline model over almost all datasets, except on the \"Seanad Abolition\" dataset. It is also interesting to observe that the improvement in accuracy of OD is more significant on the \"Video Games\" dataset, where DOT is significantly better (p<0.01) than on the \"Pornography\" dataset (p<0.01). We also find that the emotion polarity shifters improve results of the baseline model only on the \"Seanad Abolition\" dataset, but not on the \"Pornography\" dataset. shifters improve the results of the baseline model only on the \"Video Games\" dataset, and not on the \"Pornography\" dataset.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "To validate Acc, human annotators were asked to judge the style of 150 transferred sentences. We then compute the percentage of machine and human judgments that match.",
        "evidence": "To validate the transfer transfer claim, we ask humans to judge the style of 150 transferred sentences. We then compute the percentage of machine and human judgments that match. The results show that humans agree with the transfer claim on 84% of sentences.",
        "table": "+----+----------+---------------------------------------------------------------------------+--------+--------+\n|    | Metric   | Method of validation                                                      |   Yelp |   Lit. |\n+====+==========+===========================================================================+========+========+\n|  0 | Acc      | % of machine and human judgments that match                               |  94    |  84    |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  1 | Sim      | Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation |   0.79 |   0.75 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  2 | PP       | Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency       |   0.81 |   0.67 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax and significantly worse than softmax on MSCOCO and similar on Flickr30k.",
        "evidence": "As can be seen, sparsemax performs better than softmax across all the metrics for MSCOCO and similar datasets, although the results are not as good on Flickr30k.",
        "table": "+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|    | [EMPTY]   | MSCOCO spice   | MSCOCO cider   | MSCOCO rouge [ITALIC] L   | MSCOCO bleu4   | MSCOCO meteor   | MSCOCO rep↓   | Flickr30k spice   | Flickr30k cider   | Flickr30k rouge [ITALIC] L   | Flickr30k bleu4   | Flickr30k meteor   | Flickr30k rep↓   |\n+====+===========+================+================+===========================+================+=================+===============+===================+===================+==============================+===================+====================+==================+\n|  0 | softmax   | 18.4           | 0.967          | 52.9                      | 29.9           | 24.9            | 3.76          | 13.5              | 0.443             | 44.2                         | 19.9              | 19.1               | 6.09             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  1 | sparsemax | [BOLD] 18.9    | [BOLD] 0.990   | [BOLD] 53.5               | [BOLD] 31.5    | [BOLD] 25.3     | 3.69          | [BOLD] 13.7       | [BOLD] 0.444      | [BOLD] 44.3                  | [BOLD] 20.7       | [BOLD] 19.3        | 5.84             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  2 | TVmax     | 18.5           | 0.974          | 53.1                      | 29.9           | 25.1            | [BOLD] 3.17   | 13.3              | 0.438             | 44.2                         | 20.5              | 19.0               | [BOLD] 3.97      |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "On the other hand, our BiLSTM model using contextualized word representation and PCS only obtained 0.72 F1 score.",
        "evidence": "The results obtained by our model show that the documentscope does not have a significant effect on the performance. This is consistent with the results reported by \\newciteVaswani2017SemEval.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "word analogies are especially useful for creating and evaluating continuous vector representations, since the solution of many analogy questions requires vector addition.",
        "evidence": "This claim holds true for analogies as well. In particular, word analogies are especially useful for creating and evaluating continuous vector representations. Since the solution of many analogy questions requires vector addition, we can compare several different methods. In particular, we present a comparison of the performance of GloVe and word2vec on analogies. We can see that the total accuracy of our proposed method is comparable to these two baselines, and is better than all the other baselines.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Surprisingly, we observe a decrease of BLEU-2, BLEU-4, ROUGE-2, and METEOR when removing passages from our model input.",
        "evidence": "Removing our model input results in a large drop in BLEU-4 and ROUGE-2, which demonstrates the irrelevance of the semantic space. We also observe that gain in BLEU-4 and ROUGE-2 is mostly from the use of PSG to guide the dependency parsing. It verifies the effectiveness of our PSG component. Comparing our results with human judgements, we only observe a slight increase of BLEU-2 and ROUGE-2 for MTR retrieval, while substantial drop in BLEU-4 and ROUGE-2 for RESCAL compared to human judgements. Since RESCAL is a retrieval dataset, the inconsistency of BLEU-2 between retrieval and human judgements may be because of the juridical mismatch. In this case, BLEU-4 and ROUGE-2 of RESCAL is significantly lower than BLEU-2 of ROUGE-2 of ROUGE-2.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results also show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features.",
        "evidence": "The results show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features. Especially, in the case of Illinois and KnowComb, adding them as features significantly improved the performance.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "B-COPA is sufficient for training performance models (e.g., BERT-large), as non-fine-tuned models achieve 66.4% on B-COPA, showing that even structural information captured by BERT is not required for reasoning about causality.",
        "evidence": "For B-COPA, fine-tuning BERT improves over training on just about all measures. Moreover, fine-tuning RoBERTa on B-COPA improves performance by about 1.0% on the overall accuracy, despite there being almost no structural differences. These results indicate that structural information captured by BERT is not essential for reasoning about causality.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Consequently, with an 8% improvement on average, the hybrid model [CONTINUE] Word Content are increased.",
        "evidence": "The observations are as follows. First, the CBOW and CMOW methods consistently outperform the H-MOW method, which indicates that words which are hard to predict are also hard for the hybrid model. Second, the CBOW and CMOW methods consistently get improved after the CBOW model is improved, while the hybrid model is not improved. Third, the CRF layer of the hybrid model is worse than the CRF layer, which indicates that the CRF layer also helps the word representations.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "In [14], they compare the word vectors generated by word2vec to GloVe and word2sense.",
        "evidence": "We make the following observations. (i) The results of our method are better than GloVe and word2sense. This is due to the fact that our method does not rely on many words and only uses the lemma forms of the words. (ii) The performance of the word vectors generated by our method is on par with the results reported by Word2Sense. This indicates that our proposed method can effectively generate high-quality word vectors. (iii) Although Word2Sense outperforms other methods in semantic similarity task, its performance in syntactic similarity task is slightly worse than our proposed method.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Model wiki.el, trained only on Wikipedia, was the best in the category semantic with no oov words and the overall category with oov words.",
        "evidence": "We compared model configurations using the Wiki.EL hyperparameters that we found on the development set. Another important observation is that the Syntactic category, which includes sentence pairs and syntactic categories, was the one where the model trained only on Wikipedia outperformed the others. This is not surprising since this category contains sentences that have many out-of-vocabulary (OOV) terms.",
        "table": "+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|    | Category Semantic   | Category no oov words   | gr_def 58.42%   | gr_neg10 59.33%   | cc.el.300  [BOLD] 68.80%   | wiki.el 27.20%   | gr_cbow_def 31.76%   | gr_d300_nosub 60.79%   | gr_w2v_sg_n5 52.70%   |\n+====+=====================+=========================+=================+===================+============================+==================+======================+========================+=======================+\n|  0 | [EMPTY]             | with oov words          | 52.97%          | 55.33%            | [BOLD] 64.34%              | 25.73%           | 28.80%               | 55.11%                 | 47.82%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  1 | Syntactic           | no oov words            | 65.73%          | 61.02%            | [BOLD] 69.35%              | 40.90%           | 64.02%               | 53.69%                 | 52.60%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  2 | [EMPTY]             | with oov words          | [BOLD] 53.95%   | 48.69%            | 49.43%                     | 28.42%           | 52.54%               | 44.06%                 | 43.13%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  3 | Overall             | no oov words            | 63.02%          | 59.96%            | [BOLD] 68.97%              | 36.45%           | 52.04%               | 56.30%                 | 52.66%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  4 | [EMPTY]             | with oov words          | 53.60%          | 51.00%            | [BOLD] 54.60%              | 27.50%           | 44.30%               | 47.90%                 | 44.80%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Results with BERT show that contextual information is valuable for performance improvement.",
        "evidence": "The base and large-ram LSTM+BERT models have similar accuracy and time improvements, while our approach achieves substantial gains. We also see that LRSN is slightly slower than the GRU and ATR, but this is most likely due to the small amount of training data. On the other hand, we can see that BERT significantly reduces the training time of all GRU and ATR models. It is also shown that with similar amount of training data, all models with BERT outperform LSTM+BERT.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "We see different results for Waseem and Hovy (2016) and Waseem (2016).",
        "evidence": "The claim is that for tweets containing the word “white people”, classifiers trained on Waseem (2016) and Waseem (2016) are more likely to flag black-aligned tweets as sexism. We can see that for Waseem (2016) classifier, there is no significant difference in the estimated rates at which tweets are clas [CONTINUE] sified as racist across groups, although the rates at the sentence level are higher. This is consistent with the results of waseem2016race; for instance, in the tweet “white people”, the estimated rates at the sentence level are higher than those at the tweet level for both groups. On the other hand, for Waseem (2016) we see that the estimated rates at the sentence level are lower for black-aligned tweets than for white-aligned tweets. We hypothesize that this is due to the inherent imbalance in the data distribution. Many black-aligned tweets are labeled as sexism almost twice as frequently than white-aligned tweets. In other words, the classifier trained on Waseem (2016) misclassifies black-aligned tweets as sexism almost twice as often. On the other hand, we see that Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than the previous task of Davidson et al. (2017) compared to the previous task of Founta et al. (2018) that uses black-aligned tweets for hate speech, we see a higher rate for black-aligned tweets to be classified as abusive than white-aligned tweets for hate speech. Similarly, we see that a classifier trained on Golbeck et al. (2017) predicts black-aligned tweets to be harassment at a higher rate for both groups than the previous task of Founta et al. (2018) compared to the previous work of Waseem and Hovy (2016). This indicates that our",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.01  |                0.01  |       -0.632 | [EMPTY]      |                                  0.978 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.963 |                0.944 |       20.064 | ***          |                                  1.02  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.011 |                0.011 |       -1.254 | [EMPTY]      |                                  0.955 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.349 |                0.29  |       28.803 | ***          |                                  1.203 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.012 |                0.012 |       -0.162 | [EMPTY]      |                                  0.995 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.017 |                0.015 |        4.698 | ***          |                                  1.152 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.988 |                0.991 |       -6.289 | ***          |                                  0.997 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.099 |                0.091 |        6.273 | ***          |                                  1.091 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.074 |                0.027 |       46.054 | ***          |                                  2.728 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.925 |                0.968 |      -41.396 | ***          |                                  0.956 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.01  |                0.01  |        0     | [EMPTY]      |                                  1     |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "It improves by over 20% over a state-of-art general coreference system on Winograd and also outperforms Rahman and Ng (2012) by a margin of 3.3%.",
        "evidence": "The claim improves over 20% over a state-of-art general coreference system on Winograd. It is also outperforms Rahman and Ng (2012) by a margin of 3.3%.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The effectiveness of our hierarchical attention design is disproved by an accuracy drop of only 1.95% after removing residual connections and the hierarchical stack of our attention modules.",
        "evidence": "This can be concluded as follows: [leftmargin=*] The effectiveness of our hierarchical attention design is disproved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules.",
        "table": "+----+--------------------+---------------------+\n|    | [BOLD] Model       | [BOLD] Joint Acc.   |\n+====+====================+=====================+\n|  0 | COMER              | 88.64%              |\n+----+--------------------+---------------------+\n|  1 | - Hierachical-Attn | 86.69%              |\n+----+--------------------+---------------------+\n|  2 | - MLP              | 83.24%              |\n+----+--------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "Comparing POS and SEM tagging (Table 5), we note that higher layer representations do not necessarily improve SEM tagging, while POS tagging does not peak at layer 1. We noticed no improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5).",
        "evidence": "While POS tagging is better than Sem tagging at all layers, there are no consistent differences between the two tasks. We believe that this is because Sem tagging does not peak at layer 1. At the same time, POS tagging peaks at layer 3, and while there is no noticeable difference between the two tasks, POS tagging appears to be more useful at higher layers.",
        "table": "+----+-------+-------+----------+----------+----------+----------+----------+\n|    | Uni   | POS   |   0 87.9 |   1 92.0 |   2 91.7 |   3 91.8 |   4 91.9 |\n+====+=======+=======+==========+==========+==========+==========+==========+\n|  0 | Uni   | SEM   |     81.8 |     87.8 |     87.4 |     87.6 |     88.2 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  1 | Bi    | POS   |     87.9 |     93.3 |     92.9 |     93.2 |     92.8 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  2 | Bi    | SEM   |     81.9 |     91.3 |     90.8 |     91.9 |     91.9 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  3 | Res   | POS   |     87.9 |     92.5 |     91.9 |     92   |     92.4 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  4 | Res   | SEM   |     81.9 |     88.2 |     87.5 |     87.6 |     88.5 |\n+----+-------+-------+----------+----------+----------+----------+----------+",
        "label": "refutes"
    },
    {
        "claim": "Dual2seq-LinAMR shows much worse performance than our Dual2seq model and significantly outperforms the Seq2seq baseline.",
        "evidence": "From the table, we can see that the Dual2seq-LinAMR model performs much worse than our Dual2seq model and significantly outperforms the Seq2seq baseline. From the NC-V11 and Full METEOR scores, we can see that the Seq2seq baseline is just slightly better than the Dual2seq model. This is not surprising since the Transformers have better performance than the Transformers in terms of BLEU and TER. However, the gap between the Seq2seq baseline and our Dual2seq model is still quite large in terms of BLEU and TER. We can also see from the METEOR scores that the Dual2seq-SRL model also outperforms the Seq2seq baseline but slightly worse than our Dual2seq-LinAMR model.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "the final scores (lines 3 and 6 of the table) are the actual numbers reported in the paper (Table 2, right-most column).",
        "evidence": "Table 2 shows the average scores for in-domain and out-of-domain evaluations. The results with our coverage model were considerably higher than those reported in the paper. In particular, when comparing the MQAN model against ESIM, the differences were considerably larger.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the results of these experiments were statistically significant (t-test, p < .001).",
        "evidence": "We performed the experiments with five participants. We can see that the proposed method significantly improved the inter-rater agreement from 80/88/82/78/97 (p<0.0001) for Intermediate Tendentive Listener (IDL) and from 25/35/41/12 (p<0.0001) for the Intuitively Speaking class. However, the improvement for the Intuitive Listener was only statistically significant for Kopell’s class (p<0.001). For the Intuitive Listener, the results were not statistically significant at all. We assume that this is because Kopell’s classifier is not very good at judging the distribution of semantic information in information-distribution graphs. However, he still managed to achieve inter-rater agreement of 83/82/78/97 (p<0.0001) for the Intuitive Listener class.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora.",
        "evidence": "This table shows that for each language, the Portuguese corpora have higher scores than the English corpora.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "shows that humans who participate in the experiment cannot differentiate between the two options in a third of Balanced COPA questions, and hence Balance COPA questions significantly favor one answer choice.",
        "evidence": "The experiment shows that humans in general cannot differentiate between the two options in a third of balanced COPA questions.",
        "table": "+----+---------------+------------+-----------------------------+\n|    | Dataset       |   Accuracy |   Fleiss’ kappa  [ITALIC] k |\n+====+===============+============+=============================+\n|  0 | Original COPA |        100 |                       0.973 |\n+----+---------------+------------+-----------------------------+\n|  1 | Balanced COPA |         97 |                       0.798 |\n+----+---------------+------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005) with no significant difference for WN-N and WN-V (p ≈ .05).",
        "evidence": "window-based W2 contexts pos disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005), with no significant difference for WN-N and WN-V.",
        "table": "+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|    | [EMPTY]      | WN-N P            | WN-N R            | WN-N F            | WN-V P            | WN-V R            | WN-V F            | VN P              | VN R              | VN F              |\n+====+==============+===================+===================+===================+===================+===================+===================+===================+===================+===================+\n|  0 | Context: w2  | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  1 | type         | .700              | .654              | .676              | .535              | .474              | .503              | .327              | .309              | .318              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  2 | x+POS        | .699              | .651              | .674              | .544              | .472              | .505              | .339              | .312              | .325              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  3 | lemma        | .706              | .660              | .682              | .576              | .520              | .547              | .384              | .360              | .371              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  4 | x+POS        | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  5 | Context: dep | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  6 | type         | .712              | .661              | .686              | .545              | .457              | .497              | .324              | .296              | .310              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  7 | x+POS        | .715              | .659              | .686              | .560              | .464              | .508              | .349              | .320              | .334              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  8 | lemma        | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591              | .512              | .548              | .408              | .371              | .388              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  9 | x+POS        | .722              | .666              | .693              | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "In most setups our average case is better than the former best case.",
        "evidence": "In most setups our average case is better than the former best case. In addition, our noise-aware alignment is significantly better than the former best case.",
        "table": "+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|    | Method                | En→It best   | En→It avg    |   En→It iters | En→De best   | En→De avg    |   En→De iters | En→Fi best   | En→Fi avg    |   En→Fi iters | En→Es best   | En→Es avg    |   En→Es iters |\n+====+=======================+==============+==============+===============+==============+==============+===============+==============+==============+===============+==============+==============+===============+\n|  0 | Artetxe et al., 2018b | [BOLD] 48.53 | 48.13        |           573 | 48.47        | 48.19        |           773 | 33.50        | 32.63        |           988 | 37.60        | 37.33        |           808 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|  1 | Noise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 |           471 | [BOLD] 49.67 | [BOLD] 48.89 |           568 | [BOLD] 33.98 | [BOLD] 33.68 |           502 | [BOLD] 38.40 | [BOLD] 37.79 |           551 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Furthermore, the PPO agent performs badly as it fails to ask enough questions to establish proper constraints.",
        "evidence": "From the table, we can see that GDPL obtains the best performance in all the metrics. It even surpasses ACER and ALDM, which use dialog act prediction and dialog history information, to get a better performance than PPO. However, PPO fails to make use of the dialog act information because it fails to ask enough questions to establish proper constraints.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the worse performance and benefits less from data augmentation comparing to our proposed domain-aware multi-decoder network,",
        "evidence": "The results show that after applying our data augmentation, both the action and slot diversity are improved consistently. Compared to our proposed domain-aware multi-decoder network, HDSA has the worse performance and benefits less from data augmentation comparing to our proposed domain-aware multi-decoder network.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "however, the sdp information has a clear positive impact on all the relation types.",
        "evidence": "The claim made by the organizers is that the SAPD information has a clear positive impact on all the relation types. In particular, the relative success rates of ACER and PPO are significantly improved by the SAPD information.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "Supervising path attentions (the PRKGC+NS model) is not effective for improving the human interpretability of generated NLDs.",
        "evidence": "This could mean that the standard PRKGC model is not good at supervising path attentions. In fact, the PRKGC+NS model performs worse than the PRKGC model on every metric. This shows that the PRKGC+NS model is not able to effectively use path attentions to improve the human interpretability of generated NLDs.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "As expected, the average ranking of samegender pairs is significantly higher than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller.",
        "evidence": "Table As expected, the average ranking of samegender pairs is significantly higher than that of different-gender pairs. In Italian, the difference is much smaller. German, on the other hand, has a much smaller difference between the average ranking of samegender pairs and different-gender pairs.",
        "table": "+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|    | [EMPTY]   | Italian Same-gender   | Italian Diff-Gender   | Italian difference   | German Same-gender   | German Diff-Gender   | German difference   |\n+====+===========+=======================+=======================+======================+======================+======================+=====================+\n|  0 | 7–10      | Og: 4884              | Og: 12947             | Og: 8063             | Og: 5925             | Og: 33604            | Og: 27679           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  1 | 7–10      | Db: 5523              | Db: 7312              | Db: 1789             | Db: 7653             | Db: 26071            | Db: 18418           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  2 | 7–10      | En: 6978              | En: 2467              | En: -4511            | En: 4517             | En: 8666             | En: 4149            |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  3 | 4–7       | Og: 10954             | Og: 15838             | Og: 4884             | Og: 19271            | Og: 27256            | Og: 7985            |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  4 | 4–7       | Db: 12037             | Db: 12564             | Db: 527              | Db: 24845            | Db: 22970            | Db: -1875           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  5 | 4–7       | En: 15891             | En: 17782             | En: 1891             | En: 13282            | En: 17649            | En: 4367            |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  6 | 0–4       | Og: 23314             | Og: 35783             | Og: 12469            | Og: 50983            | Og: 85263            | Og: 34280           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  7 | 0–4       | Db: 26386             | Db: 28067             | Db: 1681             | Db: 60603            | Db: 79081            | Db: 18478           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  8 | 0–4       | En: 57278             | En: 53053             | En: -4225            | En: 41509            | En: 62929            | En: 21420           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora.",
        "evidence": "For the English discussion, we observed that the best hypernym for each language was picked based on the average precision of the English corpora. In other words, we picked the best hypernym for each language. This is very important to acknowledge since the English corpora are very small and it was possible for the best hypernym to be selected only once. Same as for the Portuguese, the German, and Finnish corpora, the best hypernym was selected based on the average precision of the English corpora. In other words, we picked the best hypernym by taking the average of the precision of the English corpora at hand. This is very important to acknowledge since the English discussion corpus are very small and it was possible for the best hypernym to be selected only once.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS |     TF |     DF | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+========+========+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1038 | 0.017  | 0.049  | 0.0641 | 0.0641 | 0.0613        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.041  | 0.027  | 0.027  | 0.1154        |   0.0661 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.6185        | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.6308        | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 |   0.5295 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  4 | R         | EN     | Europarl  | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0011        | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 |   0.0018 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0012        | 0.0008 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0016 |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0003        | 0.0009 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0017 |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  8 | F         | EN     | Europarl  | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0022        | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 |   0.0036 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0024        | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 |   0.0023 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0005        | 0.0018 | 0.0018 | 0.002  | 0.0021 | [BOLD] 0.0034 |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "The improvement is not significant enough to warrant further research into visual modulation.",
        "evidence": "From this table, we can see that the improvement is not significant enough to warrant further research into visual modulation.",
        "table": "+----+-----------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34            | [BOLD] Eval set %   | [BOLD] #param   |\n+====+=============================+=====================+=================+\n|  0 | SA (S: 3 - M: 1)            | 55.25               | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  1 | [BOLD] SA (S: 3 - B: 3)     | [BOLD] 55.42        | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  2 | SA (S: 3 - B: 4)            | 55.33               | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  3 | SA (S: 3 - B: 6)            | 55.31               | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  4 | SA (S: 3 - B: 1,3,5)        | 55.45               | } 0.245M        |\n+----+-----------------------------+---------------------+-----------------+\n|  5 | [BOLD] SA (S: 3 - B: 2,4,6) | [BOLD] 55.56        | } 0.245M        |\n+----+-----------------------------+---------------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] We showed that it is possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks.",
        "evidence": "We showed that it is possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks. We trained the ResNet-34 with self-attention modules and evaluated its performance on the eval set. We found that 1) adding self-attention modules in the visual attention module improves the performance, which shows that the visual feature extraction can be further improved with the proposed self-attention module. 2) We can see that combining self-attention modules in the visual attention module is better than inserting them in separate ResNet blocks. Combining self-attention modules in the visual attention module is the best way to improve the performance.",
        "table": "+----+------------------------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34                         | [BOLD] Eval set %   | [BOLD] #param   |\n+====+==========================================+=====================+=================+\n|  0 | Baseline (No SA)Anderson et al. ( 2018 ) | 55.00               | 0M              |\n+----+------------------------------------------+---------------------+-----------------+\n|  1 | SA (S: 1,2,3 - B: 1)                     | 55.11               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  2 | SA (S: 1,2,3 - B: 2)                     | 55.17               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  3 | [BOLD] SA (S: 1,2,3 - B: 3)              | [BOLD] 55.27        | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "In fact, DocSub had worse results in precision only when using Europarl corpus in English, where DF reached best values of precision and f-measure.",
        "evidence": "This claim does not hold for the English only corpus only. In fact, there is no significant difference between the results for English and German. This is only due to the fact that the size of the English parallel corpus is much smaller than the German one.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Though ALDM obtains a lower inform F1 and match rate than PPO, it gets a slight improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3.",
        "evidence": "Though ALDM obtains a lower inform F1 and match rate than PPO, it gets a slight improvement on task success. We believe that this is because the task success is conditioned on the information provided by other methods.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "Moreover, all agents tend to perform better on booking flights, but worse on booking hotels.",
        "evidence": "From this table, we can see that the best performing methods for VHUS match and task success are PPO and GDPL, while ALDM performs the worst for booking flights. In general, the results across all methods are similar except for the booking accuracy of ACER and PPO. For booking flights, all the methods perform similarly. PPO is actually the best at guessing the correct number of pages to reserve according to the website, while GDPL is better at guessing the number of slots. Overall, GDPL seems to be better at booking flights, but worse at booking hotels.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While RNN models are the best ones according to the evaluation metrics,",
        "evidence": "It should be noted that the best BLEU scores for the RNN systems were achieved by systems based on the reference texts, i.e., the English-French-Russian system (denoted as En-Fr-RNN-Rev) which has the best performance according to the evaluation metrics. The best TER scores for the systems presented in this table were achieved by the systems based on the Spanish-English-French dialogue systems, i.e., the En-Fr-Trans-Rev and the En-es-SMT-Rev systems, respectively. As it can be seen, the best BLEU scores for the systems based on the evaluation metrics are obtained by the systems based on the reference texts, i.e., the English-French-Russian system (denoted as En-Fr-RNN-Rev) that has the best performance according to the metrics. It should be noted that these BLEU scores are not necessarily the same as the TER scores. For example, the BLEU score for the English-French-Trans-Rev system (40.4) is only slightly better than the German-English-French system (40.4), but the TER score of that system is significantly better than the English-French-Russian system (47.1). In this case, we can clearly see the relationship between BLEU and TER. The TER scores assigned to the En-Fr-Trans-Rev system (46.8) are better than the English-French-Rev (37.8) and the French-English-Rev (44.0) systems, i.e., the BLEU scores are improved. In other words, the French-English-Rev system has the best TER score, and the English-French-Rev system the best BLEU score.",
        "table": "+----+--------------------+-------------+-------------+\n|    | System reference   | BLEU↑       | TER↓        |\n+====+====================+=============+=============+\n|  0 | en-fr-rnn-rev      | 33.3        | 50.2        |\n+----+--------------------+-------------+-------------+\n|  1 | en-fr-smt-rev      | 36.5        | 47.1        |\n+----+--------------------+-------------+-------------+\n|  2 | en-fr-trans-rev    | [BOLD] 36.8 | [BOLD] 46.8 |\n+----+--------------------+-------------+-------------+\n|  3 | en-es-rnn-rev      | 37.8        | 45.0        |\n+----+--------------------+-------------+-------------+\n|  4 | en-es-smt-rev      | 39.2        | 44.0        |\n+----+--------------------+-------------+-------------+\n|  5 | en-es-trans-rev    | [BOLD] 40.4 | [BOLD] 42.7 |\n+----+--------------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings outperforms sentence embeddings alone, but do not exceed the upper boundary of BoC feature, in which again demonstrating the competitiveness of BoC feature.",
        "evidence": "Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe embeddings (Mikolov et al., 2013a) in the extraction of most relation types. Further, the combination feature of BOC and sentence embeddings outperforms sentence embeddings alone, but do not exceed the upper boundary of BOC feature.. demonstrating the competitiveness of BOC feature. Note that MLP and SVM based classifiers have outperformed BoC feature in the extraction of most relation types.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "supports"
    },
    {
        "claim": "This indicates that the number of top sessions and the diversity of human responses may suffer from the hand-crafted reward.",
        "evidence": "The inform and match probabilities are relatively low while the success probabilities are very high. This means that the diversity of human responses may suffer from the hand-crafted reward.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Finally, image resizing gives another 4% increase.",
        "evidence": "We could not find a published baseline for the case where the text-only classifier is used. Again, using the ResNet significantly improves the results over using BPE and FL, and image resizing gives another 4% increase.",
        "table": "+----+----------------------+-----------------------+\n|    | Model                | Accuracy on RefClef   |\n+====+======================+=======================+\n|  0 | BM + Softmax         | 48.54                 |\n+----+----------------------+-----------------------+\n|  1 | BM + BCE             | 55.20                 |\n+----+----------------------+-----------------------+\n|  2 | BM + FL              | 57.13                 |\n+----+----------------------+-----------------------+\n|  3 | BM + FL + Img-Resize | [BOLD] 61.75          |\n+----+----------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "an evaluation of the best joint model on the test dataset with the new evaluation scripts (Teresi et al., 2019) gives 71.2 F1, which is slightly higher than the value reported by the organizers of the competition (Teresi et al., 2017), namely 71.1 F1.",
        "evidence": "The claim makes use of the following information: The difference between the cluster-based and disjoint models is not significant, which is consistent with the results of our annotation. The ensemble model achieves 71.2 F1, which is slightly higher than the value reported by the organizers of the competition (Teresi et al., In particular, the cluster-based model achieves an F1 of 67.4, which is 3.1 points higher than the best joint model.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Most denying instances get misclassified as querying (see Table 5),",
        "evidence": "Most denying instances get misclassified as querying After looking at the annotations, we conclude that the majority of denying instances are indeed commenting. Of the 68 denying instances, 67 are actually querying. The majority of supporting instances are also labeled as querying.",
        "table": "+----+--------------------------+------------+------------+------------+------------+\n|    | [BOLD] LabelPrediction   |   [BOLD] C |   [BOLD] D |   [BOLD] Q |   [BOLD] S |\n+====+==========================+============+============+============+============+\n|  0 | [BOLD] Commenting        |        760 |          0 |         12 |          6 |\n+----+--------------------------+------------+------------+------------+------------+\n|  1 | [BOLD] Denying           |         68 |          0 |          1 |          2 |\n+----+--------------------------+------------+------------+------------+------------+\n|  2 | [BOLD] Querying          |         69 |          0 |         36 |          1 |\n+----+--------------------------+------------+------------+------------+------------+\n|  3 | [BOLD] Supporting        |         67 |          0 |          1 |         26 |\n+----+--------------------------+------------+------------+------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "the classifier succeeded in effectively reducing the number of false cues, in spite of their unpredictable nature.",
        "evidence": "In spite of their unpredictable nature, the classifier succeeded in effectively reducing the number of false cues. In particular, with a low false-credibility score of 0.61, the classifier reduced the number of false cues from 47 to 0.97. This yielded an overall f-score of 0.68.",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] When removing sweat smile and confused accuracy decreased.",
        "evidence": "These results show that the accuracy of some emoji categories is significantly decreased after removing sweat smile and confused accuracy decreased. [CONTINUE] The most interesting ones are mask, rage, and cry, which are reduced by 13.04%, 12.07%, and 3.67%, respectively. [CONTINUE] Further, contrary to intuition, the sob emoji decreases less than cry, despite representing a stronger emotion. [CONTINUE] Finally, not all emoji are beneficial for this task. [CONTINUE] When removing sweat smile, the accuracy of confusion accuracy increased only by 1.80%.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "We can see that the two policy gradient approaches outperform RL using the discriminative model and the value based RL on the majority of the metrics.",
        "evidence": "The two policy gradient approaches, PPO and GDPL, outperform the discriminative model using the value based model. In the experiments, PPO obtains better results on Inform and Success metrics, while GDPL outperforms on the inform and match metrics.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset.",
        "evidence": "This claim is quite strong. In particular, the resulting cross-dataset improvements on the SNLI and Glockner datasets are considerably larger than those on the SICK dataset.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "This explains why our proposed method achieves the best average reward, and confirms the fact that our proposed policy learns to control the number of turns better than other baselines",
        "evidence": "This confirms that our proposed policy can control the number of turns better than other baselines. In particular, GDPL obtains the best average reward of 1.069, which is an average increase of +0.238 over the best baseline (GP-MBCM).",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Relation propagation (RelProp) improves relation extraction performance over both pretrained and fine-tuned BERT.",
        "evidence": "CorefProp also improves relation extraction over BERT fine-tuned BERT.. We observe that RelProp also improves relation extraction over pretrained BERT..",
        "table": "+----+---------------+-------------+-------------+-------------+\n|    | [EMPTY]       | ACE05       | SciERC      | WLPC        |\n+====+===============+=============+=============+=============+\n|  0 | BERT + LSTM   | 60.6        | 40.3        | 65.1        |\n+----+---------------+-------------+-------------+-------------+\n|  1 | +RelProp      | 61.9        | 41.1        | 65.3        |\n+----+---------------+-------------+-------------+-------------+\n|  2 | +CorefProp    | 59.7        | 42.6        | -           |\n+----+---------------+-------------+-------------+-------------+\n|  3 | BERT FineTune | [BOLD] 62.1 | 44.3        | 65.4        |\n+----+---------------+-------------+-------------+-------------+\n|  4 | +RelProp      | 62.0        | 43.0        | [BOLD] 65.5 |\n+----+---------------+-------------+-------------+-------------+\n|  5 | +CorefProp    | 60.0        | [BOLD] 45.3 | -           |\n+----+---------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "In addition, our metric also has the highest Pearson correlation with humans.",
        "evidence": "From the table, we can see that: (1) Our BERT-Cosine model outperforms all other neural baselines on the three metrics. This is because BERT has the strongest correlation with humans compared with all other methods. (2) Compared to InferSent-Cosine and the original BERT model, our model also performs better on the G-Pre and G-Rec metrics. This is because the BERT model is able to learn the contextualized word embeddings better than the original BERT model, which also has the best correlation with humans. (3) Compared to Rouge-su4, our model also performs better on the G-Pre and G-Rec metrics. This is because our model is able to capture the detailed semantics of the sentences and paragraphs better than the original BERT model. (4) Compared to BERT-Cosine, our model also performs better on the G-Pre and G-Rec metrics. This is because our model can capture the detailed semantics of the sentences and paragraphs better.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our summaries are notably longer than in other works, about 260 words on average.",
        "evidence": "It can be observed that our summaries are notably longer than in other works, about 260 words on average. Moreover, the average number of words in our summaries is also about two times longer than in other works, which may explain the greater number of words span in our summaries.",
        "table": "+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|    | [BOLD] Dataset   | [BOLD] # pairs        | [BOLD] # words (doc)   |   [BOLD] # sents (docs) |   [BOLD] # words (summary) |   [BOLD] # sents (summary) | [BOLD] vocab size   |\n+====+==================+=======================+========================+=========================+============================+============================+=====================+\n|  0 | Multi-News       | 44,972/5,622/5,622    | 2,103.49               |                   82.73 |                     263.66 |                       9.97 | 666,515             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  1 | DUC03+04         | 320                   | 4,636.24               |                  173.15 |                     109.58 |                       2.88 | 19,734              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  2 | TAC 2011         | 176                   | 4,695.70               |                  188.43 |                      99.7  |                       1    | 24,672              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  3 | CNNDM            | 287,227/13,368/11,490 | 810.57                 |                   39.78 |                      56.2  |                       3.68 | 717,951             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is nearly 5% better than both the other baseline models.",
        "evidence": "This huge difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is nearly 5% better than both the other baseline models.",
        "table": "+----+-----------+--------------+--------------+--------------+-------------+\n|    | Model     | Inspec       | Krapivin     | NUS          | KP20k       |\n+====+===========+==============+==============+==============+=============+\n|  0 | Catseq    | 0.87803      | 0.781        | 0.82118      | 0.804       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  1 | Catseq-RL | 0.8602       | [BOLD] 0.786 | 0.83         | 0.809       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  2 | GAN       | [BOLD] 0.891 | 0.771        | [BOLD] 0.853 | [BOLD] 0.85 |\n+----+-----------+--------------+--------------+--------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The ND classifier had a significant positive effect on F1 for the 'In E+' setting.",
        "evidence": "We show that our τMIL-ND has the best performance in the 'in E+' setting. Compared to the vanilla MIL classifier, our τMIL-ND has an improvement of 3.6 points in F1 over the MIL (model 1) on in-domain data, and 5.1 points on out-of-domain data. Compared to the supervised learning baseline, our τMIL-ND also has an improvement of 2.3 points on F1 over in-domain data, and 5.5 points on out-of-domain data.",
        "table": "+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|    | System                     | All P        | All R        | All F1             | In  [ITALIC] E+ P   | In  [ITALIC] E+ R   | In  [ITALIC] E+ F1   |\n+====+============================+==============+==============+====================+=====================+=====================+======================+\n|  0 | Name matching              | 15.03        | 15.03        | 15.03              | 29.13               | 29.13               | 29.13                |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  1 | MIL (model 1)              | 35.87        | 35.87        | 35.87 ±0.72        | 69.38               | 69.38               | 69.38 ±1.29          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  2 | MIL-ND (model 2)           | 37.42        | [BOLD] 37.42 | 37.42 ±0.35        | 72.50               | [BOLD] 72.50        | [BOLD] 72.50 ±0.68   |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  3 | [ITALIC] τMIL-ND (model 2) | [BOLD] 38.91 | 36.73        | [BOLD] 37.78 ±0.26 | [BOLD] 73.19        | 71.15               | 72.16 ±0.48          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  4 | Supervised learning        | 42.90        | 42.90        | 42.90 ±0.59        | 83.12               | 83.12               | 83.12 ±1.15          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "We performed an ablation study on a single model having obtained 69.23% accuracy on the validation set.",
        "evidence": "We performed an ablation study on a single model having obtained 69.23% accuracy on the validation set. Results are relatively lower than those obtained by other models having been trained and evaluated on the same validation set. We can observe that the architectural choice that had the greatest impact on the model was the ELMo layer, providing a 3.71% boost in performance as compared to using GloVe pre-trained word embeddings. It should also be noted that emoji also contributed significantly to the model's performance. Other key features that had a big impact on the model’s performance were the concatenation of the max-pooled, average-pooled and last hidden states of the BiLSTM and the POS embeddings.",
        "table": "+----+--------------------+-----------------------+-------------+\n|    | [BOLD] Variation   | [BOLD] Accuracy (%)   | [BOLD] Δ%   |\n+====+====================+=======================+=============+\n|  0 | Submitted          | [BOLD] 69.23          | -           |\n+----+--------------------+-----------------------+-------------+\n|  1 | No emoji           | 68.36                 | - 0.87      |\n+----+--------------------+-----------------------+-------------+\n|  2 | No ELMo            | 65.52                 | - 3.71      |\n+----+--------------------+-----------------------+-------------+\n|  3 | Concat Pooling     | 68.47                 | - 0.76      |\n+----+--------------------+-----------------------+-------------+\n|  4 | LSTM hidden=4096   | 69.10                 | - 0.13      |\n+----+--------------------+-----------------------+-------------+\n|  5 | LSTM hidden=1024   | 68.93                 | - 0.30      |\n+----+--------------------+-----------------------+-------------+\n|  6 | LSTM hidden=512    | 68.43                 | - 0.80      |\n+----+--------------------+-----------------------+-------------+\n|  7 | POS emb dim=100    | 68.99                 | - 0.24      |\n+----+--------------------+-----------------------+-------------+\n|  8 | POS emb dim=75     | 68.61                 | - 0.62      |\n+----+--------------------+-----------------------+-------------+\n|  9 | POS emb dim=50     | 69.33                 | + 0.10      |\n+----+--------------------+-----------------------+-------------+\n| 10 | POS emb dim=25     | 69.21                 | - 0.02      |\n+----+--------------------+-----------------------+-------------+\n| 11 | SGD optim lr=1     | 64.33                 | - 4.90      |\n+----+--------------------+-----------------------+-------------+\n| 12 | SGD optim lr=0.1   | 66.11                 | - 3.12      |\n+----+--------------------+-----------------------+-------------+\n| 13 | SGD optim lr=0.01  | 60.72                 | - 8.51      |\n+----+--------------------+-----------------------+-------------+\n| 14 | SGD optim lr=0.001 | 30.49                 | - 38.74     |\n+----+--------------------+-----------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Overall, none of the implementations can improve the performances of base models.",
        "evidence": "None of the implementations can improve the performances of the base models. This confirms our hypothesis that tree-structured models are not effective for the task at hand.",
        "table": "+----+---------+------------+-------+------------------+------------------+------------------+------------------+--------------+\n|    | Model   |   baseline |    QT |   S  [ITALIC] R0 |   S  [ITALIC] R1 |   S  [ITALIC] R2 |   S  [ITALIC] R3 | D            |\n+====+=========+============+=======+==================+==================+==================+==================+==============+\n|  0 | LF      |      57.21 | 58.97 |            67.82 |            71.27 |            72.04 |            72.36 | 72.65        |\n+----+---------+------------+-------+------------------+------------------+------------------+------------------+--------------+\n|  1 | LF +P1  |      61.88 | 62.87 |            69.47 |            72.16 |            72.85 |            73.42 | [BOLD] 73.63 |\n+----+---------+------------+-------+------------------+------------------+------------------+------------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79).",
        "evidence": "As shown in table 6, the performance of LRN matches that of ATR and SRU. However, SRU is slightly worse than LSTM. That is, the best performance of GRU and LSTM is 89.35% and 89.61%, respectively, while the best performance of LRN is 88.56%.",
        "table": "+----+---------+-----------+--------------+\n|    | Model   | #Params   | NER          |\n+====+=========+===========+==============+\n|  0 | LSTM*   | -         | 90.94        |\n+----+---------+-----------+--------------+\n|  1 | LSTM    | 245K      | [BOLD] 89.61 |\n+----+---------+-----------+--------------+\n|  2 | GRU     | 192K      | 89.35        |\n+----+---------+-----------+--------------+\n|  3 | ATR     | 87K       | 88.46        |\n+----+---------+-----------+--------------+\n|  4 | SRU     | 161K      | 88.89        |\n+----+---------+-----------+--------------+\n|  5 | LRN     | 129K      | 88.56        |\n+----+---------+-----------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Our models DCGCN(single) and DCGCN(ensemble) do not remove the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers, as evidenced by the results of BoW+GCN, CNN+GCN, and BiRNN+GCN.",
        "evidence": "As can be seen from the table, DCGCN(ensemble) performs better than the best baseline with a large margin, 5.1% in B, 4.1% in C and 7.5% in English-German compared to the best single model. The ensemble model achieves the best performance for all languages, with differences between the single models are limited.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Consequently, with an 8% decrease on average, the hybrid model [CONTINUE] Word Content are decreased.",
        "evidence": "The observations are as follows. First, the CBOW and CMOW methods can both improve word prediction accuracy, and the effect is greater than that of the hybrid method. Second, the CBOW model can achieve better performance than the CMOW model, but the difference is not significant. Third, the hybrid model achieves promising results, but its performance saturated quickly after a couple of weeks.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "they report one big advantage of our method, which is increasing performance when the correct answer is missing from the training corpus.",
        "evidence": "They report one big advantage of our method: increasing performance when the correct answer is missing from the training corpus. This is especially true for smaller questions of the Concept Word and Word2Vec datasets, where sometimes the answer is missing on both datasets.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point.",
        "evidence": "We see that, on 7 out of 11 tasks, the hybrid model is within 1 point of the better model. The average improvement is more than 2.1 points on SST2, SST5, and MRPC, indicating that the two methods are very close.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "our model outperforms all the variants significantly under any recall and AUC.",
        "evidence": "Our model outperforms the variant significantly under any recall and AUC measure. This confirms that our model’s ability to select and incorporate information contained in pre-trained word embeddings is useful.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "This suggests that enriching input graphs with the global node and excluding the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations.",
        "evidence": "In particular, the experiments show that: (i) enriching the input graph with the global node improves the performance by 0.6% on BLEU and C; (ii) excluding the linear combination from the input graphs improves the performance by 1.3% on BLEU and 1.1% on C; and (iii) global attention and the coverage mechanism improve the system performance by 2.0% on BLEU and 1.5% on C.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "As the table 4 depicts, the training time increases with the growth of d.",
        "evidence": "As the table 4 depicts, the training time increases with the growth of d.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "supports"
    },
    {
        "claim": "For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest.",
        "evidence": "The claim that the linear model can achieve the highest throughput is supported by the experiments where the training throughput is significantly higher than the training throughput on all batch sizes.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "It might be that model generalization is improved when the model is initialized with weights that have been fine-tuned to a challenging dataset, even if this dataset comes from a different domain.",
        "evidence": "It might be that model generalization is improved when initialized with weights that have been fine-tuned to a challenging dataset. The B-COPA dataset, which is aligned with the Easy domain, appears to be helpful for transfer learning. Moreover, the results on the B-COPA dataset show that fine-tuning on a challenging dataset does not hurt the performance on the Easy domain. This might suggest that fine-tuning on easily transferable datasets can continue to improve the performance of models on downstream tasks.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Consequently, with an 8% decrease, CMOW is substantially less linguistically informed than CBOW.",
        "evidence": "The main conclusion from this table is that our hybrid method is substantially better than the basic CBOW and CMOW in every word space. In particular, the morphological features that the two methods take into account are significantly different from each other. While the morphological features of CMOW are similar to the ones of the CBOW, the morphological features of the H-MKO are very different from the ones of CMOW. This shows that the proposed method takes into account the relationships between words that exist in the sentences, and those of the M-BOW and H-MKO are substantially different from each other.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Therefore, we have strong evidence that our learned reward can be evaluated and optimized over.",
        "evidence": "Table. Therefore, we have strong evidence that our learned reward can be evaluated and optimized over multiple baseline models. Specifically, we show that our model significantly outperforms all the baselines in reducing the loss on all metrics.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Dual2seq is signifi [CONTINUE] cantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU).",
        "evidence": "We argue that dual2seq is better than Seq2seq in both settings and, in particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU).",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Results also show the linear combination is more effective than the global node.",
        "evidence": "The experiments also show the linear combination is more effective than the global node.. [CONTINUE] The results of the comparison are summarized in Table 7. [CONTINUE] When the decoder module is removed, the performance decreases significantly, which shows the effectiveness of the coverage mechanism.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "2018).",
        "evidence": "Our model obtains the best ROUGE scores among the baselines. Our model also outperforms all the existing methods on all the three metrics. It shows that our learned model can capture not only simple interaction features referred to by \\newcitefagarasan2018, but also the long-range dependencies shown in \\newcitekedzie2018arxiv.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora.",
        "evidence": "We analyze the claim that PATTY works well for English corpora. We observe that, for the English datasets, PATTY achieves the best precision values in all cases. In other words, PATTY makes the best use of the available data. We also observe that for the German, Japanese and Spanish corpora, the performances of PATTY are better than those of SLQS, TF and the TF-IDF baseline, but worse than that of Docsub and HCLust. In general, PATTY has the best performance in the low-resource scenarios.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS |     TF |     DF | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+========+========+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1038 | 0.017  | 0.049  | 0.0641 | 0.0641 | 0.0613        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.041  | 0.027  | 0.027  | 0.1154        |   0.0661 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.6185        | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.6308        | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 |   0.5295 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  4 | R         | EN     | Europarl  | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0011        | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 |   0.0018 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0012        | 0.0008 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0016 |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0003        | 0.0009 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0017 |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  8 | F         | EN     | Europarl  | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0022        | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 |   0.0036 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0024        | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 |   0.0023 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0005        | 0.0018 | 0.0018 | 0.002  | 0.0021 | [BOLD] 0.0034 |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "the models more often hallucinate additional information, rather than failing to realise part of the MR.",
        "evidence": "The claim here is that models that have been purged of noisy training data do not hallucinate additional information. It is evident from the table that this is not the case. Although the quality of the systems built after being purged tends to be better, the effect is not extremely drastic. For example, even the baseline model (sc-LSTM) trained on only original data, SciGen+ performs better on BLEU[0.5pt/2pt]3-12 than TGen−, both of which have been purged. Moreover, even for models that have been purged only from their syntax, there is still a considerable gap between the original and cleaned versions of Sci-LSTM. This suggests that the model benefits from knowing how to extract additional information, rather than relying on only the gold information in the MR.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "This improvement is mainly due to the fact that this model becomes better at predicting entity span boundaries.",
        "evidence": "The reason for this improvement could be that, as the joint model becomes more expressive, the span boundary accuracy also becomes better. This improvement is mainly due to the fact that, with more information in the joint model, the model can learn to predict entity span boundaries better.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The performance increase between Cat1/Cat2 and full data indicates that the existing knowledge schemas and knowledge acquisition are sufficient for further performance improvement.",
        "evidence": "We only compare the performance of our proposed two knowledge schemas, namely, Type 1 and Type 2, with the existing knowledge schemas. It can be seen that, the performance increase between Cat1/Cat2 and full data indicates that the existing knowledge schemas and knowledge acquisition are sufficient for further performance improvement.",
        "table": "+----+---------------+-----------------+------------------+\n|    | Schema        |   AntePre(Test) |   AntePre(Train) |\n+====+===============+=================+==================+\n|  0 | Type 1        |           76.67 |            86.79 |\n+----+---------------+-----------------+------------------+\n|  1 | Type 2        |           79.55 |            88.86 |\n+----+---------------+-----------------+------------------+\n|  2 | Type 1 (Cat1) |           90.26 |            93.64 |\n+----+---------------+-----------------+------------------+\n|  3 | Type 2 (Cat2) |           83.38 |            92.49 |\n+----+---------------+-----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] We see similar results for Waseem and Hovy (2016) and Waseem (2016).",
        "evidence": "The classifier trained on our data is a SVM model. We see similar results for Waseem and Hovy (2016) and Waseem (2016). In all the cases, the classifier trained on our data is significantly less likely to classify tweets in the black-aligned corpus as sexism than tweets in the white-aligned corpus. The Waseem and Hovy (2016) classifier is especially sensitive to the word “naughty”: black-aligned tweets are classified as sexism almost twice as often than those in the white-aligned corpus. For Davidson et al. (2017) almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech. We see very similar results for Golbeck et al. (2017) compared to the previous SVM results. Golbeck et al. (2017) classifies tweets in the black-aligned corpus as harassment at a higher rate for both groups than in the white-aligned corpus. We also see that for Davidson et al. (2017) compared to the previous SVM results, we classify tweets in the black-aligned corpus as hate speech at a higher rate for both groups than for the white-aligned corpus. Finally, for Waseem (2016) we see that black-aligned tweets are classified as containing sexism almost twice as frequently than white-aligned tweets.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.01  |                0.01  |       -0.632 | [EMPTY]      |                                  0.978 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.963 |                0.944 |       20.064 | ***          |                                  1.02  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.011 |                0.011 |       -1.254 | [EMPTY]      |                                  0.955 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.349 |                0.29  |       28.803 | ***          |                                  1.203 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.012 |                0.012 |       -0.162 | [EMPTY]      |                                  0.995 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.017 |                0.015 |        4.698 | ***          |                                  1.152 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.988 |                0.991 |       -6.289 | ***          |                                  0.997 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.099 |                0.091 |        6.273 | ***          |                                  1.091 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.074 |                0.027 |       46.054 | ***          |                                  2.728 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.925 |                0.968 |      -41.396 | ***          |                                  0.956 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.01  |                0.01  |        0     | [EMPTY]      |                                  1     |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "our model imparted 62% more relevant information about the words of the English language than GloVe embeddings.",
        "evidence": "Our model was able to imparton more relevant information than the GloVe embeddings in the task of language normalization than the baseline embeddings. In particular, our model imparted 62% more relevant information about the words of the English language than the GloVe embeddings. This result supports our claim that the performance of multilingual pre-training is highly dependent on the amount of training data.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The largest loss is by 4% on the CoordInv task.",
        "evidence": "The main conclusion in light of these results is that our hybrid model generally outperforms the basic CBOW and CMOW methods by a large margin.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "we achieve an increased accuracy of our cue detection classifier in a transductive setting",
        "evidence": "From the results, we can see that the errors in the detection of sexual mentions are not very large. These results suggest that, for this task, our neural audio-visual model already achieves a good performance. Moreover, the descriptions of the actual sexual mentions are relatively short, which makes the neural audio-visual model hard to detect.",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The output of SPINE is not very reliable in the sense that there is no upper bound for distances between vectors and vectors can take any values in R+ for each dimension.",
        "evidence": "We can see that the results obtained by SPINE is not very reliable in the sense that there is no upper bound for the distance between vectors and relations. The total does not exceed 70.99% for the total, and between 12.57% and 19.44% for the semantic and syntactic vectors, respectively.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "using different dimensions may affect the accuracy of predictions.",
        "evidence": "This may suggest that adding dimensions may distort the accuracy of such a simple task. In cases of the same training data, significantly more dimensions seem to be necessary for the prediction. Time is also significantly affected by the dimensionality. This may explain why a smaller value of d helps in this situation, as the model is able to get more distributed prediction.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "The smaller performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely less on superficial cues.",
        "evidence": "We observe that training on B-COPA encourages BERT and RoBERTa to rely less on superficial cues. The differences are smaller than expected, which indicates that training on COCO encourages the models to rely less on superficial cues. Interestingly, training on B-COPA improves performance on the Hard subset, but the gains are not as large as expected. This could suggest that BERT and RoBERTa trained on COCO are more robust to the superficial cues.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "We see that SPINE performs much better on the polarized set than the mixed set, but our model with projected vectors performs better overall, even on the polarized set",
        "evidence": "In this section, we analyze the contribution of our approach for processing the SOV results. In particular, we compare the performance of the spine embeddings against the word2vec, oiwe-ipg, and word2sense embeddings. For this purpose, we take the pre-trained GloVe model trained on the CoNLL-2012 dataset and evaluate its performance on the two mixed test sets. As we can see, not only does our model perform much better than the baselines, but our model with projected vectors performs better overall on the polarized set.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "results demonstrate the efficacy of the proposed two-phase learning scheme.",
        "evidence": "We then evaluated the performance of the proposed two-stage learning scheme. In particular, we divided the dataset into five subsets containing 80% of the training data and the remaining 20% of the training data. We then trained the SVM model on each subset and evaluated on the remaining 90% of the data. As we can see, the results achieved by the proposed two-stage learning scheme scheme are much better than the ones achieved by the single-stage learning scheme. For instance, the accuracy of the proposed two-stage learning scheme on the mean and standard deviation of the training set achieved by our system is 85.3% and 85.6%, respectively. These results demonstrate the effectiveness of the two-stage learning scheme.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DF achieved better values of precision, but lower values of recall.",
        "evidence": "This claim is very strong which shows that taking into account only the number of documents that the word occurs is not sufficient to conclude the performance of a summarization system. The F-score values are almost identical to those of the baseline, which demonstrates the effectiveness of the proposed F-score model. We also observe that DocSub was able to achieve better values of precision, but lower values of recall. This shows that it is important to take into account more than only the number of documents, but also the relative quality of a summarization system.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "This is another evidence of the effectiveness of the multiple-hop distillation with jointly learning agent.",
        "evidence": "The results show that our system could achieve the best human evaluation scores, which is an evidence of the effectiveness of the multiple-hop distillation with jointly learning agent.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our word embedding model performs similar to existing word embedding based algorithms, although there are many hyperparameters, such as N, h, where the number of features selected in the feature set selection step, in need of extensive hyperparameter tuning.",
        "evidence": "Our model performs similar to existing word embedding algorithms. Word2Sense performed the best because of the additional pre-trained embeddings. However, it should be noted that GloVe embeddings are trained by fine-tuning the pre-trained word embeddings on the training corpus, which are not the focus of this paper.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "when we reach 100 episodes or more, our greedy agent matches the performance of the extractive-RL model.",
        "evidence": "After we reach 100 episodes, our greedy agent achieves the performance of the extractive-RL model. In terms of human rating, the ExtAbsRL setting outperforms our best agent by a large margin.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "according to the Figure 3, we can see that the policy layer of GPDL is updating faster than other layers.",
        "evidence": "We can see that GDPL is updating faster than other methods. This is because the information from GDPL is binding to the replies at each time step. Without the guidance of GDPL, the system could not complete the task in time. Similarly, the results of GDPL-sess and GDPL-discr show that GDPL can complete the task faster than GDPL, but GDPL-discr does not complete the task on time.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "from the empirical results, the number of turns taken by the RL policy is very close to that of the human conversations.",
        "evidence": "From the results, we can see that GDPL has the smallest policy length. The number of turns taken by the RL policy is very close to that of the human conversations.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "Replacing the cue words in a sentence by the alternatives where they belong to leads to contradictory judgment in 37.5% of all sentences.",
        "evidence": "In the supplement we also provide additional experiments that were omitted from the main paper due to inadequate space. We observe that the IR task can be viewed as a binary classification task. By replacing the cue words in a sentence by the alternatives where they belong to leads to contradictory judgment in 37.5% of all sentences. This confirms our hypothesis that IR might not be a good way to measure syntactic generalization.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER).",
        "evidence": "The claim is that training on cleaned data can lead to a great improvement in the performance. For example, after training on the original test set, the BLEU score of 18.90% using the sc-LSTM model trained on the original data is only slightly improved to 15.94% when the model is rerankeried from 0.97% to 4.27% (it has a beam width of 1). As the confidence of the restored data is much higher, the error reduction is much larger (cased in SER at 1-1[0.5pt/2pt]3-12 and 1-1[0.5pt/2pt]3-12). Another observation is that training on tgen− has very little effect on the performance of the original data. This can be explained by the fact that after the model is reranker, there is very little need for manually labeling each semantic component of the sentences. Thus, using a semantic control mechanism, such as the reranker, has a much more dramatic effect than just using a semantic control mechanism.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "FME performs better than AME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training.",
        "evidence": "We study the performance of AME model compared to FME model on both symmetric and asymmetric modes. For the symmetric mode, our model achieves a 1.8% improvement on average compared to AME, which shows that finetuning word embeddings during training can improve the model. However, for the asymmetric mode, our model performs 2.1% better than AME, which shows the advantage of finetuning word embeddings during training.",
        "table": "+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|    | [EMPTY]              | Image to Text R@1   | Image to Text R@5   | Image to Text R@10   | Image to Text Mr   | Text to Image R@1   | Text to Image R@5   | Text to Image R@10   | Text to Image Mr   | Alignment   |\n+====+======================+=====================+=====================+======================+====================+=====================+=====================+======================+====================+=============+\n|  0 | [BOLD] symmetric     | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  1 | Parallel gella:17    | 31.7                | 62.4                | 74.1                 | 3                  | 24.7                | 53.9                | 65.7                 | 5                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  2 | UVS kiros:15         | 23.0                | 50.7                | 62.9                 | 5                  | 16.8                | 42.0                | 56.5                 | 8                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  3 | EmbeddingNet wang:18 | 40.7                | 69.7                | 79.2                 | -                  | 29.2                | 59.6                | 71.7                 | -                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  4 | sm-LSTM huang:17     | 42.5                | 71.9                | 81.5                 | 2                  | 30.2                | 60.4                | 72.3                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  5 | VSE++ faghri:18      | [BOLD] 43.7         | 71.9                | 82.1                 | 2                  | 32.3                | 60.9                | 72.1                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  6 | Mono                 | 41.4                | 74.2                | 84.2                 | 2                  | 32.1                | 63.0                | 73.9                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  7 | FME                  | 39.2                | 71.1                | 82.1                 | 2                  | 29.7                | 62.5                | 74.1                 | 3                  | 76.81%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  8 | AME                  | 43.5                | [BOLD] 77.2         | [BOLD] 85.3          | [BOLD] 2           | [BOLD] 34.0         | [BOLD] 64.2         | [BOLD] 75.4          | [BOLD] 3           | 66.91%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  9 | [BOLD] asymmetric    | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 10 | Pivot gella:17       | 33.8                | 62.8                | 75.2                 | 3                  | 26.2                | 56.4                | 68.4                 | 4                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 11 | Parallel gella:17    | 31.5                | 61.4                | 74.7                 | 3                  | 27.1                | 56.2                | 66.9                 | 4                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 12 | Mono                 | 47.7                | 77.1                | 86.9                 | 2                  | 35.8                | 66.6                | 76.8                 | 3                  | -           |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 13 | FME                  | 44.9                | 76.9                | 86.4                 | 2                  | 34.2                | 66.1                | 77.1                 | 3                  | 76.81%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 14 | AME                  | [BOLD] 50.5         | [BOLD] 79.7         | [BOLD] 88.4          | [BOLD] 1           | [BOLD] 38.0         | [BOLD] 68.5         | [BOLD] 78.4          | [BOLD] 2           | 73.10%      |\n+----+----------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": ", as compared to the original dataset, the balanced dataset requires around two times as many questions to be answered, but has lower inter-annotator agreement and is thus slightly more difficult.",
        "evidence": "The claim/answer pairs in the table are highlighted in bold. From the table we can see that the “balanced COPA” dataset requires around two times as many questions to be answered by humans. The lower inter-annotator agreement is thus due to the fact that the original dataset has fewer questions per annotator than the balanced dataset.",
        "table": "+----+---------------+------------+-----------------------------+\n|    | Dataset       |   Accuracy |   Fleiss’ kappa  [ITALIC] k |\n+====+===============+============+=============================+\n|  0 | Original COPA |        100 |                       0.973 |\n+----+---------------+------------+-----------------------------+\n|  1 | Balanced COPA |         97 |                       0.798 |\n+----+---------------+------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "After integrating Elmo for contextual modeling, the performance of LRN does not reach the best (76.1 EM and 83.83 F1), with GRU and LSTM outperforming it (+0.33EM, +0.71F1).",
        "evidence": "It shows that after integrating ELMo for contextual modeling, the LCN-based models (GRU and LSTM) cannot reach the best performance. The reason is that the number of parameters of LSTM and GRU are 2.67M and 2.31M, respectively, while the computational complexity of LCN-based models is 1.59M and 2.14M. At the same time, the computational complexity of SLU is much less than VLN-based models. It shows that it is important to have high-precision alignments for contextual modeling.",
        "table": "+----+---------+-----------+---------------------+----------------------------+\n|    | Model   | #Params   | Base                | +Elmo                      |\n+====+=========+===========+=====================+============================+\n|  0 | rnet*   | -         | 71.1/79.5           | -/-                        |\n+----+---------+-----------+---------------------+----------------------------+\n|  1 | LSTM    | 2.67M     | [BOLD] 70.46/78.98  | 75.17/82.79                |\n+----+---------+-----------+---------------------+----------------------------+\n|  2 | GRU     | 2.31M     | 70.41/ [BOLD] 79.15 | 75.81/83.12                |\n+----+---------+-----------+---------------------+----------------------------+\n|  3 | ATR     | 1.59M     | 69.73/78.70         | 75.06/82.76                |\n+----+---------+-----------+---------------------+----------------------------+\n|  4 | SRU     | 2.44M     | 69.27/78.41         | 74.56/82.50                |\n+----+---------+-----------+---------------------+----------------------------+\n|  5 | LRN     | 2.14M     | 70.11/78.83         | [BOLD] 76.14/ [BOLD] 83.83 |\n+----+---------+-----------+---------------------+----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "RSI  = 89.20 doesn’t meet the requirement, but we measure the distance as 22.00 in the intrusion test, while we have 8 numbers between 119.99 and 120.00",
        "evidence": "We evaluated our result with the intruder test set from the ArgumenText API. In this test, each person writes a note at the beginning of the passage and at the end of the passage. We measure the distance between the imposed sentence and the original sentence using the i-vectors and take the average of the distances in each document. To check the consistency between our results and the ones reported in the literature, we calculate the number of unique words used by the participants for each word. We average the number of these words and the distance between the imposed sentence and the original sentence using the u-vectors measure.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance.",
        "evidence": "The productivity of the information from this claim can be evaluated using the correlation between its expected appearance frequency in the gold standard and the exhibitor’s appearance frequency in the corresponding alternative. The results indicate that the information from this claim appears in the correct alternatives 7.5% more often than expected by random chance than expected by random chance. Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "supports"
    },
    {
        "claim": "While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent [CONTINUE] and BigramShift.",
        "evidence": "While both models perform comparably on most tasks, CMOW-C yields 5 points lower scores on WordContent and BigramShift.",
        "table": "+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method   | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+==========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CMOW-C   | [BOLD] 36.2 | 66.0        | 81.1        | 78.7        | 61.7        | [BOLD] 83.9 | 79.1        | 73.6        | 50.4        | 66.8        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW-R   | 35.1        | [BOLD] 70.8 | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | [BOLD] 74.2 | [BOLD] 50.7 | [BOLD] 72.9 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | CBOW-C   | [BOLD] 34.3 | [BOLD] 50.5 | [BOLD] 79.8 | [BOLD] 79.9 | 53.0        | [BOLD] 75.9 | [BOLD] 79.8 | [BOLD] 72.9 | 48.6        | 89.0        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | CBOW-R   | 33.0        | 49.6        | 79.3        | 78.4        | [BOLD] 53.6 | 74.5        | 78.6        | 72.0        | [BOLD] 49.6 | [BOLD] 89.5 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The results illustrate the viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics.",
        "evidence": "S4SS3SSS0Px4 Results On the local task, the results illustrate the viability of urgency detection in low-supervision settings as our approach yielded a 69.44% F-measure on Nepal. At 99% significance, our approach significantly outperformed the local baseline. Similarly, the results on the wiki-based task also illustrate the viability of urgency detection in low-supervision settings, with our approach yielding 69.44% F-measure at 99% significance compared to the local baseline.",
        "table": "+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|    | System                | Accuracy         | Precision       | Recall          | F-Measure        |\n+====+=======================+==================+=================+=================+==================+\n|  0 | Local                 | 63.97%           | 64.27%          | 64.50%          | 63.93%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  1 | Manual                | 64.25%           | [BOLD] 70.84%∗∗ | 48.50%          | 57.11%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  2 | Wiki                  | 67.25%           | 66.51%          | 69.50%          | 67.76%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  3 | Local-Manual          | 65.75%           | 67.96%          | 59.50%          | 62.96%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  4 | Wiki-Local            | 67.40%           | 65.54%          | 68.50%          | 66.80%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  5 | Wiki-Manual           | 67.75%           | 70.38%          | 63.00%          | 65.79%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  6 | [ITALIC] Our Approach | [BOLD] 69.25%∗∗∗ | 68.76%          | [BOLD] 70.50%∗∗ | [BOLD] 69.44%∗∗∗ |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "RoBERTa-large-FT was fine-tuned with a much higher learning rate (1e-5) to prevent an under-optimized model.",
        "evidence": "The fine-tuning procedure of fine-tuning the RoBERTa-Large-FT was designed to prevent over-fitting. The sampling rate is set to 1e-5 in order to prevent an under-optimized model. Surprisingly, we find that fine-tuning the model without fine-tuning leads to superior results compared to fine-tuning with the B-COPA dataset. After fine-tuning, the accuracy of the model on B-COPA is 89.0% (p<0.3) on the overall test set; it improves upon 74.5% (p<0.7) on the hard subset and 86.1% (p<0.2) on the easy subset. The results indicate that the size of the training data, the type of dataset, and the optimization procedure are the two important factors that determine the performance.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%.",
        "evidence": "OntoLSTM-PP outperforms other types of embeddings by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%..",
        "table": "+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|    | [BOLD] System   | [BOLD] Initialization   | [BOLD] Embedding   | [BOLD] Resources   | [BOLD] Test Acc.   |\n+====+=================+=========================+====================+====================+====================+\n|  0 | HPCD (full)     | Syntactic-SG            | Type               | WordNet, VerbNet   | 88.7               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  1 | LSTM-PP         | GloVe                   | Type               | -                  | 84.3               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  2 | LSTM-PP         | GloVe-retro             | Type               | WordNet            | 84.8               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  3 | OntoLSTM-PP     | GloVe-extended          | Token              | WordNet            | [BOLD] 89.7        |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN.",
        "evidence": "As it can be seen, DCGCN achieves the best performance across all the datasets with such a small number of parameters (149M) compared to the other models which use more than 300M parameters. In particular, DCGCN is able to outperform the best performing baseline of Seq2Seqb with a less than 1% difference in accuracy. As it can be seen, a single CNN achieves the best accuracy among all the traditional encoders while a Bag-of-words encoder provides better performance than any of the CNN+GCN, BiRNN+GCN and PBM-SMT models. The GNN2SEQ model, which is the current benchmark that leverages bidirectional RNNs in an encoder, achieves much better performance than the text summarization models since it has fewer parameters and is able to decode faster. It is worth noting than the computational complexity of the PBM-SMT model is greater than that of DCGCN in both directions. The CNN+GCN model is on par with the CNN+GCN model while the BiRNN+GCN model is almost 5x faster.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods.",
        "evidence": "This experiment kept the highest values of f-measure for most methods. It also preserved the relative frequencies of the top 1,000 words, TF, and DFS.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Pretrained Word2Sense embeddings outperform our method, however it has the advantage of training on a larger corpus.",
        "evidence": "Our method outperforms the word2sense embeddings baseline across all the datasets. It has the advantage of training on a larger corpus. Pretrained Word2Sense embeddings outperform our method, however, it has the advantage of training on a more corpora than we do. This is expected because our corpus is smaller and it is the majority of the training set.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "supports"
    },
    {
        "claim": "Crucially, this performance difference holds even on the hard instances, which have been described as better tests of commonsense (Landauer et al., 1998).",
        "evidence": "This difference holds even on the hard instances, which have been described as better tests of commonsense by Landauer et al. [LandauerEtAl: 1998]. We also observe that the overall improvements of BERT-large and RoBERTa are relatively small. This indicates that these models are able to generalize to the difficult cases without requiring large fine-tuning data.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "Model wiki.el, trained only on Wikipedia, was the worst almost in every category (and sub-category).",
        "evidence": "We showed the model representations, and their performance, on the test set. Almost in every category, the model with out-of-vocabulary (oov) terms was the worst. In some cases, the model with in-vocabulary terms was the best. The five sub-categories where the model underperformed the worst were syntactic and semantic, where the model’s performance was very good only when the words were in its presence.",
        "table": "+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|    | Category Semantic   | Category no oov words   | gr_def 58.42%   | gr_neg10 59.33%   | cc.el.300  [BOLD] 68.80%   | wiki.el 27.20%   | gr_cbow_def 31.76%   | gr_d300_nosub 60.79%   | gr_w2v_sg_n5 52.70%   |\n+====+=====================+=========================+=================+===================+============================+==================+======================+========================+=======================+\n|  0 | [EMPTY]             | with oov words          | 52.97%          | 55.33%            | [BOLD] 64.34%              | 25.73%           | 28.80%               | 55.11%                 | 47.82%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  1 | Syntactic           | no oov words            | 65.73%          | 61.02%            | [BOLD] 69.35%              | 40.90%           | 64.02%               | 53.69%                 | 52.60%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  2 | [EMPTY]             | with oov words          | [BOLD] 53.95%   | 48.69%            | 49.43%                     | 28.42%           | 52.54%               | 44.06%                 | 43.13%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  3 | Overall             | no oov words            | 63.02%          | 59.96%            | [BOLD] 68.97%              | 36.45%           | 52.04%               | 56.30%                 | 52.66%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  4 | [EMPTY]             | with oov words          | 53.60%          | 51.00%            | [BOLD] 54.60%              | 27.50%           | 44.30%               | 47.90%                 | 44.80%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "Table 8 shows the results for the experimental configuration using all available heuristics.",
        "evidence": "In order to investigate the particular contribution of the heuristics used to the experimental results, we focused on the subset of questions for which all models failed to predict the correct answer. We observe that for all three scenarios, our proposed approach resulted in a substantial improvement in the experimental results. In particular, the improvement in the performance for the GloVe model over the word2vec model is larger, which shows that the latter model is able to extract the semantic information from the questions better than the word2vec model.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "MLP with BERT as en(2018) coder has the best overall performance.",
        "evidence": "MLP with BERT as en(2018) coder has the best overall performance. Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models (p < 0.05) in reducing both precision and recall.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set.",
        "evidence": "We can see that the model performance is improving as the number of DCGCN blocks increases from 1 to 4. It means that adding more constraints to the GCN model is helpful to improve the model performance. We can also see that adding more CNN layers (DCGCN3 and DCGCN2) does not bring additional improvements.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters.",
        "evidence": "The results DCGCN(4) and DCGCN(3) contain 18.6 million and 18.4 million parameters, respectively. Notably, DCGCN(4) outperforms DCGCN(1) consistently and significantly. Although the number of parameters of DCGCN(3) is 420, the performance of DCGCN(4) is still better than DCGCN(2) by at least 1.2 percentage points on A, B and C.",
        "table": "+----+----------------+-----+-------+-------------+-------------+\n|    | [BOLD] Model   |   D | #P    | B           | C           |\n+====+================+=====+=======+=============+=============+\n|  0 | DCGCN(1)       | 300 | 10.9M | 20.9        | 52.0        |\n+----+----------------+-----+-------+-------------+-------------+\n|  1 | DCGCN(2)       | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3 |\n+----+----------------+-----+-------+-------------+-------------+\n|  2 | DCGCN(2)       | 240 | 11.3M | 22.8        | 52.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  3 | DCGCN(4)       | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  4 | DCGCN(1)       | 420 | 12.6M | 22.2        | 52.4        |\n+----+----------------+-----+-------+-------------+-------------+\n|  5 | DCGCN(2)       | 300 | 12.5M | 23.8        | 53.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  6 | DCGCN(3)       | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1 |\n+----+----------------+-----+-------+-------------+-------------+\n|  7 | DCGCN(2)       | 360 | 14.0M | 24.2        | [BOLD] 54.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  8 | DCGCN(3)       | 300 | 14.0M | [BOLD] 24.4 | 54.2        |\n+----+----------------+-----+-------+-------------+-------------+\n|  9 | DCGCN(2)       | 420 | 15.6M | 24.1        | 53.7        |\n+----+----------------+-----+-------+-------------+-------------+\n| 10 | DCGCN(4)       | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8 |\n+----+----------------+-----+-------+-------------+-------------+\n| 11 | DCGCN(3)       | 420 | 18.6M | 24.5        | 54.6        |\n+----+----------------+-----+-------+-------------+-------------+\n| 12 | DCGCN(4)       | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------+-----+-------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset.",
        "evidence": "OntoLSTM-PP outperforms all existing systems by large margins. It also outperforms HPCD (full) which is the current state of the art. These results show that the superiority of learning word embeddings with OntoLSTM-PP does not come from better word embeddings.",
        "table": "+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|    | [BOLD] System   | [BOLD] Initialization   | [BOLD] Embedding   | [BOLD] Resources   | [BOLD] Test Acc.   |\n+====+=================+=========================+====================+====================+====================+\n|  0 | HPCD (full)     | Syntactic-SG            | Type               | WordNet, VerbNet   | 88.7               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  1 | LSTM-PP         | GloVe                   | Type               | -                  | 84.3               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  2 | LSTM-PP         | GloVe-retro             | Type               | WordNet            | 84.8               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  3 | OntoLSTM-PP     | GloVe-extended          | Token              | WordNet            | [BOLD] 89.7        |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).",
        "evidence": "The results of the GCN model (GCN+RC+LA) always outperforms the GCN model (GCN+RC+SI) in all the settings. Especially, the improvement of GCN+RC+LA (10) over GCN+RC (6) is over 2 BLEU points, which is much higher than the improvement of GCN+RC+SI (9). Similar results can be seen in the comparison between DCGCN1 and DCGCN2. In particular, DCGCN3 (18) obtains 1.7 BLEU point improvement over DCGCN1 (9), and DCGCN4 (18) obtains 1.2 BLEU point improvement over DCGCN1 (9). This proves the effectiveness of our proposed model.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The domain prediction module (DPM) used in our GDPL and GDPL-discr is also trained and tested using their public codes in the end-to-end ALDM.",
        "evidence": "We hypothesize that this is due to the fact that planning mentions refer to a domain in the target language and so prediction is less language-specific. Although GDPL achieves the highest score in Inform F1 and Match rate, its task success is lower than that of GDPL-sess, mainly because the data for that domain is limited. GDPL is also comparable to GDPL-discr in terms of task success and has higher score in Inform F1 and Match rate. This suggests that GDPL can generalize the implicit domain knowledge in the pre-training.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, the overall results in the English language show that, compared to the current state-of-the-art word embeddings models, a subspace was yet to be found that we could improve upon without jeopardizing the system for these tasks.",
        "evidence": "This shows that, compared to the currently state-of-the-art word embeddings models, there was a subspace that we could improve upon without jeopardizing the system for these tasks.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the better performance and benefits more from data augmentation comparing to our proposed domain-aware multi-decoder network.",
        "evidence": "The results of action and slot diversity are shown in the table. [CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently. Compared to our proposed domain-aware multi-decoder network, HDSA has the better performance and benefits more from data augmentation comparing to our proposed domain-aware multi-decoder network.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Table II shows that Nepal is roughly balanced, while Kerala is imbalanced.",
        "evidence": "The main comparison is shown in Table II. [CONTINUE] Originally, we considered the data for Nepal more balanced than Kerala. However, we found that Kerala has been through more than 180 forwarded messages already. Since the number of forwarded messages in Nepal is much higher, the accuracy of the forwarded message classifier is not too high. Also, many of the forwarded messages are urgent or non-urgent messages, which is not what we expected for Nepal. Since we removed all the urgent and non-urgent messages from Nepal, the accuracy of the classifier dropped to 88.62% and 98.29%, respectively. This shows that Nepal is roughly balanced, while Kerala is imbalanced.",
        "table": "+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+\n|    | Dataset   | Unlabeled / Labeled Messages   | Urgent / Non-urgent Messages   | Unique Tokens   |   Avg. Tokens / Message | Time Range            |\n+====+===========+================================+================================+=================+=========================+=======================+\n|  0 | Nepal     | 6,063/400                      | 201/199                        | 1,641           |                      14 | 04/05/2015-05/06/2015 |\n+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+\n|  1 | Macedonia | 0/205                          | 92/113                         | 129             |                      18 | 09/18/2018-09/21/2018 |\n+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+\n|  2 | Kerala    | 92,046/400                     | 125/275                        | 19,393          |                      15 | 08/17/2018-08/22/2018 |\n+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "We report the two best performance for slot filling, for which we trained one system without ontology and another without ontology and coarse-grained slot types (Acc.)",
        "evidence": "We report the results for the systems that employ our domain-adaptive delexcalization framework and domain-aware belief span modeling pipeline. The first group of columns shows the results of the systems that use our domain-adaptive delexcalization framework. The other three columns show the results of the systems that do not use our proposed domain-aware belief span modeling pipeline. Each system’s performance is assessed by two metrics: inform and success rate. The inform rate is the percentage of system utterances that are comprehensible and human-written in the target language, and the success rate is the ratio of utterances that are able to return their correct form in the target language. Our augmented data pipeline (DAMD) achieves the best performance, with more than 27% relative improvement in terms of inform and 13% relative improvement in terms of success rate over the state-of-the-art HDSA system without ontology augmentation. Moreover, even without multi-action data augmentation, our structured belief span modeling can still achieve significantly better performance than the state-of-the-art HDSA system, with more than 6% relative improvement in terms of inform and 7% relative improvement in terms of success rate.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "On the WinoCoref dataset, KnowComb does not improve by 15%.",
        "evidence": "Performance on Winograd and on the WinoCoref dataset is better than that on KnowComb. However, the results on the WinoCoref dataset are lower than those on the Illinois and KnowComb. Also, it is worth noting that the performance of KnowComb on the Illinois dataset is only slightly better than that on the WinoCoref dataset.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "On the other hand, compared to the BiLSTM baseline, PCS introduces significantly more in-scope (1,039) than out-of-scope (298) relations",
        "evidence": "The claim parser introduces significantly more in-scope relations than out-of-scope relations, which is consistent with our expectations. From the results, we can conclude that the PCS method is more effective for detecting corrections than the BiLSTM baseline.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, EWC does not outperform no-reg and L2 on News, as it only gives a 0.5 BLEU improvement over the baseline News model.",
        "evidence": "EWC outperforms No-Reg and L2 on TED and News, but performs slightly worse on IT.",
        "table": "+----+-----------+--------------------------+---------------+--------------+-------------+\n|    |   [EMPTY] | [BOLD] Training scheme   | [BOLD] News   | [BOLD] TED   | [BOLD] IT   |\n+====+===========+==========================+===============+==============+=============+\n|  0 |         1 | News                     | 37.8          | 25.3         | 35.3        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  1 |         2 | TED                      | 23.7          | 24.1         | 14.4        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  2 |         3 | IT                       | 1.6           | 1.8          | 39.6        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  3 |         4 | News and TED             | 38.2          | 25.5         | 35.4        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  4 |         5 | 1 then TED, No-reg       | 30.6          | [BOLD] 27.0  | 22.1        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  5 |         6 | 1 then TED, L2           | 37.9          | 26.7         | 31.8        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  6 |         7 | 1 then TED, EWC          | [BOLD] 38.3   | [BOLD] 27.0  | 33.1        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  7 |         8 | 5 then IT, No-reg        | 8.0           | 6.9          | 56.3        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  8 |         9 | 6 then IT, L2            | 32.3          | 22.6         | 56.9        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  9 |        10 | 7 then IT, EWC           | 35.8          | 24.6         | [BOLD] 57.0 |\n+----+-----------+--------------------------+---------------+--------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] As the results of applying the co-occurrence baseline (ρ = 0) shows (Table 2), the semantic relations in this data are not strongly concentrated within a sentence boundary, as evidenced by the relatively low F1 scores for the relation of TestTiming (0.90) and TestFinding (0.76).",
        "evidence": "As can be seen from the table, the semantic relations in the dataset are not strongly concentrated within a sentence boundary. Especially, the number of intra-sentential co-occurrence relations is significantly higher than that of other relations, which is consistent with our results on other datasets.",
        "table": "+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|    | Relation type                 |   Count | Intra-sentential co-occ.  [ITALIC] ρ=0   | Intra-sentential co-occ.  [ITALIC] ρ=5   |   Intra-sentential co-occ.  [ITALIC] ρ=10 | BoC(Wiki-PubMed-PMC) LR   | BoC(Wiki-PubMed-PMC) SVM   | BoC(Wiki-PubMed-PMC) ANN   |\n+====+===============================+=========+==========================================+==========================================+===========================================+===========================+============================+============================+\n|  0 | TherapyTiming(TP,TD)          |     428 | [BOLD] 0.84                              | 0.59                                     |                                      0.47 | 0.78                      | 0.81                       | 0.78                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  1 | NextReview(Followup,TP)       |     164 | [BOLD] 0.90                              | 0.83                                     |                                      0.63 | 0.86                      | 0.88                       | 0.84                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  2 | Toxicity(TP,CF/TR)            |     163 | [BOLD] 0.91                              | 0.77                                     |                                      0.55 | 0.85                      | 0.86                       | 0.86                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  3 | TestTiming(TN,TD/TP)          |     184 | 0.90                                     | 0.81                                     |                                      0.42 | 0.96                      | [BOLD] 0.97                | 0.95                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  4 | TestFinding(TN,TR)            |     136 | 0.76                                     | 0.60                                     |                                      0.44 | [BOLD] 0.82               | 0.79                       | 0.78                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  5 | Threat(O,CF/TR)               |      32 | 0.85                                     | 0.69                                     |                                      0.54 | [BOLD] 0.95               | [BOLD] 0.95                | 0.92                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  6 | Intervention(TP,YR)           |       5 | [BOLD] 0.88                              | 0.65                                     |                                      0.47 | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  7 | EffectOf(Com,CF)              |       3 | [BOLD] 0.92                              | 0.62                                     |                                      0.23 | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  8 | Severity(CF,CS)               |      75 | [BOLD] 0.61                              | 0.53                                     |                                      0.47 | 0.52                      | 0.55                       | 0.51                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  9 | RecurLink(YR,YR/CF)           |       7 | [BOLD] 1.0                               | [BOLD] 1.0                               |                                      0.64 | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 10 | RecurInfer(NR/YR,TR)          |      51 | 0.97                                     | 0.69                                     |                                      0.43 | [BOLD] 0.99               | [BOLD] 0.99                | 0.98                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 11 | GetOpinion(Referral,CF/other) |       4 | [BOLD] 0.75                              | [BOLD] 0.75                              |                                      0.5  | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 12 | Context(Dis,DisCont)          |      40 | [BOLD] 0.70                              | 0.63                                     |                                      0.53 | 0.60                      | 0.41                       | 0.57                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 13 | TestToAssess(TN,CF/TR)        |      36 | 0.76                                     | 0.66                                     |                                      0.36 | [BOLD] 0.92               | [BOLD] 0.92                | 0.91                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 14 | TimeStamp(TD,TP)              |     221 | [BOLD] 0.88                              | 0.83                                     |                                      0.5  | 0.86                      | 0.85                       | 0.83                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 15 | TimeLink(TP,TP)               |      20 | [BOLD] 0.92                              | 0.85                                     |                                      0.45 | 0.91                      | [BOLD] 0.92                | 0.90                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 16 | Overall                       |    1569 | 0.90                                     | 0.73                                     |                                      0.45 | 0.92                      | [BOLD] 0.93                | 0.91                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Compared to the original metapath2vec model with default d, by leveraging the right d, we improve performance at a better cost-efficiency.",
        "evidence": "This claim is supported by an extensive ablation study performed on the Metapath2Vec dataset. In particular, the effectiveness of the right d is of great importance. When the right d is 1, the model achieves the lowest AUC score but at the cost of the following computations: 4H time, 32 hours, 20 hours and 8 hours per annum. By increasing the right d, the model is able to achieve better efficiency at the cost of significantly more time.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
        "evidence": "From the results, our joint model still does not achieve the state-of-the-art performance on all the metrics. The CoNLL F1 score of our joint model is only slightly lower than the strong lemma baseline by 3.8 points.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our agent does not outperform the comparison agents with a large margin.",
        "evidence": "Comparison agents. Our agent does not outperform the comparison agents by a large margin. Among the comparison agents, our agent has 62% preference for the retrieval-stgy agent and has 22% prefer for the neural agent.",
        "table": "+----+----------------+------------------+----------------+-----------------+\n|    | [EMPTY]        | Ours Better(%)   |   No Prefer(%) |   Ours Worse(%) |\n+====+================+==================+================+=================+\n|  0 | Retrieval-Stgy | [BOLD] 62        |             22 |              16 |\n+----+----------------+------------------+----------------+-----------------+\n|  1 | PMI            | [BOLD] 54        |             32 |              14 |\n+----+----------------+------------------+----------------+-----------------+\n|  2 | Neural         | [BOLD] 60        |             22 |              18 |\n+----+----------------+------------------+----------------+-----------------+\n|  3 | Kernel         | [BOLD] 62        |             26 |              12 |\n+----+----------------+------------------+----------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "As the best comparison model, we investigate ablation models by removing parts from our model.",
        "evidence": "Ablation Study. We are interested in how different parts of our model affect the final results. Therefore, we conduct ablation studies by removing a component of our model. The results of the \"-Word-ATT\" row refers to the results without word-attention. We can see that our model still outperforms the word-attention baseline with a large margin, which demonstrates the effectiveness of the attention mechanism. The \"-capsule\" row refers to the results without using thecapsule embedding. We can see that our model still performs better than the word-attention baseline, which indicates the effectiveness of the character-level attention mechanism.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "However, our proposed method has comparable performance with the original GloVe embeddings.",
        "evidence": "We clearly observe that our proposed method performs better than the original GloVe embeddings. In particular, for the analg. (sem) dataset, our proposed method achieves 79.96% and 63.52% on average, while the GloVe embeddings averagely achieves 70.99% and 70.99% on the same dataset.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "supports"
    },
    {
        "claim": "In general, our principle P2 can improve all the models in any ablative condition (i.e., P1, P2, P1+P2), while P1 does not always lead to an improvement.",
        "evidence": "Note that complex models (e.g., P1, P2, P1+P2) are not always better than the baseline model, which might indicate that P1 does not improve all the models.",
        "table": "+----+----------+--------------+----------+----------+--------+\n|    | Model    | LF           |   HCIAE  |   CoAtt  |   RvA  |\n+====+==========+==============+==========+==========+========+\n|  0 | baseline | 57.21        |    56.98 |    56.46 |  56.74 |\n+----+----------+--------------+----------+----------+--------+\n|  1 | +P1      | 61.88        |    60.12 |    60.27 |  61.02 |\n+----+----------+--------------+----------+----------+--------+\n|  2 | +P2      | 72.65        |    71.5  |    71.41 |  71.44 |\n+----+----------+--------------+----------+----------+--------+\n|  3 | +P1+P2   | [BOLD] 73.63 |    71.99 |    71.87 |  72.88 |\n+----+----------+--------------+----------+----------+--------+",
        "label": "refutes"
    },
    {
        "claim": "By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning.",
        "evidence": "We only consider adjectives to show the effectiveness of our fine-tuning approach since the base model cannot. As expected, the flipped to negative works best while the base model is not as effective. This may be because the base model and the CNN model are trained on concepts that are used frequently by the claim, which is not the case in the flipped to positive case.",
        "table": "+----+---------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]             |   <bold>RNN</bold> |   <bold>CNN</bold> | <bold>DAN</bold>   |\n+====+=====================+====================+====================+====================+\n|  0 | Positive            |                9.7 |                4.3 | +<bold>23.6</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  1 | Negative            |                6.9 |                5.5 | +<bold>16.1</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  2 | Flipped to Positive |               20.2 |               24.9 | +27.4              |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  3 | Flipped to Negative |               31.5 |               28.6 | +19.3              |\n+----+---------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] TRANSFORMER-MULTI is stronger than TRANSFORMER-SINGLE [CONTINUE] .2% overall improvement over TRANSFORMER-SINGLE for the goldtwo-mention task.",
        "evidence": "The claim-embedding pairs extracted from Transformer-Multi are not better than Multi-Scale Transformer-single. This is because Multi-Scale Transformer can benefit from information from other levels, e.g., lexical, syntactic, and attribute features. So we do not remove the information from Multi-Scale Transformer for fair comparison. .2% overall improvement over Transformer-Single for the Goldtwo-mention task.",
        "table": "+----+--------------------+-------------+-------------+-----------------------+-------------+\n|    | [EMPTY]            | M           | F           | B                     | O           |\n+====+====================+=============+=============+=======================+=============+\n|  0 | Random             | 43.6        | 39.3        | [ITALIC] 0.90         | 41.5        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  1 | Token Distance     | 50.1        | 42.4        | [ITALIC] 0.85         | 46.4        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  2 | Topical Entity     | 51.5        | 43.7        | [ITALIC] 0.85         | 47.7        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  3 | Syntactic Distance | 63.0        | 56.2        | [ITALIC] 0.89         | 59.7        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  4 | Parallelism        | [BOLD] 67.1 | [BOLD] 63.1 | [ITALIC]  [BOLD] 0.94 | [BOLD] 65.2 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  5 | Parallelism+URL    | [BOLD] 71.1 | [BOLD] 66.9 | [ITALIC]  [BOLD] 0.94 | [BOLD] 69.0 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  6 | Transformer-Single | 58.6        | 51.2        | [ITALIC] 0.87         | 55.0        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  7 | Transformer-Multi  | 59.3        | 52.9        | [ITALIC] 0.89         | 56.2        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no significant effect on SER from cleaning the missed slots.",
        "evidence": "We observe that the removal of the slots only has a minor effect on the performance of the systems trained on the original dataset. This confirms our hypothesis that the less slots in the training data lead to a better performance of the model.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Systems A-C are trained without the target type from which they report.",
        "evidence": "The claim makes use of domain-aware belief state tracking and domain-aware belief span prediction. Seq2Seq + Attention and MD-sequicity provide the strong baselines. Our DAMD model significantly improves over these baselines when evaluated with generated system action forms across all evaluation metrics, although the improvement is marginal. One possible reason is that modeling distributional similarity of belief state spans appears to be beneficial more often than modeling specific target types. While multi-action data augmentation has the highest inform and success rates for systems A-C, it has the smallest impact on systems B-C.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO.",
        "evidence": "We compared the level of improvement achieved by the hybrid model against CMOW and CBOW in all tasks. In particular, we see that the performance of the hybrid model is on par with CMOW or slightly better than it when the latter is trained at the depth of 400. As the dimensionality of word embeddings increases, the improvements of the hybrid model over CMOW or CBOW shrinks, although they are still considerable. We can see that the largest gains from hybrid models are on the noun and verb analogy tasks (topconst and somo), where the performance of CBOW is roughly on par with that of the hybrid model. This is not surprising as it is common in this kind of tasks that the noun is usually far from the verb in the context.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "StateNet PSI does not outperform StateNet, and StateNet PS performs best among all 3 models.",
        "evidence": "The results show that StateNet PS does not outperform StateNet, and in fact StateNet PS performs best among all 3 models. StateNet PSI outperforms StateNet in all 3 cases, and StateNet PSI performs best among all 3 models. This indicates that the components of statenet PSI are effective for the task at hand.",
        "table": "+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|    | [BOLD] DST Models                                        | [BOLD] Joint Acc. DSTC2   | [BOLD] Joint Acc. WOZ 2.0   |\n+====+==========================================================+===========================+=============================+\n|  0 | Delexicalisation-Based (DB) Model Mrkšić et al. ( 2017 ) | 69.1                      | 70.8                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  1 | DB Model + Semantic Dictionary Mrkšić et al. ( 2017 )    | 72.9                      | 83.7                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  2 | Scalable Multi-domain DST Rastogi et al. ( 2017 )        | 70.3                      | -                           |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  3 | MemN2N Perez and Liu ( 2017 )                            | 74.0                      | -                           |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  4 | PtrNet Xu and Hu ( 2018 )                                | 72.1                      | -                           |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  5 | Neural Belief Tracker: NBT-DNN Mrkšić et al. ( 2017 )    | 72.6                      | 84.4                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  6 | Neural Belief Tracker: NBT-CNN Mrkšić et al. ( 2017 )    | 73.4                      | 84.2                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  7 | Belief Tracking: Bi-LSTM Ramadan et al. ( 2018 )         | -                         | 85.1                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  8 | Belief Tracking: CNN Ramadan et al. ( 2018 )             | -                         | 85.5                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  9 | GLAD Zhong et al. ( 2018 )                               | 74.5                      | 88.1                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n| 10 | StateNet                                                 | 74.1                      | 87.8                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n| 11 | StateNet_PS                                              | 74.5                      | 88.2                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n| 12 | [BOLD] StateNet_PSI                                      | [BOLD] 75.5               | [BOLD] 88.9                 |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "In this task, ATR and SRU outperform LRN in terms of both EM and F1 score.",
        "evidence": "From this table, we can see that ATR and SRU significantly outperform the LCN-based models in terms of both EM and F1 score. Specifically, ATR achieves an improvement of 2.27% EM and 2.65% F1 score over the SLR-based model, representing a relative improvement of 2.61%. In the base setting, atr and SRU achieve 2.59% and 1.65% improvements over the LSTM and GRU models respectively. Compared to the base model, 2.44% model still represents a considerable improvement. As for the F1 score, SRU achieves 2.50% improvement over the LSTM model with only 2.44M parameters compared to the 2.31M+2.31M model used by the GRU and ATR models.",
        "table": "+----+---------+-----------+---------------------+----------------------------+\n|    | Model   | #Params   | Base                | +Elmo                      |\n+====+=========+===========+=====================+============================+\n|  0 | rnet*   | -         | 71.1/79.5           | -/-                        |\n+----+---------+-----------+---------------------+----------------------------+\n|  1 | LSTM    | 2.67M     | [BOLD] 70.46/78.98  | 75.17/82.79                |\n+----+---------+-----------+---------------------+----------------------------+\n|  2 | GRU     | 2.31M     | 70.41/ [BOLD] 79.15 | 75.81/83.12                |\n+----+---------+-----------+---------------------+----------------------------+\n|  3 | ATR     | 1.59M     | 69.73/78.70         | 75.06/82.76                |\n+----+---------+-----------+---------------------+----------------------------+\n|  4 | SRU     | 2.44M     | 69.27/78.41         | 74.56/82.50                |\n+----+---------+-----------+---------------------+----------------------------+\n|  5 | LRN     | 2.14M     | 70.11/78.83         | [BOLD] 76.14/ [BOLD] 83.83 |\n+----+---------+-----------+---------------------+----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets. As Table 4 shows, previous models perform significantly better on the Easy subset than on the Hard subset, with the exception of Sasaki et al.",
        "evidence": "As shown, previous models perform significantly better on the Easy subset than on the Hard subset, with the exception of Sasaki et al.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] Words regularly describing negative sentiment or emotions (such as 'not', 'my', and 'can't') are among the most distinctive features for complaints.",
        "evidence": "Top unigrams regularly describing negative sentiment or emotions (such as 'Not', 'My', and 'Can't') are among the most distinctive features for complaints. Several other top unigrams (error, issue, working, and fix) are also observed in the table.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 only marginally boost the performance to around 87-88%, [CONTINUE] which is not significantly higher than the UnsupEmb and MFT baselines.",
        "evidence": "We see that the activation representations in the first layer of BERT is sufficient to slightly improve model performance. The activation representations in the second layer only marginally improve over the baseline. In general, the performance of our unsupemb and mft baselines is not significantly better than the unsupemb and mft baselines. Our results confirm that increasing the number of layers slightly improves model performance.",
        "table": "+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|    | [ITALIC] k           | Ar                   | Es                   | Fr                   | Ru                   | Zh                   | En                   |\n+====+======================+======================+======================+======================+======================+======================+======================+\n|  0 | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  1 | 0                    | 88.0                 | 87.9                 | 87.9                 | 87.8                 | 87.7                 | 87.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  2 | 1                    | 92.4                 | 91.9                 | 92.1                 | 92.1                 | 91.5                 | 89.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  3 | 2                    | 91.9                 | 91.8                 | 91.8                 | 91.8                 | 91.3                 | 88.3                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  4 | 3                    | 92.0                 | 92.3                 | 92.1                 | 91.6                 | 91.2                 | 87.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  5 | 4                    | 92.1                 | 92.4                 | 92.5                 | 92.0                 | 90.5                 | 86.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  6 | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  7 | 0                    | 81.9                 | 81.9                 | 81.8                 | 81.8                 | 81.8                 | 81.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  8 | 1                    | 87.9                 | 87.7                 | 87.8                 | 87.9                 | 87.7                 | 84.5                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  9 | 2                    | 87.4                 | 87.5                 | 87.4                 | 87.3                 | 87.2                 | 83.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 10 | 3                    | 87.8                 | 87.9                 | 87.9                 | 87.3                 | 87.3                 | 82.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 11 | 4                    | 88.3                 | 88.6                 | 88.4                 | 88.1                 | 87.7                 | 82.1                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 12 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 13 | [EMPTY]              | 32.7                 | 49.1                 | 38.5                 | 34.2                 | 32.1                 | 96.6                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "Dual2seq is not consistently better than the other systems under all three metrics, [CONTINUE] as OpenNMT-tf and Transformer-tf both outperform Dual2seq in terms of BLEU and Meteor scores.",
        "evidence": "We can see that Dual2seq is not always better than the other systems under all three metrics. For example, OpenNMT-tf and Transformer-tf both outperform Dual2seq in terms of BLEU and METEOR scores. It is also clear that, given the same set of hyperparameters, OpenNMT-tf and Transformer-tf outperform Dual2seq in terms of all three metrics.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "This is particularly noteworthy because our user simulator takes a very strict agenda (Section 4.1) compared to that of humans, which is more dynamic and changing as the conversation continues.",
        "evidence": "This is especially important because the user simulator follows a very strict agenda. The content of the responses is thus much more dynamic and changing as the conversation continues.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "( 2018 )) and the rank correlation between NeuralTD and human summaries is higher than with supervised models.",
        "evidence": "The Pearson correlation between neuralTD and the human summaries is higher than with supervised models..",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams.",
        "evidence": "The claim here is that TGen is as good as or better than SC-LSTM for matching rare and OOV words. We can see that the performance of TGen− is only slightly worse than TGen+ which was trained on the full dataset. This is reasonable because the NIST metric favors longer sequences and more frequent n-grams. While the ROUGE and CIDEr scores show very little difference between models trained on the original and the cleaned data, we point out that the BLEU score of BLEU of BLEU+ is significantly higher than the score of SC-LSTM trained on the original, which means that matching rare words is more important than matching OOV words. Similar to the results on the validation set, we can see that the attention mechanism of TGen+ outperforms TGen− which is trained on the full dataset. However, the improvement is much more significant for the model trained on the cleaned data (1-1[0.5pt/2pt]3-12), where TGen+ outperforms TGen− by as much as 2.3% on CIDEr.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The summaries generated by our system receive decent ROUGE metrics, but are lower than most of the recent systems, because our learned reward is optimised towards high correlation with human judgement instead of ROUGE metrics.",
        "evidence": "Table. Our system performs slightly better than most of the recent systems, but is significantly worse than Zhang et al. Our learned reward is towards high correlation with human judgement instead of ROUGE metrics.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "supports"
    },
    {
        "claim": "among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec do not achieve high ARI and Silhouette coefficient scores on the \"Video Games\" and \"Pornography\" datasets.",
        "evidence": "We compare DOD with baseline methods. We observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec, do not achieve high ARI and Sil scores. In contrast, OD achieves high ARI and Sil scores compared with the baseline methods. We also observe that the text-similarity based baselines, such as WMD and Doc2vec, outperform the baseline methods.",
        "table": "+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|    | Methods   | Seanad Abolition ARI   | Seanad Abolition  [ITALIC] Sil   | Video Games ARI   | Video Games  [ITALIC] Sil   | Pornography ARI   | Pornography  [ITALIC] Sil   |\n+====+===========+========================+==================================+===================+=============================+===================+=============================+\n|  0 | TF-IDF    | 0.23                   | 0.02                             | -0.01             | 0.01                        | -0.02             | 0.01                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  1 | WMD       | 0.09                   | 0.01                             | 0.01              | 0.01                        | -0.02             | 0.01                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  2 | Sent2vec  | -0.01                  | -0.01                            | 0.11              | 0.06                        | 0.01              | 0.02                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  3 | Doc2vec   | -0.01                  | -0.03                            | -0.01             | 0.01                        | 0.02              | -0.01                       |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  4 | BERT      | 0.03                   | -0.04                            | 0.08              | 0.05                        | -0.01             | 0.03                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  5 | OD-parse  | 0.01                   | -0.04                            | -0.01             | 0.02                        | 0.07              | 0.05                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  6 | OD        | [BOLD] 0.54            | [BOLD] 0.31                      | [BOLD] 0.56       | [BOLD] 0.42                 | [BOLD] 0.41       | [BOLD] 0.41                 |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE.",
        "evidence": "The results show that the proposed AAS method with weights wAC=1 and WAD=105 shows the lowest WER and DCE.",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 38.4        | 0.958        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 41.0        | 0.775        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 31.1        | [BOLD] 0.392 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 29.1        | 0.421        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 27.7        | 0.476        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 9.3         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] On the other hand, we found the quality of 3-step NLDs is relatively lower than the others.",
        "evidence": "On the one hand, we found that 3-step NLDs are relatively good. Based on the results, we can conclude that the quality of 3-step NLDs is relatively comparable to other models. On the other hand, we found that the information from all three NLDs is relatively weak, which may imply that NLDs in general are difficult to evaluate.",
        "table": "+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|    |   # steps |   Reachability |   Derivability Step 1 | Derivability Step 2   | Derivability Step 3   |\n+====+===========+================+=======================+=======================+=======================+\n|  0 |         1 |            3   |                   3.8 | -                     | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  1 |         2 |            2.8 |                   3.8 | 3.7                   | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  2 |         3 |            2.3 |                   3.9 | 3.8                   | 3.8                   |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "When the experiment was repeated so that the finetuning phase included the text-only data, the performance did not return to approximately the same level as without tuning (+multi-modal finetune row in Table 6).",
        "evidence": "We repeated the experiment so that the finetuning phase included the text-only data. The results are shown in Table 6. [CONTINUE] First, we can see that ensembling and multi-modal finetuning led to better performance. [CONTINUE] Second, fine-tuning with multi-modal finetune and multi-lingual features combined worked the best. [CONTINUE] Finally, we tried fine-tuning without multi-modal finetune but the performance dropped.",
        "table": "+----+----------------------------------------+--------------+--------------+--------------+\n|    | en-fr                                  | flickr16     | flickr17     | mscoco17     |\n+====+========================================+==============+==============+==============+\n|  0 | subs3M [ITALIC]  [ITALIC] LM detectron | 68.30        | 62.45        | 52.86        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  1 | +ensemble-of-3                         | 68.72        | 62.70        | 53.06        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  2 | −visual features                       | [BOLD] 68.74 | [BOLD] 62.71 | 53.14        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  3 | −MS-COCO                               | 67.13        | 61.17        | [BOLD] 53.34 |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  4 | −multi-lingual                         | 68.21        | 61.99        | 52.40        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  5 | subs6M [ITALIC]  [ITALIC] LM detectron | 68.29        | 61.73        | 53.05        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  6 | subs3M [ITALIC]  [ITALIC] LM gn2048    | 67.74        | 61.78        | 52.76        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  7 | subs3M [ITALIC]  [ITALIC] LM text-only | 67.72        | 61.75        | 53.02        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  8 | en-de                                  | flickr16     | flickr17     | mscoco17     |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  9 | subs3M [ITALIC]  [ITALIC] LM detectron | 45.09        | 40.81        | 36.94        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 10 | +ensemble-of-3                         | 45.52        | [BOLD] 41.84 | [BOLD] 37.49 |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 11 | −visual features                       | [BOLD] 45.59 | 41.75        | 37.43        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 12 | −MS-COCO                               | 45.11        | 40.52        | 36.47        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 13 | −multi-lingual                         | 44.95        | 40.09        | 35.28        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 14 | subs6M [ITALIC]  [ITALIC] LM detectron | 45.50        | 41.01        | 36.81        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 15 | subs3M [ITALIC]  [ITALIC] LM gn2048    | 45.38        | 40.07        | 36.82        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 16 | subs3M [ITALIC]  [ITALIC] LM text-only | 44.87        | 41.27        | 36.59        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 17 | +multi-modal finetune                  | 44.56        | 41.61        | 36.93        |\n+----+----------------------------------------+--------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "(2017), we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set.",
        "evidence": "(in percentages of total tweets). We find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech and 2% of those in the white-aligned set being classified as hate speech. In both cases, these disparities are statistically significant (p < 0.001, and t < 0.001, for Waseem and Hovy (2016) and Waseem (2016). For Davidson et al. (2017) and Golbeck et al. (2017), we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech and 2% of tweets in the white-aligned set classified as hate speech. In case of Waseem and Hovy (2016), we find there is no significant difference in the estimated rates at which tweets are clas [CONTINUE] sified as racist across groups, although the rates remain low.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data.",
        "evidence": "It should be noted that the CNN/Daily Mail dataset is even larger than the Multi-News dataset. In particular, the number of documents (documents) in Multi-News is two orders of magnitude larger than the CNN/Daily Mail dataset. The total number of words (summary) in our Multi-News dataset is also two orders of magnitude larger than the previous MDS news data.",
        "table": "+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|    | [BOLD] Dataset   | [BOLD] # pairs        | [BOLD] # words (doc)   |   [BOLD] # sents (docs) |   [BOLD] # words (summary) |   [BOLD] # sents (summary) | [BOLD] vocab size   |\n+====+==================+=======================+========================+=========================+============================+============================+=====================+\n|  0 | Multi-News       | 44,972/5,622/5,622    | 2,103.49               |                   82.73 |                     263.66 |                       9.97 | 666,515             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  1 | DUC03+04         | 320                   | 4,636.24               |                  173.15 |                     109.58 |                       2.88 | 19,734              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  2 | TAC 2011         | 176                   | 4,695.70               |                  188.43 |                      99.7  |                       1    | 24,672              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  3 | CNNDM            | 287,227/13,368/11,490 | 810.57                 |                   39.78 |                      56.2  |                       3.68 | 717,951             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "Despite LRN and oLRN having faster training times than SRU (+15%/+6%), SRU still achieves a higher BLEU score.",
        "evidence": "Although SRU achieves a higher BLEU score than the other models, it takes significantly more training time than LRN and ATR. In particular, the gap is at 1.34 BLEU points for SRU and 1.15 BLEU points for LRN. On the other hand, we observe that ATR has the fastest training time (1.33M) but the lowest BLEU score.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Additionally, when using bounding box features, sparsemax outperforms softmax, showing that selecting only the bounding boxes of the relevant objects leads to a better answering capability.",
        "evidence": "As we can see, sparsemax and sparse-TVMAX both outperform softmax in terms of accuracy, showing that better bounding box selection can indeed lead to a better answering capability. Moreover, the results are slightly superior for the bounding box features, showing that selecting only the relevant objects leads to a better answering capability.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "The UnsupEmb baseline performs comparably to the Word2Tag upper bound on both POS and SEM tagging.",
        "evidence": "The main takeout is that unsupemb provides a generally better baseline for POS tagging than word2tag.",
        "table": "+----+-----------+-------+------------+------------+\n|    | [EMPTY]   |   MFT |   UnsupEmb |   Word2Tag |\n+====+===========+=======+============+============+\n|  0 | POS       | 91.95 |      87.06 |      95.55 |\n+----+-----------+-------+------------+------------+\n|  1 | SEM       | 82    |      81.11 |      91.41 |\n+----+-----------+-------+------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "POS-disambiguation does not fragment the vocabulary and consistently increases the coverage with the effect being more pronounced for lemmatized targets.",
        "evidence": "We claim that the disambiguation of lemmatized targets is not noise-reduced which is consistent with previous work. The disambiguation of X+POS is consistently better than that of lemmatized targets for all three target types VN, WN-V and WN-N. The effect is especially pronounced for WN-N where the coverage is more sparse than those of other targets.",
        "table": "+----+----------+------+--------+--------+\n|    | target   |   VN |   WN-V |   WN-N |\n+====+==========+======+========+========+\n|  0 | type     |   81 |     66 |     47 |\n+----+----------+------+--------+--------+\n|  1 | x+POS    |   54 |     39 |     43 |\n+----+----------+------+--------+--------+\n|  2 | lemma    |   88 |     76 |     53 |\n+----+----------+------+--------+--------+\n|  3 | x+POS    |   79 |     63 |     50 |\n+----+----------+------+--------+--------+\n|  4 | shared   |   54 |     39 |     41 |\n+----+----------+------+--------+--------+",
        "label": "refutes"
    },
    {
        "claim": "In most cases the racial disparities persist, and are generally larger in magnitude than the disparities for other classes.",
        "evidence": "The results show that in all but one of the comparisons, racial disparities persist in all the tasks regardless of whether tweets are labeled as sexism or not. The only case where disparities are smaller than those for other classes is for the Waseem and Hovy (2016) classifier. Here we see that, although the Waseem and Hovy (2016) classifier is similar to the Davidson et al. (2017) classifier, the racism and sexism disparities are calculated using the measure of tweets in the black-aligned corpus, not in the white-aligned corpus. Note, for example, that the baseline for the Waseem (2016) classifier is trained using the black-aligned corpus only, whereas we use the white-aligned corpora for our experiments. We see also that the disparities are larger in the Waseem and Hovy (2016) classifier than in the Davidson et al. (2017) classifier, which is similar to the situation we saw with the Golbeck et al. (2017) classifier.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "CorefProp also improves relation extraction on SciERC.",
        "evidence": "CorefProp also improves relation extraction on SciERC. [CONTINUE] Relation propagation (RelProp) improves relation extraction over pretrained BERT, but does not improve fine-tuned BERT.",
        "table": "+----+---------------+-------------+-------------+-------------+\n|    | [EMPTY]       | ACE05       | SciERC      | WLPC        |\n+====+===============+=============+=============+=============+\n|  0 | BERT + LSTM   | 60.6        | 40.3        | 65.1        |\n+----+---------------+-------------+-------------+-------------+\n|  1 | +RelProp      | 61.9        | 41.1        | 65.3        |\n+----+---------------+-------------+-------------+-------------+\n|  2 | +CorefProp    | 59.7        | 42.6        | -           |\n+----+---------------+-------------+-------------+-------------+\n|  3 | BERT FineTune | [BOLD] 62.1 | 44.3        | 65.4        |\n+----+---------------+-------------+-------------+-------------+\n|  4 | +RelProp      | 62.0        | 43.0        | [BOLD] 65.5 |\n+----+---------------+-------------+-------------+-------------+\n|  5 | +CorefProp    | 60.0        | [BOLD] 45.3 | -           |\n+----+---------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "LRN obtains an accuracy of 90.49 with BERT, the highest among all models.",
        "evidence": "The claim: Our RLN obtains an accuracy of 90.49 with BERT, the highest among all models.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively.",
        "evidence": "From the table, we can see that G2S-GGNN and G2S-GAT have 33.5% and 5.2% better entailment performances than S2S when Ref is Entailment and Gen is Entailment, respectively. Moreover, the results on Entailment are better than those on Gen in all the cases. We can also see that S2S and G2S-GAT have comparable performances when coming to terms of Entailment. However, our results show that G2S-GGNN and G2S-GAT have better self-regression than S2S when coming to terms of Entailment. The reason is that the attention mechanism does not play much of a role in the G2S-GAT model.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The intuition here is that each model is optimizing different signals (lexical matching and type accuracy), which may or may not be independent.",
        "evidence": "Cluster+KCP is inferior to all the baselines, which is consistent with results from \\newciteCUBulska2015a. The intuition here is that the KCP is optimizing two signals (lexical matching and type accuracy) that may or may not be independent. The CLUSTER+KCP model performs very close to the State-Of-The-Art model, which is also trained with lemma annotations. Our proposed joint model outperforms all the single-channel baselines, including the strong lemma baseline, by a substantial margin.",
        "table": "+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|    | [BOLD] Model                     | R       | MUC P   | [ITALIC] F1   | B3 P    | CEAF- [ITALIC] e P   | CoNLL  [ITALIC] F1   |\n+====+==================================+=========+=========+===============+=========+======================+======================+\n|  0 | [BOLD] Baselines                 | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  1 | Cluster+Lemma                    | 75.5    | 79.9    | 73.6          | 85      | 71.7                 | 76.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  2 | CV Cybulska and Vossen ( 2015a ) | -       | 75      | 64            | 78      | -                    | 73                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  3 | KCP Kenyon-Dean et al. ( 2018 )  | 71      | 71      | 69            | 67      | 67                   | 69                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  4 | Cluster+KCP                      | 77.4    | 79.3    | 71.5          | 87.2    | 66.4                 | 73.6                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  5 | [BOLD] Model Variants            | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  6 | Disjoint                         | 80.3    | 83.6    | 75.9          | 86      | 71.9                 | 78.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  7 | Joint                            | 81      | 84.5    | 77.3          | 85.1    | 73.8                 | [BOLD] 79.5          |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual.",
        "evidence": "We can see that the fine-tuned-disc model performs the best when the gold sentence is monolingual. Fine-tuned-LM can only reach 46.80% on the development data and 43.20% on the test data, but there is still improvement compared to the monolingual version. This shows the superiority of the Disc-Net model over the language-only model.",
        "table": "+----+-----------------+--------------+--------------+--------------+--------------+\n|    | [EMPTY]         | dev CS       | dev mono     | test CS      | test mono    |\n+====+=================+==============+==============+==============+==============+\n|  0 | CS-only-LM      | 45.20        | 65.87        | 43.20        | 62.80        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  1 | Fine-Tuned-LM   | 49.60        | 72.67        | 47.60        | 71.33        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  2 | CS-only-disc    | [BOLD] 75.60 | 70.40        | 70.80        | 70.53        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  3 | Fine-Tuned-disc | 70.80        | [BOLD] 74.40 | [BOLD] 75.33 | [BOLD] 75.87 |\n+----+-----------------+--------------+--------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "According to the table, the drop of precision demonstrates that the word-level attention is quite useful.",
        "evidence": "This table shows that the word-level attention is quite useful.. From the recall values of the -word-att model, we can observe that the drop of precision demonstrates that the word-level attention is quite useful.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "supports"
    },
    {
        "claim": "While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points higher scores on WordContent and BigramShift.",
        "evidence": "While both models perform comparably on most tasks, CMOW-C yields 5 points higher scores on WordContent and BigramShift.",
        "table": "+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method   | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+==========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CMOW-C   | [BOLD] 36.2 | 66.0        | 81.1        | 78.7        | 61.7        | [BOLD] 83.9 | 79.1        | 73.6        | 50.4        | 66.8        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW-R   | 35.1        | [BOLD] 70.8 | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | [BOLD] 74.2 | [BOLD] 50.7 | [BOLD] 72.9 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | CBOW-C   | [BOLD] 34.3 | [BOLD] 50.5 | [BOLD] 79.8 | [BOLD] 79.9 | 53.0        | [BOLD] 75.9 | [BOLD] 79.8 | [BOLD] 72.9 | 48.6        | 89.0        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | CBOW-R   | 33.0        | 49.6        | 79.3        | 78.4        | [BOLD] 53.6 | 74.5        | 78.6        | 72.0        | [BOLD] 49.6 | [BOLD] 89.5 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations.",
        "evidence": "We observe similar trends as before: POS tagging does not benefit from features from the upper layers, while Sem tagging improves with layer 4 representations. We believe this is because deeper layers can capture higher-level structure, which aligns with the information from the bottom layers.",
        "table": "+----+-------+-------+----------+----------+----------+----------+----------+\n|    | Uni   | POS   |   0 87.9 |   1 92.0 |   2 91.7 |   3 91.8 |   4 91.9 |\n+====+=======+=======+==========+==========+==========+==========+==========+\n|  0 | Uni   | SEM   |     81.8 |     87.8 |     87.4 |     87.6 |     88.2 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  1 | Bi    | POS   |     87.9 |     93.3 |     92.9 |     93.2 |     92.8 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  2 | Bi    | SEM   |     81.9 |     91.3 |     90.8 |     91.9 |     91.9 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  3 | Res   | POS   |     87.9 |     92.5 |     91.9 |     92   |     92.4 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  4 | Res   | SEM   |     81.9 |     88.2 |     87.5 |     87.6 |     88.5 |\n+----+-------+-------+----------+----------+----------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "After removing the graph attention module, our model gives 24.9 BLEU points.",
        "evidence": "It shows that: (1) After removing the graph attention module, the model gives 24.9 BLEU points, which is decreased from 25.5 to 24.2 for B and from 53.7 to 54.4 for C; (2) After removing both the linear combination and the graph attention module, the model gives 23.7 BLEU points, which is decreased from 23.7 to 23.2 and from 53.2 to 53.0 for B and C; (3) When we remove both the global node and the coverage mechanism, the model gives 23.8 BLEU points, which is decreased from 53.0 to 53.0. [CONTINUE] From these results, we can conclude that the coverage mechanism is the most important module.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "the distribution on dialog success criteria with ACER has the least bias among all.",
        "evidence": "The claim that ACER has the least bias among all dialog systems is quite strong. That is, ACER achieved 55% task success rate and PPO got 74% task success rate. In contrast, the highest quality and success rate for ALDM was only 19% and 24% respectively.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that for the NYT10 dataset, m = {1, 2, 3} gives good performance with m = 1 achieving the highest F1 score.",
        "evidence": "We investigate the impact of the number of matching entities on the F1 score using the InferSent baseline. We observe that for the NYT10 dataset, m = {1, 2, 3} gives good performance with m = 1 achieving the highest F1 score. Similarly, we observe that for the NYT11 dataset, m = 4 gives good performance. These results demonstrate the effectiveness of our multi-factor matching scheme.",
        "table": "+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|    |   [ITALIC] m |   NYT10 Prec. |   NYT10 Rec. | NYT10 F1     |   NYT11 Prec. |   NYT11 Rec. | NYT11 F1     |\n+====+==============+===============+==============+==============+===============+==============+==============+\n|  0 |            1 |         0.541 |        0.595 | [BOLD] 0.566 |         0.495 |        0.621 | 0.551        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  1 |            2 |         0.521 |        0.597 | 0.556        |         0.482 |        0.656 | 0.555        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  2 |            3 |         0.49  |        0.617 | 0.547        |         0.509 |        0.633 | 0.564        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  3 |            4 |         0.449 |        0.623 | 0.522        |         0.507 |        0.652 | [BOLD] 0.571 |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  4 |            5 |         0.467 |        0.609 | 0.529        |         0.488 |        0.677 | 0.567        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "(2017).8 Overall both BERT (76.5%) and [CONTINUE] RoBERTa (87.7%) considerably outperform the best previous model (71.4%).",
        "evidence": "(Sasaki et al., et al. As it can be seen, for overall accuracy BERT and RoBERTa achieve much better results than the previous best model, \\newciteGordon_Commonsense_2011-1, which usedPMI. More importantly, the improvements in accuracy on the hard data seem to be much larger than expected, indicating that our approach is able to address the hard data problem much better than previous unsupervised approaches. Due to word frequency differences, RoBERTa-large-FT slightly underperforms BERT-large-FT. However, given its very large corpus size, the NSP differences are not statistically significant.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources.",
        "evidence": "Beck et al. In particular, our model is superior when trained without external resources. Our model improves the BLEU score by 7.5 points when trained without external resources, from 49.1 to 57.3.Beck et al.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The larger performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely more on superficial cues.",
        "evidence": "We suspect that the large performance gap between Easy and Hard subsets indicates that training on B-COPA encourages BERT and RoBERTa to rely more on superficial cues. Indeed, training on B-COPA improves performance on the Hard subset, both for BERT and RoBERTa. Moreover, training on B-COPA improves the performance on the Easy subset less than training on the Hard subset, indicating that training on Easy subsets improves BERT and RoBERTa’s ability to rely on superficial cues.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "For German descriptions, The results are 11.05% worse on average compared to (Gella et al., 2017) in symmetric mode.",
        "evidence": "For German descriptions, the results are 11.05% worse on average compared to (Gella et al., 2017) in symmetric mode. AME also outperforms the FME model more significantly than the other models.",
        "table": "+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|    | [EMPTY]           | Image to Text R@1   | Image to Text R@5   | Image to Text R@10   | Image to Text Mr   | Text to Image R@1   | Text to Image R@5   | Text to Image R@10   | Text to Image Mr   | Alignment   |\n+====+===================+=====================+=====================+======================+====================+=====================+=====================+======================+====================+=============+\n|  0 | [BOLD] symmetric  | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  1 | Parallel gella:17 | 28.2                | 57.7                | 71.3                 | 4                  | 20.9                | 46.9                | 59.3                 | 6                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  2 | Mono              | 34.2                | 67.5                | 79.6                 | 3                  | 26.5                | 54.7                | 66.2                 | 4                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  3 | FME               | 36.8                | 69.4                | 80.8                 | 2                  | 26.6                | 56.2                | 68.5                 | 4                  | 76.81%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  4 | AME               | [BOLD] 39.6         | [BOLD] 72.7         | [BOLD] 82.7          | [BOLD] 2           | [BOLD] 28.9         | [BOLD] 58.0         | [BOLD] 68.7          | [BOLD] 4           | 66.91%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  5 | [BOLD] asymmetric | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  6 | Pivot gella:17    | 28.2                | 61.9                | 73.4                 | 3                  | 22.5                | 49.3                | 61.7                 | 6                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  7 | Parallel gella:17 | 30.2                | 60.4                | 72.8                 | 3                  | 21.8                | 50.5                | 62.3                 | 5                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  8 | Mono              | [BOLD] 42.0         | 72.5                | 83.0                 | 2                  | 29.6                | 58.4                | 69.6                 | 4                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  9 | FME               | 40.5                | 73.3                | 83.4                 | 2                  | 29.6                | 59.2                | [BOLD] 72.1          | 3                  | 76.81%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 10 | AME               | 40.5                | [BOLD] 74.3         | [BOLD] 83.4          | [BOLD] 2           | [BOLD] 31.0         | [BOLD] 60.5         | 70.6                 | [BOLD] 3           | 73.10%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0.",
        "evidence": "We performed experiments with our best model on the development set. Specifically, we gradually increased the number of masked words in each block from n to 6. We can see that the performance first stagnates when n is fixed at 1. Then the performance starts to increase again, reaching the best value for m=6 in the third block. However, after reaching m=6, the performance starts to drop, although still higher than our previous best result. This indicates that the current technique is already good enough for being applied to the production dataset.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    |   [ITALIC] Block |   [ITALIC] n |   [ITALIC] m | B           | C           |\n+====+==================+==============+==============+=============+=============+\n|  0 |                1 |            1 |            1 | 17.6        | 48.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 |                1 |            1 |            2 | 19.2        | 50.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 |                1 |            2 |            1 | 18.4        | 49.1        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 |                1 |            1 |            3 | 19.6        | 49.4        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 |                1 |            3 |            1 | 20.0        | 50.5        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 |                1 |            3 |            3 | 21.4        | 51.0        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 |                1 |            3 |            6 | 21.8        | 51.7        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 |                1 |            6 |            3 | 21.7        | 51.5        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 |                1 |            6 |            6 | 22.0        | 52.1        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 |                2 |            3 |            6 | [BOLD] 23.5 | 53.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 |                2 |            6 |            3 | 23.3        | [BOLD] 53.4 |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 |                2 |            6 |            6 | 22.0        | 52.1        |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "However, the sdp information does not have a clear positive impact on all the relation types (Table 1).",
        "evidence": "This claim is supported by the best results of our baseline models, which achieve an F1-score of 50.10. Macro-averaged F1-score values indicate that the improvement is relatively large. The results clearly show that the use of SDD does not significantly improve the performance of our baselines. This is probably due to the fact that pre-training and fine-tuning SDD already has a good performance. The feature embedding has already been learned, making the use of new features redundant. Furthermore, most of the features in the baseline models are not sensitive to the dimension of the hidden vector, which also indicates that we can use the pre-trained embedding as the initialization for the new features. However, incorporating the IDF information into the baseline models is not enough to fully understand the usefulness of SDD.",
        "table": "+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|    | [BOLD] Relation   |   [BOLD] best F1 (in 5-fold) without sdp |   [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.   |\n+====+===================+==========================================+=======================================+================+\n|  0 | USAGE             |                                    60.34 |                                 80.24 | + 19.90        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  1 | MODEL-FEATURE     |                                    48.89 |                                 70    | + 21.11        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  2 | PART_WHOLE        |                                    29.51 |                                 70.27 | +40.76         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  3 | TOPIC             |                                    45.8  |                                 91.26 | +45.46         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  4 | RESULT            |                                    54.35 |                                 81.58 | +27.23         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  5 | COMPARE           |                                    20    |                                 61.82 | + 41.82        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  6 | macro-averaged    |                                    50.1  |                                 76.1  | +26.00         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "Reward 3, i.e., preference between generated summaries and reference, has slightly higher correlations with system performance than Reward 1, i.e., difference in summary properties from statistical values computed on references (regression loss in Eq.",
        "evidence": "Table 3 shows that the preference of system performance over reference summaries is slightly correlated with that of human preference. Specifically, MLP with BERT as encoder has the highest correlation (p < 0.05) whereas that of the other models (SimRED, MLP and G-Rec) is only slightly better than the baseline (Peyrard and Gurevych (2018)).",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data.",
        "evidence": "We claim that our DCGCN model achieves a BLEU score of 33.2, which is only 0.1 BLEU points higher than the previous state of the art without external data. Moreover, DCGCN(ensemble) achieves an even better score of 35.3, which is, again, 0.1 BLEU points higher than the previous state of the art. Note that we used less data for DCGCN than other baselines.",
        "table": "+----+------------------------------------+-------------------+-------------+\n|    | [BOLD] Model                       | [BOLD] External   | B           |\n+====+====================================+===================+=============+\n|  0 | Seq2SeqK (Konstas et al.,  2017 )  | -                 | 22.0        |\n+----+------------------------------------+-------------------+-------------+\n|  1 | GraphLSTM (Song et al.,  2018 )    | -                 | 23.3        |\n+----+------------------------------------+-------------------+-------------+\n|  2 | GCNSEQ (Damonte and Cohen,  2019 ) | -                 | 24.4        |\n+----+------------------------------------+-------------------+-------------+\n|  3 | DCGCN(single)                      | -                 | 25.9        |\n+----+------------------------------------+-------------------+-------------+\n|  4 | DCGCN(ensemble)                    | -                 | [BOLD] 28.2 |\n+----+------------------------------------+-------------------+-------------+\n|  5 | TSP (Song et al.,  2016 )          | ALL               | 22.4        |\n+----+------------------------------------+-------------------+-------------+\n|  6 | PBMT (Pourdamghani et al.,  2016 ) | ALL               | 26.9        |\n+----+------------------------------------+-------------------+-------------+\n|  7 | Tree2Str (Flanigan et al.,  2016 ) | ALL               | 23.0        |\n+----+------------------------------------+-------------------+-------------+\n|  8 | SNRG (Song et al.,  2017 )         | ALL               | 25.6        |\n+----+------------------------------------+-------------------+-------------+\n|  9 | Seq2SeqK (Konstas et al.,  2017 )  | 0.2M              | 27.4        |\n+----+------------------------------------+-------------------+-------------+\n| 10 | GraphLSTM (Song et al.,  2018 )    | 0.2M              | 28.2        |\n+----+------------------------------------+-------------------+-------------+\n| 11 | DCGCN(single)                      | 0.1M              | 29.0        |\n+----+------------------------------------+-------------------+-------------+\n| 12 | DCGCN(single)                      | 0.2M              | [BOLD] 31.6 |\n+----+------------------------------------+-------------------+-------------+\n| 13 | Seq2SeqK (Konstas et al.,  2017 )  | 2M                | 32.3        |\n+----+------------------------------------+-------------------+-------------+\n| 14 | GraphLSTM (Song et al.,  2018 )    | 2M                | 33.6        |\n+----+------------------------------------+-------------------+-------------+\n| 15 | Seq2SeqK (Konstas et al.,  2017 )  | 20M               | 33.8        |\n+----+------------------------------------+-------------------+-------------+\n| 16 | DCGCN(single)                      | 0.3M              | 33.2        |\n+----+------------------------------------+-------------------+-------------+\n| 17 | DCGCN(ensemble)                    | 0.3M              | [BOLD] 35.3 |\n+----+------------------------------------+-------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "their recall are 0.595, 0.517, and 0.441 on three thresholds 0.1, 0.2 and 0.3 respectively, while our model achieves 0.650, 0.519, 0.422.",
        "evidence": "The experimental results are summarized in Table 3. Our model outperforms the baselines significantly. [CONTINUE] The results of \"-Word-ATT\" row refers to the results without word embeddings. According to the table, the relative performance of our model against -Word-ATT is higher than that of the baselines.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "this impressive improvement comes from the large dataset and considerable time spent on hyperparameter tuning, but only better-than-human results compared to RoBERTa and BERT finetuning.",
        "evidence": "This improvement comes from the large dataset and considerable time spent on hyperparameter tuning. BERT-large is shown to be significantly better than RoBERTa and BERTLARGE.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "acoustic supervision (27.7%) and multi-task learning (26.1%) show higher WER than minimizing DCE (31.1%) and FSEGAN (29.1%).",
        "evidence": "The claim makes the claim that multi-task learning with acoustic supervision and multi-task learning with FSEGAN show better results than minimizing DCE and multi-task learning with FSEGAN. We chose to focus on the acoustic supervision method (AAS) after noticing that FSEGAN and Wiener filter shows worse results than minimizing DCE and multi-task learning. We can see that AAS (with wAC = 0.4, wAD > 0.5) performs similar to minimizing DCE (i.e. acoustic supervision) and outperforms FSEGAN (i.e. minimizing DCE) in terms of WER.",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 38.4        | 0.958        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 41.0        | 0.775        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 31.1        | [BOLD] 0.392 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 29.1        | 0.421        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 27.7        | 0.476        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 9.3         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Furthermore, it also yields better policy matches, except for PPO, suggesting that GDPL is more compatible with the real users.",
        "evidence": "The ALDM and ACER approaches are designed to measure and compare the performance of a dialog system in a dialog environment. GDPL outperforms the other methods in all the measures. It is also observed that GDPL is more compatible with the real users than the other methods. This means that GDPL can generate more natural and accurate responses. In contrast, the other methods tend to generate safe responses like “I don’t know” or simply copying from the source.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "This is particularly true for the BIDAF model.",
        "evidence": "This is especially true for the BiDAF model, which only achieves around 49.98% F1 in the in-domain setting and 49.76% in the out-of-domain setting. The difference between the in-domain and out-of-domain results of BiDAF (ELMo) is significantly large, which shows that the model does not generalize well to unseen data.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "therefore, the role of attention in link prediction can be explained.",
        "evidence": "The results clearly indicate that attention has a significant role in link prediction, as PCNN+ATT (1) and PCNN+ATT (m) significantly outperform rank+exatt, which indicates that the link prediction can be further improved by exploiting the attention mechanism.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "In addition, our single DCGCN model obtains better results than previous ensemble models.",
        "evidence": "Asbeck et al. From the table, we can see that our single DCGCN model achieves the best results in all of the three measures, improving over the state-of-the-art by 41.1%, 19.1% and 30.4% for tagging T, B and C, respectively. In addition, our single model obtains the best results in all three measures, with an advantage over the previous ensemble methods by 7.2%, 6.1% and 17.3% respectively. This result confirms the superiority of the GCN model in capturing structural information. The attention mechanism also plays an important role in our model.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "According to Pearson correlation, gr def model had the highest correlation with human ratings of similarity.",
        "evidence": "According to Pearson correlation, the model described by the best model was GR def. It had the highest correlation with human ratings of similarity.",
        "table": "+----+---------------+---------------+-----------+-------------------+\n|    | Model         | Pearson       |   p-value | Pairs (unknown)   |\n+====+===============+===============+===========+===================+\n|  0 | gr_def        | [BOLD] 0.6042 |   3.1e-35 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  1 | gr_neg10      | 0.5973        |   2.9e-34 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  2 | cc.el.300     | 0.5311        |   1.7e-25 | 4.9%              |\n+----+---------------+---------------+-----------+-------------------+\n|  3 | wiki.el       | 0.5812        |   2.2e-31 | 4.5%              |\n+----+---------------+---------------+-----------+-------------------+\n|  4 | gr_cbow_def   | 0.5232        |   2.7e-25 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  5 | gr_d300_nosub | 0.5889        |   3.8e-33 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  6 | gr_w2v_sg_n5  | 0.5879        |   4.4e-33 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "SciBERT significantly boosts performance for scientific datasets including SciERC and GENIA.",
        "evidence": " SciBERT significantly boosts performance for scientific datasets including SciERC and GENIA.",
        "table": "+----+--------------+-----------------+-------------------+----------------+\n|    | [EMPTY]      | SciERC Entity   | SciERC Relation   | GENIA Entity   |\n+====+==============+=================+===================+================+\n|  0 | Best BERT    | 69.8            | 41.9              | 78.4           |\n+----+--------------+-----------------+-------------------+----------------+\n|  1 | Best SciBERT | [BOLD] 72.0     | [BOLD] 45.3       | [BOLD] 79.5    |\n+----+--------------+-----------------+-------------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments [CONTINUE] From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature.",
        "evidence": "The results from this section support our claim. In particular, we find strong, sentence-level Spearman’s ρ correlations between the two scores and human judgments on the Yelp and Lit. datasets. The strong correlations also indicate that the IR-based attribution model is capable of measuring the quality of human judgments.",
        "table": "+----+----------+---------------------------------------------------------------------------+--------+--------+\n|    | Metric   | Method of validation                                                      |   Yelp |   Lit. |\n+====+==========+===========================================================================+========+========+\n|  0 | Acc      | % of machine and human judgments that match                               |  94    |  84    |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  1 | Sim      | Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation |   0.79 |   0.75 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  2 | PP       | Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency       |   0.81 |   0.67 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] However, words regularly describing negative sentiment or emotions are not one of the most distinctive features for complaints.",
        "evidence": "Next, we use unigrams (unigrams and part-of-speech unigrams) to analyze the source of complaints. Generally, words regularly describing negative sentiment or emotions are not one of the most distinctive features for complaints. This is consistent with the results of \\newciteHernault:2010 :SMD:2175352.2175383.1. where the focus on nouns is more on adjectives and proper nouns. 3) No comment words are uncovered through unigrams (not, no, won't) several unigrams (error, issue, working, fix)",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "We observe that BERT trained on Balanced COPA is more sensitive to a few highly productive superficial cues than BERT trained on original COPA.",
        "evidence": "From the table, we can see that BERT trained on balanced COPA is more sensitive to a few highly productive superficial cues than BERT trained on original COPA. Note that these cues are fairly frequent: for example, we counted the number of “positive” and “negative” cues for “woman” and “mother”; and we counted the number of “proper COPA” cues for “went” and “into”.",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "our model has much better quality over the extractive summarization system in three aspects.",
        "evidence": "From the table, we can see that our system is much better than the extractive summarization system in three aspects: (1) with a human rating of 2.52, 2.27 and 1.66, the refresh score of 3.0 and the best score of 7.7 are achieved. Based on these results, we can conclude that our model is much better than the extractive summarization system.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "For example, DCGCN4 contains 36 layers.",
        "evidence": "The results in the table are not directly comparable due to different tokenization methods. However, GCN+RNN+CNN (3) performs better than GCN+CNN (2); GCN+RNN+CNN (4) is better than GCN+CNN (2); and GCN+RNN+CNN (6) is the best among all. Interestingly, the best result of GCN+CNN (9) is still better than the best result of DCGCN2 (18). This can be explained by the fact that the attention mechanism in DCGCN4 has 36 layers. Due to the attention mechanism, the final representation of the input is collected at each layer. Therefore, the attention mechanism at each layer takes into account more information than GCNs due to the additional attention layer.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "We do not have competitive results to Guo et al.",
        "evidence": "As shown from the table, the proposed G2S-GAT has better performance than S2S and G2S-GAT, but the best results are obtained by G2S-GGNN. For example, in LDC2015e86, G2S-GGNN obtains a new state-of-the-art result with a drop of 0.53 BLEU points. In same paper, Guo et al. As we can see, our G2S models perform on par with the state-of-the-art. However, the results of Guo et al. As we see, our G2S models perform significantly better than all previous state-of-the-art.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "I examine the results of our findings with regard to the best-performing classifier.",
        "evidence": "show that the antonym based learned representations are more useful for sentiment task as compared to prefixing with NOT_. The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg. baseline with F1 scores improving from 0.72 to 0.78 for positive sentiment and from 0.83 to 0.87 for negative sentiment..",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Several groups of words are much more likely to appear in a complaint, and are used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech).",
        "evidence": "The claim cluster prediction results for the LIWC and Word2Vec topics are presented in Table. [CONTINUE] It can be observed that words referring to ‘sale’ or ‘order’ are the most frequent words to appear in a complaint. However, the words about orders or deliveries (in the retail domain) are much more likely to appear in a complaint. [CONTINUE] Complaints about parts of tech products (in tech) are also rather frequent words. [CONTINUE] General topics typical of complaint words include requiring assistance or customer support. Several groups of words are much more likely to appear in a complaint: about changes in service providers (in complaints to service providers) and about parts of tech products (in tech complaints).",
        "table": "+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Label   | [BOLD] Complaints  [BOLD] Words                           | [BOLD] Complaints  [ITALIC] r   | [BOLD] Not Complaints  [BOLD] Label   | [BOLD] Not Complaints  [BOLD] Words                        | [BOLD] Not Complaints  [ITALIC] r   |\n+====+===================================+===========================================================+=================================+=======================================+============================================================+=====================================+\n|  0 | [BOLD] LIWC Features              | [BOLD] LIWC Features                                      | [BOLD] LIWC Features            | [BOLD] LIWC Features                  | [BOLD] LIWC Features                                       | [BOLD] LIWC Features                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  1 | NEGATE                            | not, no, can’t, don’t, never, nothing, doesn’t, won’t     | .271                            | POSEMO                                | thanks, love, thank, good, great, support, lol, win        | .185                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  2 | RELATIV                           | in, on, when, at, out, still, now, up, back, new          | .225                            | AFFECT                                | thanks, love, thank, good, great, support, lol             | .111                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  3 | FUNCTION                          | the, i, to, a, my, and, you, for, is, in                  | .204                            | SHEHE                                 | he, his, she, her, him, he’s, himself                      | .105                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  4 | TIME                              | when, still, now, back, new, never, after, then, waiting  | .186                            | MALE                                  | he, his, man, him, sir, he’s, son                          | .086                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  5 | DIFFER                            | not, but, if, or, can’t, really, than, other, haven’t     | .169                            | FEMALE                                | she, her, girl, mom, ma, lady, mother, female, mrs         | .084                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  6 | COGPROC                           | not, but, how, if, all, why, or, any, need                | .132                            | ASSENT                                | yes, ok, awesome, okay, yeah, cool, absolutely, agree      | .080                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  7 | [BOLD] Word2Vec Clusters          | [BOLD] Word2Vec Clusters                                  | [BOLD] Word2Vec Clusters        | [BOLD] Word2Vec Clusters              | [BOLD] Word2Vec Clusters                                   | [BOLD] Word2Vec Clusters            |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  8 | Cust. Service                     | service, customer, contact, job, staff, assist, agent     | .136                            | Gratitude                             | thanks, thank, good, great, support, everyone, huge, proud | .089                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  9 | Order                             | order, store, buy, free, delivery, available, package     | .128                            | Family                                | old, friend, family, mom, wife, husband, younger           | .063                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 10 | Issues                            | delayed, closed, between, outage, delay, road, accident   | .122                            | Voting                                | favorite, part, stars, model, vote, models, represent      | .060                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 11 | Time Ref.                         | been, yet, haven’t, long, happened, yesterday, took       | .122                            | Contests                              | Christmas, gift, receive, entered, giveaway, enter, cards  | .058                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 12 | Tech Parts                        | battery, laptop, screen, warranty, desktop, printer       | .100                            | Pets                                  | dogs, cat, dog, pet, shepherd, fluffy, treats              | .054                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 13 | Access                            | use, using, error, password, access, automatically, reset | .098                            | Christian                             | god, shall, heaven, spirit, lord, belongs, soul, believers | .053                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "These results indicate dense connections do play a significant role in our model.",
        "evidence": "These results indicate that the connection between the node and GCN is indeed a significant component in our model. The difference between the BLEU scores of the less dense blocks (DCGCN4) and the more dense blocks ({3, 4} dense blocks) is considerably smaller than the gap between the dense blocks (DCGCN4) and the baseline model (DCGCN3). This indicates that the proposed dense blocks can construct a better node graph.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "supports"
    },
    {
        "claim": "This empirically shows that compared to recurrent graph encoders, DCGCNs do not necessarily learn better representations for graphs.",
        "evidence": "Asbeck et al. , we experiment with the DCGCN model and compare it against the other two baselines. This shows that compared to recurrent graph encoders, DCGCN models learned from the lattice lattices do not necessarily learn better representations for graphs. We hypothesize that the reason is similar to what we saw for bag-of-words models. Namely, the sequential structure of the graph captures more sequential information and less long-distance dependencies, compared to self-attention or GCN models.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The most interesting ones are mask, rage, and cry, which significantly increase accuracy.",
        "evidence": "These results show that the most accurate ones are mask, rage, and cry, which significantly increase accuracy. [CONTINUE] It is interesting to notice the number of no-emoji for each emoji is also increased, which means the accuracy is improved. For example, the sob emoji has a reduction of 3.67% compared to its accuracy. Similarly, the rage emoji has a reduction of 13.04%, which is 13.04% more than the rage emoji. [CONTINUE] Finally, not all emoji are beneficial for this task. [CONTINUE] When comparing sweat smile and confused accuracy, we can see the former is better, but not by much. [CONTINUE] It is interesting to see sweat smile has a much higher accuracy than confused accuracy.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Results with BERT show that contextual information is not always valuable for performance improvement.",
        "evidence": "+ln+bert shows that our sequence-to-sequence training is significantly more efficient than base-only andlrn. However, further experiments show that this improvement is not always derived from showing improved token representations. We reserve this ability for future work.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU.",
        "evidence": "Our model outperforms PG-MMR when trained with the same number of parameters as PG-original, and is comparable to CopyTransformer when trained with less data. In particular, Hi-MAP outperforms it on R-2 and R-SU by about 1.5 and 1.5 points, respectively.",
        "table": "+----+------------------------------------------+--------------+--------------+---------------+\n|    | [BOLD] Method                            | [BOLD] R-1   | [BOLD] R-2   | [BOLD] R-SU   |\n+====+==========================================+==============+==============+===============+\n|  0 | First-1                                  | 26.83        | 7.25         | 6.46          |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  1 | First-2                                  | 35.99        | 10.17        | 12.06         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  2 | First-3                                  | 39.41        | 11.77        | 14.51         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  3 | LexRank Erkan and Radev ( 2004 )         | 38.27        | 12.70        | 13.20         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  4 | TextRank Mihalcea and Tarau ( 2004 )     | 38.44        | 13.10        | 13.50         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  5 | MMR Carbonell and Goldstein ( 1998 )     | 38.77        | 11.98        | 12.91         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  6 | PG-Original Lebanoff et al. ( 2018 )     | 41.85        | 12.91        | 16.46         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  7 | PG-MMR Lebanoff et al. ( 2018 )          | 40.55        | 12.36        | 15.87         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  8 | PG-BRNN Gehrmann et al. ( 2018 )         | 42.80        | 14.19        | 16.75         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  9 | CopyTransformer Gehrmann et al. ( 2018 ) | [BOLD] 43.57 | 14.03        | 17.37         |\n+----+------------------------------------------+--------------+--------------+---------------+\n| 10 | Hi-MAP (Our Model)                       | 43.47        | [BOLD] 14.89 | [BOLD] 17.41  |\n+----+------------------------------------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Seq2Seq model trained with user annotation is better than Seq2Seq model trained with user and system action annotation.",
        "evidence": "The claim is that if user and system action annotations are combined, the performance of a seq2seq model trained with user annotation is even better than a seq2seq model trained with system action annotation. For example, the combined score of the copy model is 13.7% lower than the combined score of the md-sequicity model, but the use of domain-adaptive training degrades its performance. This shows that a model trained with user and system action annotation can achieve better results than a model trained only on system action annotations, even if the user and system action annotations are not concatenated.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model gr def had the best performance in both cases, even when we included the out-of-vocabulary (oov) terms.",
        "evidence": "We noticed that model GB def was the best in both categories, even when we included the out-of-vocabulary (OOV) terms.",
        "table": "+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|    | Category Semantic   | Category no oov words   | gr_def 58.42%   | gr_neg10 59.33%   | cc.el.300  [BOLD] 68.80%   | wiki.el 27.20%   | gr_cbow_def 31.76%   | gr_d300_nosub 60.79%   | gr_w2v_sg_n5 52.70%   |\n+====+=====================+=========================+=================+===================+============================+==================+======================+========================+=======================+\n|  0 | [EMPTY]             | with oov words          | 52.97%          | 55.33%            | [BOLD] 64.34%              | 25.73%           | 28.80%               | 55.11%                 | 47.82%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  1 | Syntactic           | no oov words            | 65.73%          | 61.02%            | [BOLD] 69.35%              | 40.90%           | 64.02%               | 53.69%                 | 52.60%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  2 | [EMPTY]             | with oov words          | [BOLD] 53.95%   | 48.69%            | 49.43%                     | 28.42%           | 52.54%               | 44.06%                 | 43.13%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  3 | Overall             | no oov words            | 63.02%          | 59.96%            | [BOLD] 68.97%              | 36.45%           | 52.04%               | 56.30%                 | 52.66%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  4 | [EMPTY]             | with oov words          | 53.60%          | 51.00%            | [BOLD] 54.60%              | 27.50%           | 44.30%               | 47.90%                 | 44.80%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "we report below both the performance as assessed with automatic evaluation metrics in Table 3 as well as with human evaluations in Tables 4 and 5, to show that the model trained with our objective does not necessarily sacrifice ROUGE F1 in favour of maintaining the headlines’ readability.",
        "evidence": "We report both the performance as assessed by the automatic evaluation metrics in Table 3 as well as with human evaluations in Table 4 to show that the model trained with our objective does not necessarily sacrifice ROUGE F1 in favour of maintaining the headlines’ readability.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "They are effective when the approximate model class is complex and/or the interaction with the environment is infrequent, but become intractable as the interaction becomes more frequent or the state-action space grows large.",
        "evidence": "They are effective when the approximate model class is relatively simple. This indicates that the interaction is infrequent and/or sparse. However, they are not intractable. In fact, they indicate that the interaction with the environment is roughly proportional to the size of the state-action space.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "LRN is not the fastest model, with ATR outperforming it by 8%∼27%.",
        "evidence": "Based on the results, we can see that ATR is the fastest model with a best accuracy of 85.88%, followed by GRU and SRU. LSTM is the worst, which is surprising seeing that the state-of-the-art seq2seq model is also a seq2seq model.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high.",
        "evidence": "The first row does not report the results for the linear dataset. In fact, the implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high.. The results for the balanced dataset are obtained using a batch size of 1 and the linear dataset is presented in the second row. Similarly, the results for the linear dataset are obtained using a batch size of 1 and the results for the balanced dataset are obtained using a batch size of 25.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Our model outperforms the previous stateof-the-art models on both datasets in terms of F1 score.",
        "evidence": "Our model outperforms the previous stateof-the-art models on both datasets in terms of F1 score. On the NYT10 dataset, our model has 4.3% higher F1 score compared to the previous best model EA. Similarly, we achieve 3.3% higher F1 score compared to the previous best model GGWA. Our model improves the precision scores on both datasets with good recall scores.",
        "table": "+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|    | Model                 | NYT10 Prec.   | NYT10 Rec.   | NYT10 F1      | NYT11 Prec.   | NYT11 Rec.   | NYT11 F1      |\n+====+=======================+===============+==============+===============+===============+==============+===============+\n|  0 | CNN zeng2014relation  | 0.413         | 0.591        | 0.486         | 0.444         | 0.625        | 0.519         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  1 | PCNN zeng2015distant  | 0.380         | [BOLD] 0.642 | 0.477         | 0.446         | 0.679        | 0.538†        |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  2 | EA huang2016attention | 0.443         | 0.638        | 0.523†        | 0.419         | 0.677        | 0.517         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  3 | BGWA jat2018attention | 0.364         | 0.632        | 0.462         | 0.417         | [BOLD] 0.692 | 0.521         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  4 | BiLSTM-CNN            | 0.490         | 0.507        | 0.498         | 0.473         | 0.606        | 0.531         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  5 | Our model             | [BOLD] 0.541  | 0.595        | [BOLD] 0.566* | [BOLD] 0.507  | 0.652        | [BOLD] 0.571* |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Comparing the 784-dimensional models, CBOW and CMOW do not seem to complement each other.",
        "evidence": "The results seem to show that the hierarchical models (i.e., CMOW and CBOW) are not complementary. The difference is most evident on the SUBJ dataset, where CBOW is +3.1% in accuracy and CMOW is +7.2%, but they are very close in other datasets, i.e., +0.6%, +1.5%, +2.9%, +14.3% for SUBJ, MRPC, TREC, SST5, STS-B, and SICK-R, respectively. This is very different from what we would expect for subj.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "In conclusion, these results above can show the ineffectiveness of our DCGCN models.",
        "evidence": "We can see from the table that the best results of our GCN model are obtained by +RC (4) and +RC+LA (10). These results show the effectiveness of our DCGCN model. Especially, with +RC (9), +RC+LA can achieve the best BLEU score of 50.5. And with +CNN (6), the best BLEU score is 21.3. These results demonstrate the effectiveness of our DCGCN model.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail.",
        "evidence": "The results of G2S-GAT and G2S-GGNN show a large improvement in accuracy in tasks where the distribution of node degrees has a long tail, which shows that our GINS can be employed to a greater extent.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units do not sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied.",
        "evidence": "According to the table, although GRU and LRSN outperform the LSTM by 0.3∼0.9 in terms of accuracy, they do not sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the top most distinctive features for a tweet not being labeled as a complaint.",
        "evidence": "Top unigrams and part-of-speech features for complaints and not complaints All of the unigrams and part-of-speech features are relevant to complaints and not complaints. [CONTINUE] However, unigrams and part-of-speech features about emotions (good, great, win, affect, assent) are not top features for complaints. [CONTINUE] On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSMO, AFFECT, ASSENT) are among the top most distinctive features for a tweet not being labeled as a complaint. [CONTINUE] In addition, other positive sentiment words (good, great, win, POSMO, ASSENT) that are not used as features are also observed. [CONTINUE] Complaints use more possessive pronouns, indicating that the user is describing personal experiences. [CONTINUE] A distinctive part-of-speech pattern used by complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items of services possessed by the complainer (e.g., my account, my order). [CONTINUE] Question marks are used to indicate complaints, as many complaints are formulated as questions to the responsible party (e.g., why is this not working?, when will [CONTINUE] get my response?).",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Most denying instances get misclassified as commenting (see Table 5),",
        "evidence": "Most denying instances get misclassified as commenting After doing preliminary analysis of the data, we conclude that the majority of denying instances are commenting. This finding is consistent with the results of \\newciteMoYu2014convolutional, who also found that majority of denying instances are misclassified as commenting.",
        "table": "+----+--------------------------+------------+------------+------------+------------+\n|    | [BOLD] LabelPrediction   |   [BOLD] C |   [BOLD] D |   [BOLD] Q |   [BOLD] S |\n+====+==========================+============+============+============+============+\n|  0 | [BOLD] Commenting        |        760 |          0 |         12 |          6 |\n+----+--------------------------+------------+------------+------------+------------+\n|  1 | [BOLD] Denying           |         68 |          0 |          1 |          2 |\n+----+--------------------------+------------+------------+------------+------------+\n|  2 | [BOLD] Querying          |         69 |          0 |         36 |          1 |\n+----+--------------------------+------------+------------+------------+------------+\n|  3 | [BOLD] Supporting        |         67 |          0 |          1 |         26 |\n+----+--------------------------+------------+------------+------------+------------+",
        "label": "supports"
    },
    {
        "claim": "As can be seen in Table 1, sparsemax and TVMAX achieve better results overall when compared with softmax, indicating that the use of selective attention leads to better captions.",
        "evidence": "As can be seen, sparsemax and TVMAX achieve better results overall when compared with softmax, indicating that the use of selective attention leads to better captions..",
        "table": "+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|    | [EMPTY]   | MSCOCO spice   | MSCOCO cider   | MSCOCO rouge [ITALIC] L   | MSCOCO bleu4   | MSCOCO meteor   | MSCOCO rep↓   | Flickr30k spice   | Flickr30k cider   | Flickr30k rouge [ITALIC] L   | Flickr30k bleu4   | Flickr30k meteor   | Flickr30k rep↓   |\n+====+===========+================+================+===========================+================+=================+===============+===================+===================+==============================+===================+====================+==================+\n|  0 | softmax   | 18.4           | 0.967          | 52.9                      | 29.9           | 24.9            | 3.76          | 13.5              | 0.443             | 44.2                         | 19.9              | 19.1               | 6.09             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  1 | sparsemax | [BOLD] 18.9    | [BOLD] 0.990   | [BOLD] 53.5               | [BOLD] 31.5    | [BOLD] 25.3     | 3.69          | [BOLD] 13.7       | [BOLD] 0.444      | [BOLD] 44.3                  | [BOLD] 20.7       | [BOLD] 19.3        | 5.84             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  2 | TVmax     | 18.5           | 0.974          | 53.1                      | 29.9           | 25.1            | [BOLD] 3.17   | 13.3              | 0.438             | 44.2                         | 20.5              | 19.0               | [BOLD] 3.97      |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "We find EWC does not outperform the L2 approach.",
        "evidence": "Here, we compare our approach (EWC) against the L2 approach and two alternatives where we trained first on the Health and Bio corpora, followed by the L2 approach. Health is clearly better than Bio on our own, and the combination of the two approaches is better than either alone.",
        "table": "+----+-----------+--------------------------+-----------------+--------------+\n|    |   [EMPTY] | [BOLD] Training scheme   | [BOLD] Health   | [BOLD] Bio   |\n+====+===========+==========================+=================+==============+\n|  0 |         1 | Health                   | [BOLD] 35.9     | 33.1         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  1 |         2 | Bio                      | 29.6            | 36.1         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  2 |         3 | Health and Bio           | 35.8            | 37.2         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  3 |         4 | 1 then Bio, No-reg       | 30.3            | 36.6         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  4 |         5 | 1 then Bio, L2           | 35.1            | 37.3         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  5 |         6 | 1 then Bio, EWC          | 35.2            | [BOLD] 37.8  |\n+----+-----------+--------------------------+-----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "there were no adjectives in the questions except for the \"concept\" and \"property\" words, for the adjectives were replaced with prepositional phrases, for instance.",
        "evidence": "The system proposed by us was able to answer more questions than GloVe. In particular, we noticed that the accuracy of the system for seen questions was similar to that of GloVe. Since we only considered the questions that were seen in training, this indicates that our system is robust to variability in the dataset.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "On the other side, H-CMOW shows, among others, no improvements at BShift.",
        "evidence": "Among others, H-CWOW shows, among others, the best performance on noun, verb, and adjective tuples. As the compression ratio increases, the improvements of H-CWOW over CMOW/400 are more than 5% larger than those of CMOW. These results indicate that the proposed method is effective at reducing the vocabulary size.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "In contrast, RoBERTa-large drops only 3.1 points when trained and evaluated on the split from Sap et al.",
        "evidence": "BERT-Large drops 3.1 points in accuracy when trained and evaluated on the split from Sap et al. Compared to BERT-Large, RoBERTa-Large obtains 11.1 points in accuracy with a smaller finetuning margin.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax but still superior to softmax on MSCOCO and similar on Flickr30k.",
        "evidence": "As can be seen, sparsemax obtained the best results on MSCOCO and similar datasets, although the results are not as good on Flickr30k. We believe the reason for this is the limited training data in the Flickr30k setting. The average length of ground truth captions for TVMAX is almost twice longer than the sparsemax captioning one. Moreover, the results on MSCOCO are better than those on Flickr30k, which confirms that TVMAX is able to extract better captions than the softmax baseline. As can be seen, sparsemax scores higher than TVMAX in all the metrics on Flickr30k, which confirms the results obtained by our retraining procedure. However, the results are slightly worse than sparsemax on MSCOCO and similar on Flickr30k.",
        "table": "+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|    | [EMPTY]   | MSCOCO spice   | MSCOCO cider   | MSCOCO rouge [ITALIC] L   | MSCOCO bleu4   | MSCOCO meteor   | MSCOCO rep↓   | Flickr30k spice   | Flickr30k cider   | Flickr30k rouge [ITALIC] L   | Flickr30k bleu4   | Flickr30k meteor   | Flickr30k rep↓   |\n+====+===========+================+================+===========================+================+=================+===============+===================+===================+==============================+===================+====================+==================+\n|  0 | softmax   | 18.4           | 0.967          | 52.9                      | 29.9           | 24.9            | 3.76          | 13.5              | 0.443             | 44.2                         | 19.9              | 19.1               | 6.09             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  1 | sparsemax | [BOLD] 18.9    | [BOLD] 0.990   | [BOLD] 53.5               | [BOLD] 31.5    | [BOLD] 25.3     | 3.69          | [BOLD] 13.7       | [BOLD] 0.444      | [BOLD] 44.3                  | [BOLD] 20.7       | [BOLD] 19.3        | 5.84             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  2 | TVmax     | 18.5           | 0.974          | 53.1                      | 29.9           | 25.1            | [BOLD] 3.17   | 13.3              | 0.438             | 44.2                         | 20.5              | 19.0               | [BOLD] 3.97      |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "The best performing system is KnowComb.",
        "evidence": "The results show that the best performing system is KnowComb. It also outperforms all other approaches on both datasets.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "We divide the dataset into 5 folds according to the users’ identity information (e.g., 619,1802, etc.).",
        "evidence": "In order to perform a more fine-grained analysis of the results, we run additional experiments on the training set. We classify the dataset into five folds according to the identity information provided by the users: total negation cues, true negation cues, false negation cues, average scope length, average sentence length and average tweet length.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "the major drawback is that the dataset contains only 583 examples in the test set, which makes it difficult to measure model performance robustly.",
        "evidence": "The comparison is not entirely fair, as the dataset contains only 583 examples in the test set, which makes it difficult to measure model performance robustly. We believe that the unsupervised approach is preferable, as it provides much more labeled training data. It is also seen that good performance can be achieved with the assistance of PMI, which is a much effective approach when dealing with long sequences.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "The alternative creates a conjoined structure consisting of both the non-polar cue and the target alternative.",
        "evidence": "The results confirm our hypothesis that the alternative signal is semantically richer than the non-polar cue. In particular, the dimensionality of the “other” alternative space is smaller than that of the “non-polar” alternative space, because the former is composed of punctuation marks and meaningless words.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "As the table 4 depicts, the precision increases with the growth of d, but the training time also increases.",
        "evidence": "As the table 4 depicts, the precision increases with the growth of d. However, the training time also increases.. The time is increased from 4H to 20H when the value of d is 16.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "refutes"
    },
    {
        "claim": "This is unexpected as encoding a bigger graph (containing more information) should be easier than encoding smaller graphs.",
        "evidence": "The S2S and G2S models are not able to encode smaller graphs efficiently. This is not surprising as encoding a bigger graph should be easier than encoding a smaller one as the latter contains more information. G2S-GAT and G2S-GGNN have similar performance and the increase in out-degrees has been smaller than in the previous experiments.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes.",
        "evidence": "The results show that S2S, G2S-GGNN and G2S-GAT have similar performance but the latter slightly outperforms the former in the case of high degree graphs. [CONTINUE] G2S-GGNN and G2S-GAT have similar performance in the low to medium range of 0-7, but the performance gap is much larger in the range of 0-20 and 30-240. [CONTINUE] As the graphs get longer, the gaps between S2S and G2S-GGNN increase.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Our model does not improve the results in the translation tasks.",
        "evidence": "Results show that the neural ABSA models are not much better than the current state-of-the-art models. In most cases our average performance is worse than the former best case. In addition, the noise-aware model is 4.2% better than the average ABSA model in English to German translation. This shows that the problem is serious and we should tackle it more.",
        "table": "+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|    | Method                | En→It best   | En→It avg    |   En→It iters | En→De best   | En→De avg    |   En→De iters | En→Fi best   | En→Fi avg    |   En→Fi iters | En→Es best   | En→Es avg    |   En→Es iters |\n+====+=======================+==============+==============+===============+==============+==============+===============+==============+==============+===============+==============+==============+===============+\n|  0 | Artetxe et al., 2018b | [BOLD] 48.53 | 48.13        |           573 | 48.47        | 48.19        |           773 | 33.50        | 32.63        |           988 | 37.60        | 37.33        |           808 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|  1 | Noise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 |           471 | [BOLD] 49.67 | [BOLD] 48.89 |           568 | [BOLD] 33.98 | [BOLD] 33.68 |           502 | [BOLD] 38.40 | [BOLD] 37.79 |           551 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "the best-performing system outperforms the baseline in all cue types with the largest gains of 9.5% and 8.6% on the actual and false cue recall, respectively.",
        "evidence": "The system proposed by us outperforms the baseline in all cue types with the largest gains of 9.5% and 8.6% on the actual and false cue recall, respectively.",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In total, 1,232 tweets (62.4%) are complaints and 739 are not complaints (37.6%).",
        "evidence": "In total, 1,232 tweets (62.4%) are complaints and 739 are not complaints (37.6%)",
        "table": "+----+----------------------------+---------------------+-------------------------+\n|    | [BOLD] Category            |   [BOLD] Complaints |   [BOLD] Not Complaints |\n+====+============================+=====================+=========================+\n|  0 | Food & Beverage            |                  95 |                      35 |\n+----+----------------------------+---------------------+-------------------------+\n|  1 | Apparel                    |                 141 |                     117 |\n+----+----------------------------+---------------------+-------------------------+\n|  2 | Retail                     |                 124 |                      75 |\n+----+----------------------------+---------------------+-------------------------+\n|  3 | Cars                       |                  67 |                      25 |\n+----+----------------------------+---------------------+-------------------------+\n|  4 | Services                   |                 207 |                     130 |\n+----+----------------------------+---------------------+-------------------------+\n|  5 | Software & Online Services |                 189 |                     103 |\n+----+----------------------------+---------------------+-------------------------+\n|  6 | Transport                  |                 139 |                     109 |\n+----+----------------------------+---------------------+-------------------------+\n|  7 | Electronics                |                 174 |                     112 |\n+----+----------------------------+---------------------+-------------------------+\n|  8 | Other                      |                  96 |                      33 |\n+----+----------------------------+---------------------+-------------------------+\n|  9 | Total                      |                1232 |                     739 |\n+----+----------------------------+---------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "The Wiener filtering method shows lower DCE, but higher WER than no enhancement.",
        "evidence": "The Wiener filtering method shows lower DCE than no enhancement.",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 17.3        | 0.828        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 19.5        | 0.722        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 15.8        | [BOLD] 0.269 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 14.9        | 0.291        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 15.6        | 0.330        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 5.7         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "our method uses the combination of SPINE and Word2Sense to improve the performance of sentiment classification task",
        "evidence": "Our method uses the combination of Spine and Word2Sense to improve the performance of the sentiment classification task. The results show that our proposed method outperforms all the baselines.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics .",
        "evidence": "From this table, we can see that the bias is relatively aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics.",
        "table": "+----+------------------------------------------------+--------------+------------+---------------+\n|    | Category                                       |   Female (%) |   Male (%) |   Neutral (%) |\n+====+================================================+==============+============+===============+\n|  0 | Office and administrative support              |       11.015 |     58.812 |        16.954 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  1 | Architecture and engineering                   |        2.299 |     72.701 |        10.92  |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  2 | Farming, fishing, and forestry                 |       12.179 |     62.179 |        14.744 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  3 | Management                                     |       11.232 |     66.667 |        12.681 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  4 | Community and social service                   |       20.238 |     62.5   |        10.119 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  5 | Healthcare support                             |       25     |     43.75  |        17.188 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  6 | Sales and related                              |        8.929 |     62.202 |        16.964 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  7 | Installation, maintenance, and repair          |        5.22  |     58.333 |        17.125 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  8 | Transportation and material moving             |        8.81  |     62.976 |        17.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  9 | Legal                                          |       11.905 |     72.619 |        10.714 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 10 | Business and financial operations              |        7.065 |     67.935 |        15.58  |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 11 | Life, physical, and social science             |        5.882 |     73.284 |        10.049 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 12 | Arts, design, entertainment, sports, and media |       10.36  |     67.342 |        11.486 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 13 | Education, training, and library               |       23.485 |     53.03  |         9.091 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 14 | Building and grounds cleaning and maintenance  |       12.5   |     68.333 |        11.667 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 15 | Personal care and service                      |       18.939 |     49.747 |        18.434 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 16 | Healthcare practitioners and technical         |       22.674 |     51.744 |        15.116 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 17 | Production                                     |       14.331 |     51.199 |        18.245 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 18 | Computer and mathematical                      |        4.167 |     66.146 |        14.062 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 19 | Construction and extraction                    |        8.578 |     61.887 |        17.525 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 20 | Protective service                             |        8.631 |     65.179 |        12.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 21 | Food preparation and serving related           |       21.078 |     58.333 |        17.647 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 22 | Total                                          |       11.76  |     58.93  |        15.939 |\n+----+------------------------------------------------+--------------+------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Still, lemma-based targets significantly7 (p ≤ .005) outperform type-based targets in terms of F-measure in all cases.",
        "evidence": "The type and type.POS targets significantly outperform type-based targets in terms of F-measure in all cases. Also, lemmatized targets significantly7 (p ≤ .005) outperform type-based targets in terms of F-measure in all cases. Lemma-based targets are significantly better than type-based targets in terms of F-measure in all cases. X+POS targets significantly7 (p ≤ .005) outperform lemmatized targets in terms of F-measure in all cases. Lemma-based targets are significantly better than type-based targets in terms of F-Measure in all cases.",
        "table": "+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|    | [EMPTY]      | WN-N P            | WN-N R            | WN-N F            | WN-V P            | WN-V R            | WN-V F            | VN P              | VN R              | VN F              |\n+====+==============+===================+===================+===================+===================+===================+===================+===================+===================+===================+\n|  0 | Context: w2  | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  1 | type         | .700              | .654              | .676              | .535              | .474              | .503              | .327              | .309              | .318              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  2 | x+POS        | .699              | .651              | .674              | .544              | .472              | .505              | .339              | .312              | .325              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  3 | lemma        | .706              | .660              | .682              | .576              | .520              | .547              | .384              | .360              | .371              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  4 | x+POS        | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  5 | Context: dep | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  6 | type         | .712              | .661              | .686              | .545              | .457              | .497              | .324              | .296              | .310              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  7 | x+POS        | .715              | .659              | .686              | .560              | .464              | .508              | .349              | .320              | .334              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  8 | lemma        | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591              | .512              | .548              | .408              | .371              | .388              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  9 | x+POS        | .722              | .666              | .693              | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "Table 2 shows that the model with cyclic loss (M2) and the model with cyclic loss, paraphrase loss, and language model loss (M5) both have lower Sim than M0 on both datasets under similar Acc.",
        "evidence": "We can see that the model with cyclic loss (M2) performs better than the model with paraphrase loss (M5). Particularly, the pairwise comparison between M2 and M6 shows that the model with cyclic loss, paraphrase loss, and language model loss does not outperform M0 on both datasets under similar accuracy.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.818 | 0.719        | 37.3        | 10.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.819 | 0.734        | 26.3        | 14.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.813 | 0.770        | 36.4        | 18.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.807 | 0.796        | 28.4        | 21.5        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.798 | 0.783        | 39.7        | 19.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785        | 27.1        | 20.3        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.805 | [BOLD] 0.817 | 43.3        | 21.6        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.818 | 0.805        | [BOLD] 29.0 | [BOLD] 22.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "The proposed method outperforms the original embeddings and performs on par with the SOV.",
        "evidence": "The results show that our proposed method is on par with the word2vec embeddings.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "supports"
    },
    {
        "claim": "In addition, the training time results in Table 3 confirm the computational disadvantage of LRN over all other recurrent units, where LRN slows down compared to ATR and SRU by approximately 25%.",
        "evidence": "We confirm the findings of Vaswani et al. More importantly, the training time results in LRN are slightly faster than ATR and SRN, while being comparable in terms of BLEU.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "As for the micro F1 evaluation metric, our model does not achieve the highest performance (83.54%) on the FNC-1 testing subset.",
        "evidence": "As for the micro F1 metric, our model does not achieve the highest performance (83.54%) on the FNC-1 testing subset. As for the accuracy, our model achieves 28.53% on the agree and 10.43% on the disagree, which are higher than the scores of the baseline methods.",
        "table": "+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|    | Model                             |   Accuracy (%) agree |   Accuracy (%) disagree |   Accuracy (%) discuss |   Accuracy (%) unrelated | Micro F1(%)   |\n+====+===================================+======================+=========================+========================+==========================+===============+\n|  0 | Average of Word2vec Embedding     |                12.43 |                    1.3  |                  43.32 |                    74.24 | 45.53         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  1 | CNN-based Sentence Embedding      |                24.54 |                    5.06 |                  53.24 |                    79.53 | 81.72         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  2 | RNN-based Sentence Embedding      |                24.42 |                    5.42 |                  69.05 |                    65.34 | 78.70         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  3 | Self-attention Sentence Embedding |                23.53 |                    4.63 |                  63.59 |                    80.34 | 80.11         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  4 | Our model                         |                28.53 |                   10.43 |                  65.43 |                    82.43 | [BOLD] 83.54  |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results.",
        "evidence": "Attention heads. The results show that attending to multiple contexts at once, is important to boosting performance to state-of-the-art results. For example, after attending to 2 contexts, the accuracy drops by 6.84%, which shows that attending to just one context is not sufficient. Similarly, after attending to 4 contexts, the accuracy only drops by 2.43%, which shows that attending to multiple contexts at once is crucial to boosting performance to state-of-the-art results.",
        "table": "+----+--------------+------------+-------------+----------+\n|    |   # of Heads | Accuracy   |   Val. Loss | Effect   |\n+====+==============+============+=============+==========+\n|  0 |            1 | 89.44%     |      0.2811 | -6.84%   |\n+----+--------------+------------+-------------+----------+\n|  1 |            2 | 91.20%     |      0.2692 | -5.08%   |\n+----+--------------+------------+-------------+----------+\n|  2 |            4 | 93.85%     |      0.2481 | -2.43%   |\n+----+--------------+------------+-------------+----------+\n|  3 |            8 | 96.02%     |      0.2257 | -0.26%   |\n+----+--------------+------------+-------------+----------+\n|  4 |           10 | 96.28%     |      0.2197 | [EMPTY]  |\n+----+--------------+------------+-------------+----------+\n|  5 |           16 | 96.32%     |      0.219  | +0.04    |\n+----+--------------+------------+-------------+----------+",
        "label": "supports"
    },
    {
        "claim": "(1) BERT is able to capture the gist of the summaries, and thus is appropriate for predicting good summaries; (2) the sentences in good summaries tend to have high tf-idf similarity to the target article;",
        "evidence": "We claim that: (1) BERT is able to capture the gist of the articles, and thus is appropriate for predicting good summaries; (2) the sentences in good summaries tend to have high tf-idf similarity to the target article; and (3) InferSent and Cosine are two recent methods for measuring the quality of abstractive summaries. Specifically, we compute the cosine similarity between the reference summary and the generated abstractive summaries, and then average the scores over all reference summaries and their unigrams counterparts (using the Benjamini-Hochreiter test).",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "Despite our system achieving the same level of performance compared to a state-of-art general coreference system, we still observe significant performance improvement on the ACE and OntoNotes datasets.",
        "evidence": "Performance on standard ACE and OntoNotes datasets. Our KnowComb system achieves the same level of performance as does the state-of-art general coreference system we base it on. As hard coreference problems are rare in standard coreference datasets, we do not observe significant performance improvement. However, these results show that our additional Predicate Schemas do not harm the predictions for regular mentions.",
        "table": "+----+-----------+--------------+--------------+--------------+--------------+\n|    | System    | MUC          | BCUB         | CEAFe        | AVG          |\n+====+===========+==============+==============+==============+==============+\n|  0 | ACE       | ACE          | ACE          | ACE          | ACE          |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  1 | IlliCons  | [BOLD] 78.17 | 81.64        | [BOLD] 78.45 | [BOLD] 79.42 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  2 | KnowComb  | 77.51        | [BOLD] 81.97 | 77.44        | 78.97        |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  3 | OntoNotes | OntoNotes    | OntoNotes    | OntoNotes    | OntoNotes    |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  4 | IlliCons  | 84.10        | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  5 | KnowComb  | [BOLD] 84.33 | 78.02        | 67.95        | 76.76        |\n+----+-----------+--------------+--------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Results also show the global node is more effective than the linear combination.",
        "evidence": "The experiments also show the global node is more effective than the linear combination. [CONTINUE] The results of the comparison are summarized in Table 7. [CONTINUE] From the table we can see that when the global node is not used, the performance drops significantly, which shows the importance of combining the global node. [CONTINUE] Further, the results of the comparison are better than the linear combination. [CONTINUE] From the comparison of the three different modules, we can see the coverage mechanism is the most important module. [CONTINUE] When the coverage mechanism is removed, the performance of the model decreases significantly.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem.",
        "evidence": "The main reasons for the problems identified by our system were misspelling in the original sentence and wrong values in the disfluency label. In the case of the original sentence, we found 22 inaccurate slot values in the original and cleaned sentence pairs. We can also find that there are several errors in the original sentence that lead to wrong or wrong values in the disfluency label. Another reason is that the language model may have not been trained enough for the disfluency label in the original sentence and the characters in the disfluency label may have not been learned. Using our DisFL postprocessing, we can remove some of the errors, but some other errors remain such as missing or incorrect values in the missing/wrong-valued slots.",
        "table": "+----+------------------------+--------------+---------------+----------------+----------------+\n|    | [BOLD] Training data   |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] Disfl |\n+====+========================+==============+===============+================+================+\n|  0 | Original               |            0 |            22 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  1 | Cleaned added          |            0 |            23 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  2 | Cleaned missing        |            0 |             1 |              0 |              2 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  3 | Cleaned                |            0 |             0 |              0 |              5 |\n+----+------------------------+--------------+---------------+----------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "The models using BoC do not outperform models using BoW as well as ASM features.",
        "evidence": "Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table. The models using BoC outperform models using BoW as well as ASM features. [CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most of the features. The models using BoC (Wiki-PubMed-PMC) outperform models using BoW (Mikolov et al., 2013a) in extraction of most of the features.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "this shows the importance of the SRBR strategy.",
        "evidence": "To further verify the importance of the SRL strategy, we performed experiments on a subset of questions, i.e. all questions – i.e., at least one question for each concept word – and all questions for the semantic similarity ranking. We can see that for all three scenarios, our proposed algorithm outperforms GloVe and word2vec in terms of accuracy. In particular, for the “all questions” subset, the proposed algorithm improves by 2.89% over GloVe, and by 3.38% over word2vec. This shows the importance of the SRL strategy.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "especially for DAMD, modeling multi-action system responses, which are notoriously rare and difficult to collect, significantly improves the performance.",
        "evidence": "The claim is that multi-action system response (DAMD) dramatically outperforms other models in terms of inform and success rates, [CONTINUE] Moreover, even without multi-action data augmentation, DAMD achieves very high metrics of both inform and success compared to the state-of-the-art, [CONTINUE] Moreover, applying multi-action data augmentation on top of the baseline is very effective, [CONTINUE] Specifically, in our case, applying multi-action data augmentation on top of seq2seq improves [CONTINUE] the inform and success rates by over 5% and 2% respectively, and outperforms HDSA in BLEU, the domain-specific score, and the combined score.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "it is critical to realize that the conversational negation corpus is indeed a true label corpus, while neither O1 nor O2 are correct.",
        "evidence": "It is critical to realize that the negation corpus is a complete dataset, and that these “images” are in fact extracted from the conversation corpus. The “false negation cues” in the corpus are 247 real sentences and 247 false negation cues, which is much higher than the number of “true negation cues” (2921) and “false negation cues” (247). In order to determine whether or not these reduced real sentences are actually contained in the conversational data, we also calculate per-sentence lengths and average tweet length. It is evident that the average sentence length is 13.6, and the average tweet length is 22.3. These statistics show that the average negation sentence length is actually quite short.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task.",
        "evidence": "This section shows the performance of our approach compared to the baseline and otherrn-based approaches. It can be seen that, although the performance of ELRn is slightly better than the plain RNNs on the SNLI task, the results are much worse on the PTB task. This shows that for the PTB task, the more complex the problem is, and the easier the task is, the more our approach is able to solve compared to the otherrn-based approaches.",
        "table": "+----+---------+--------------+--------------+\n|    | Model   | SNLI         | PTB          |\n+====+=========+==============+==============+\n|  0 | LRN     | [BOLD] 85.06 | [BOLD] 61.26 |\n+----+---------+--------------+--------------+\n|  1 | gLRN    | 84.72        | 92.49        |\n+----+---------+--------------+--------------+\n|  2 | eLRN    | 83.56        | 169.81       |\n+----+---------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "It can be noted that the use of discourse markers is crucial for the task since the results after removing them from the dataset is far from optimal.",
        "evidence": "The claim is that the proposed CNN-LSTMOur-neg-Ant significantly outperforms all baselines for both positive and negative sentiment. It should be noted, however, that the SVM-w/o neg. baseline is much stronger than the CNN-LSTM and the SVM-our-neg. Hence, it is important to understand the difficulty of the task due to the small size of the test set compared to the sentiment analysis dataset.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "1, where the x-axis refers to each metric and the y-axis refers to the number of sessions.",
        "evidence": "S5SS3SSS0Px4 Results The system was evaluated on a randomly sampled subset of the claims from the test set. In each case, the x-axis refers to the number of sessions, and the y-axis refers to the number of successfully completed sessions. The system was evaluated on a randomly sampled subset of the test set, and the results are comparable to the results on the full test set. The system was evaluated on a randomly sampled subset of the test set, and the results are similar to the results on the full test set.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "For Task B, the baseline model outperformed all models trained on the stacked learner when using only plain averaged word embeddings.",
        "evidence": "For Task B, the model trained on the stacked learner outperformed all other models when using only plain averaged word embeddings.",
        "table": "+----+--------------------------------------+--------------------+\n|    | [EMPTY]                              | Micro F1           |\n+====+======================================+====================+\n|  0 | Baseline                             | 0.709              |\n+----+--------------------------------------+--------------------+\n|  1 | W2V (<italic>d</italic>=50)          | 0.736              |\n+----+--------------------------------------+--------------------+\n|  2 | W2V (<italic>d</italic>=500)         | 0.753              |\n+----+--------------------------------------+--------------------+\n|  3 | S2V                                  | 0.748              |\n+----+--------------------------------------+--------------------+\n|  4 | S2V + W2V (<italic>d</italic>=50)    | 0.744              |\n+----+--------------------------------------+--------------------+\n|  5 | S2V + K + W2V(<italic>d</italic>=50) | 0.749              |\n+----+--------------------------------------+--------------------+\n|  6 | SIF (DE)                             | 0.759              |\n+----+--------------------------------------+--------------------+\n|  7 | SIF (DE-EN)                          | <bold>0.765</bold> |\n+----+--------------------------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "This is evident from the significant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters.",
        "evidence": "This is because the dataset is highly skewed with positive examples more than others. We can see from the table that OD significantly outperforms OD-parse: Compared to OD, OD is much more balanced. On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 respectively compared to the scores of 0.01, -0.01 and 0.05 of OD-parse. This is because the datasets of “Video games” and “Pornography” are relatively clean while “Seanad Abolition” and “Video games” are not. This is evident from the significant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters. Although Sentiment polarity shifters improve ARI score on “Seanad Abolition” and “Pornography” datasets, they bring very little change to the other three datasets. We can also see that sentiment polarity shifters improve ARI score on “Video games” dataset and “Pornography” dataset, but not on “Seanad Abolition” dataset.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "It is observed that the former outperforms the latter, indicating the key role of dialogue state estimation.",
        "evidence": "The first group shows the results of our models, and the next group shows the results of our augmented models. Our proposed DAMD model significantly outperforms other models in terms of inform and success rates, and has the highest combined score of all. Moreover, even without domain-aware belief span modeling, the model is able to inform and success much better than the copy mechanism. This shows that the domain-aware belief span model can estimate the capability of a system reasonably well. The results of the second group show that our proposed DAMD model has better performance than the others. Especially, the model with multi-action data augmentation has the highest inform and success scores.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While Transformer models are the best ones according to the evaluation metrics,",
        "evidence": "As it can be seen, the best model is the one base on the source-to-target parallel corpus, whose BLEU score is 36.8 and TER score is 46.8. It is worth mentioning that the BLEU score of the system reference is 33.3, which is very close to the BLEU score of the transformer model.",
        "table": "+----+--------------------+-------------+-------------+\n|    | System reference   | BLEU↑       | TER↓        |\n+====+====================+=============+=============+\n|  0 | en-fr-rnn-rev      | 33.3        | 50.2        |\n+----+--------------------+-------------+-------------+\n|  1 | en-fr-smt-rev      | 36.5        | 47.1        |\n+----+--------------------+-------------+-------------+\n|  2 | en-fr-trans-rev    | [BOLD] 36.8 | [BOLD] 46.8 |\n+----+--------------------+-------------+-------------+\n|  3 | en-es-rnn-rev      | 37.8        | 45.0        |\n+----+--------------------+-------------+-------------+\n|  4 | en-es-smt-rev      | 39.2        | 44.0        |\n+----+--------------------+-------------+-------------+\n|  5 | en-es-trans-rev    | [BOLD] 40.4 | [BOLD] 42.7 |\n+----+--------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "accuracy on average the proposed method outperform other approaches by 2.8% and 2.45% for B-CNN and R-CNN respectively.",
        "evidence": "S6SS1SSS0Px4 Results The results obtained by our method are presented in Table. As it can be seen, our proposed method outperform other approaches by 2.8% and 2.45% for B-CNN and R-CNN respectively.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our joint model does not outperform all the base lines, with a gap of only 10.5 CoNLL F1 points from the last published results (KCP), and only surpassing our strong lemma baseline by 3 points.",
        "evidence": "We see that our joint model is only slightly better than the disjoint baseline (10.5 CoNLL F1 points vs. 10.5 CoNLL F1 points), but only slightly worse than the strong lemma baseline (3 points). This suggests that the joint model is able to achieve competitive performance, while not achieving the state-of-the-art results.",
        "table": "+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|    | [BOLD] Model                     | R       | MUC P   | [ITALIC] F1   | B3 P    | CEAF- [ITALIC] e P   | CoNLL  [ITALIC] F1   |\n+====+==================================+=========+=========+===============+=========+======================+======================+\n|  0 | [BOLD] Baselines                 | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  1 | Cluster+Lemma                    | 75.5    | 79.9    | 73.6          | 85      | 71.7                 | 76.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  2 | CV Cybulska and Vossen ( 2015a ) | -       | 75      | 64            | 78      | -                    | 73                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  3 | KCP Kenyon-Dean et al. ( 2018 )  | 71      | 71      | 69            | 67      | 67                   | 69                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  4 | Cluster+KCP                      | 77.4    | 79.3    | 71.5          | 87.2    | 66.4                 | 73.6                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  5 | [BOLD] Model Variants            | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  6 | Disjoint                         | 80.3    | 83.6    | 75.9          | 86      | 71.9                 | 78.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  7 | Joint                            | 81      | 84.5    | 77.3          | 85.1    | 73.8                 | [BOLD] 79.5          |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "When redundancy removal was applied to LogReg, it produces only marginal improvement.",
        "evidence": "This indicates that redundancy removal was not effective in improving the summarization performance. This is consistent with our hypothesis.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "( 2018 ) who \\detrained\" their model for RL, our NeuralTD is simpler without any \\reinforcement\\’\\ training.",
        "evidence": "The claim By \\detraining their model for RL, our neuralTD achieves a competitive reward without any \\reinforcement\\�\\training.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "there are slight but consistent decreases when comparing to the metric trained using all 2,011 content words",
        "evidence": "The fine-tuned BERT model using all the paired data obtained from GloVe is at least slightly better than the model trained using all 2,011 content words. Specifically, BERT+MLP with pretrained word embeddings (pmeans-RNN, simred, BERT) has the best performance. It is significantly better than the simred+MLP model using all the paired data obtained from GloVe (pmeans-RNN) at Reg. Loss(1) and Precision. Loss(2) measures the precision and recall of the fine-tuned BERT model compared to the simred+MLP and BERT models. The results show that BERT+MLP with pretrained word embeddings is the best performing model across all metrics.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "on the other hand, we are still noticeably outperformed by Refresh when directly comparing the length of the two summaries.",
        "evidence": " refresh is still slightly better than our system at the moment of this writing. However, further analysis is required to clarify this behavior.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Syntactic part-ofspeech features alone obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section hold high predictive accuracy for the task.",
        "evidence": "The model that uses only the syntactic part-ofspeech features alone gets the best results overall. Syntactic sentiment features and complaint specific features are predictive of the labels in the majority of the cases. The best performing sentiment feature group is V&B, which is similar to the Stanford features. However, the best performing complaint feature group is Complaint specific, which is similar to the LIWC features. Here we see that LSTM and MLP features obtain much higher predictive accuracy than simple sentiment or complaint feature groups. We also see that CNNs and RNNs perform poorly in this task, perhaps because they lack the syntactic information needed to assess the sentiment and complaint labels.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg.",
        "evidence": "From the table, we can see that the proposed CNN-LSTM-Ug-neg-Ant outperforms all the baselines. Notably, the proposed CNN-LSTM-Ug-neg-Ant improves upon the simple CNN-LSTM-w/o neg. baseline by 3.4% for positive sentiment and by 3.3% for negative sentiment, which amounts to an overall improvement of 5.8%.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "for example, for the system B in Table 2, the input systems are made available to the evaluation system, and this gives [BOLD] MUC-B1 (“E2”, default) a 4.6 precision point advantage over standard MUC-B1 (“E1”, see Table 2).",
        "evidence": "We believe that this is due to the fact that, in each case, the lemma text is made available to the evaluation system. By making the lemma text available to the evaluation system, we mean that the system can effectively make use of the lemma text in different ways, and so our model is able to gain an advantage over the standard MUC-B1 baseline. This is clear from the differences between the CoNLL F1 scores.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, while we notice a definite improvement over Peyrard and Gurevych (2018), our results still lack behind the golden-set correlation, suggesting that future work could further improve the capacity of learning summary-level correlation.",
        "evidence": "We notice a definite improvement over Peyrard and Gurevych (2018), although our results still cannot reach the performance of the method proposed by them. In particular, our MLP model with BERT as encoder has the best scores, which are significantly better than the scores reported by Peyrard and Gurevych. However, note that these results of Peyrard and Gurevych (2018) model is trained on the entire training set, while we do not use the test set for our experiments. Nevertheless, these results show that our model has the potential to achieve better results, and that our generalization framework could further improve the results.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is lower than Guo et al. (2019), a state-of-the-art model that does not employ external information.",
        "evidence": "In order to present the results from our G2S-GGNN model, we also compare it against recent state-of-the-art results. In order to avoid any influence of our approach using external information, we only compare our results against the state-of-the-art results that are cited above. The results demonstrate that our G2S-GGNN model achieves a BLEU score of 27.87, which is lower than the state-of-the-art model Guo et al. Moreover, our G2S-GAT model also outperforms its S2S and G2S-GIN baselines, demonstrating the effectiveness of the proposed approach.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees.",
        "evidence": "The results show that the throughput of our implementation is significantly higher than the results of unbalanced trees. More importantly, the results show that the information from the unbalanced trees is redundant, as we can still train the model with balanced trees.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is actually 4.8 percent (71.9% vs. 69.0%).",
        "evidence": "The claim is that BERT trained on Easy and Hard datasets achieves the best results amongst previous work. However, this improvement is almost entirely attributed to high accuracy on the Easy subset. In other words, we expect the Easy dataset to be very difficult for BERT, and thus the improvement of its performance over previous work is very small.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "covering the rare words can boost the performances across different out-of-domain (OOD) datasets significantly",
        "evidence": "Table 4 shows the performance boost from adding the rare words to all baseline models. [CONTINUE] The results show that coverage information considerably improves the performances of all baseline models. The resulting cross-out-of-domain gains are larger than those in-domain gains on the SNLI and Glockner datasets.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The system performs well on synthetic dataset with a minimum of 80% P@1 and 98% P@10.",
        "evidence": "The system performs well with a minimum of 80% accuracy for P@1 and 98% accuracy for P@10.",
        "table": "+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|    | [BOLD] Language   | [BOLD] # Test   | [BOLD] P@1   | [BOLD] P@3   | [BOLD] P@5   | [BOLD] P@10   | [BOLD] MRR   |\n+====+===================+=================+==============+==============+==============+===============+==============+\n|  0 | [BOLD] Language   | [BOLD] Samples  | [BOLD] P@1   | [BOLD] P@3   | [BOLD] P@5   | [BOLD] P@10   | [BOLD] MRR   |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  1 | Bengali           | 140000          | 91.30        | 97.83        | 98.94        | 99.65         | 94.68        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  2 | Czech             | 94205           | 95.84        | 98.72        | 99.26        | 99.62         | 97.37        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  3 | Danish            | 140000          | 85.84        | 95.19        | 97.28        | 98.83         | 90.85        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  4 | Dutch             | 140000          | 86.83        | 95.01        | 97.04        | 98.68         | 91.32        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  5 | English           | 140000          | 97.08        | 99.39        | 99.67        | 99.86         | 98.27        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  6 | Finnish           | 140000          | 97.77        | 99.58        | 99.79        | 99.90         | 98.69        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  7 | French            | 140000          | 86.52        | 95.66        | 97.52        | 98.83         | 91.38        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  8 | German            | 140000          | 87.58        | 96.16        | 97.86        | 99.05         | 92.10        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  9 | Greek             | 30022           | 84.95        | 94.99        | 96.88        | 98.44         | 90.27        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 10 | Hebrew            | 132596          | 94.00        | 98.26        | 99.05        | 99.62         | 96.24        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 11 | Hindi             | 140000          | 82.19        | 93.71        | 96.28        | 98.30         | 88.40        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 12 | Indonesian        | 140000          | 95.01        | 98.98        | 99.50        | 99.84         | 97.04        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 13 | Italian           | 140000          | 89.93        | 97.31        | 98.54        | 99.38         | 93.76        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 14 | Marathi           | 140000          | 93.01        | 98.16        | 99.06        | 99.66         | 95.69        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 15 | Polish            | 140000          | 95.65        | 99.17        | 99.62        | 99.86         | 97.44        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 16 | Portuguese        | 140000          | 86.73        | 96.29        | 97.94        | 99.10         | 91.74        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 17 | Romanian          | 140000          | 95.52        | 98.79        | 99.32        | 99.68         | 97.22        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 18 | Russian           | 140000          | 94.85        | 98.74        | 99.33        | 99.71         | 96.86        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 19 | Spanish           | 140000          | 85.91        | 95.35        | 97.18        | 98.57         | 90.92        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 20 | Swedish           | 140000          | 88.86        | 96.40        | 98.00        | 99.14         | 92.87        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 21 | Tamil             | 140000          | 98.05        | 99.70        | 99.88        | 99.98         | 98.88        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 22 | Telugu            | 140000          | 97.11        | 99.68        | 99.92        | 99.99         | 98.38        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 23 | Thai              | 12403           | 98.73        | 99.71        | 99.78        | 99.85         | 99.22        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 24 | Turkish           | 140000          | 97.13        | 99.51        | 99.78        | 99.92         | 98.33        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "G2S-GGNN does not outperform others with the same amount of Gigaword sentences (200K), as shown in Table 3, with a BLEU score of 32.23.",
        "evidence": "Here, we compare G2S-GGNN against other models with the same amount of gigaword sentences, as shown in Table 3. It can be seen from the table that G2S-GGNN outperforms other models with the same amount of gigaword sentences, with a BLEU score of 32.23, which is significantly higher than the BLEU score of 27.40 by Konstas et al. Song et al. Guo et al.",
        "table": "+----+-----------------------+-------------------------+---------------------+\n|    | <bold>Model</bold>    | <bold>External</bold>   | <bold>BLEU</bold>   |\n+====+=======================+=========================+=====================+\n|  0 | Konstas et al. (2017) | 200K                    | 27.40               |\n+----+-----------------------+-------------------------+---------------------+\n|  1 | Song et al. (2018)    | 200K                    | 28.20               |\n+----+-----------------------+-------------------------+---------------------+\n|  2 | Guo et al. (2019)     | 200K                    | 31.60               |\n+----+-----------------------+-------------------------+---------------------+\n|  3 | G2S-GGNN              | 200K                    | <bold>32.23</bold>  |\n+----+-----------------------+-------------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "TF and DF achieved different values of precision, recall and f-measure using the English corpora, with TF achieving a higher precision (P=0.0150) and f-measure (F=0.0293) than DF when using the Europarl corpus in English.",
        "evidence": "Similar to the German and Spanish corpora, TF achieves a higher precision (p=0.0150) and F-measure (F=0.0293) than DF.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.",
        "evidence": "We now proceed to analyze the impact of the language modeling loss in our model. Comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss significantly reduces PP, sometimes at the cost of semantic preservation. For example, M3, which includes the loss from CSC and CSC+lang, reduces PP from 22.3 to 39.2 and reduces the grammaticality score from 8.81 to 12.0. Similarly, M4, which includes the loss from CSC and CSC+para, reduces PP to 33.9 and further reduces the grammaticality score to 12.8.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.694 | 0.728        | [BOLD] 22.3 | 8.81        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.702 | 0.747        | 23.6        | 11.7        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.692 | 0.781        | 49.9        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.698 | 0.754        | 39.2        | 12.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.702 | 0.757        | 33.9        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.688 | 0.753        | 28.6        | 11.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.704 | [BOLD] 0.794 | 63.2        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.706 | 0.768        | 49.0        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] ACER and PPO obtain high performance in inform F1 and match rate as well.",
        "evidence": "From the table, we can see that GDPL achieves the best performance in Inform F1 and Match rate as well. GDPL even surpasses human in completing 80.9% of agenda turns. By comparing with GDPL-sess, we can see that incorporating structural information has great potential in improving the performance. This is mainly due to the fact that, GDPL-sess doesn’t need a lot of data to estimate ROUGE. By comparing with GDPL-discr, we can see that its performance is similar to GDPL. We also observe that PPO has similar performance with ACER and GDPL in terms of F1 and Match rate. The results confirm that PPO is also able to estimate ROUGE.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "these results show that the questionnaire takers had an average accuracy of 98.2% in answering word intrusion questions for words associated with meanings imparted by standard word embeddings,",
        "evidence": "Our results show that the questionnaire takers had an average accuracy of 98.2% in answering word intrusion questions. This level of precision is considerably higher than the scores reported by the NLG literature. However, there is a significant gap between our results and the results reported by the NLG literature. We believe that the difference is because the NLG literature provides additional information, which was not included in our questionnaire.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 25-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al.",
        "evidence": "We submitted two runs of our experiments. In the first, we used a 64.3% accuracy model trained on the WSJ data. In the second, we used ELMo Peters et al. emoji embeddings Pennington et al. We also considered concatenating the emoji embeddings instead of concatenating them, and fixing the learning rate much higher than 1. We observed that using a greater BiLSTM hidden size did not help the model. Optimization strategies were performed using the same learning rate schedule as described by Conneau et al. We also used POS embeddings which performed best with a hidden size of 50 and fixing the learning rate lr to 0.1.",
        "table": "+----+--------------------+-----------------------+-------------+\n|    | [BOLD] Variation   | [BOLD] Accuracy (%)   | [BOLD] Δ%   |\n+====+====================+=======================+=============+\n|  0 | Submitted          | [BOLD] 69.23          | -           |\n+----+--------------------+-----------------------+-------------+\n|  1 | No emoji           | 68.36                 | - 0.87      |\n+----+--------------------+-----------------------+-------------+\n|  2 | No ELMo            | 65.52                 | - 3.71      |\n+----+--------------------+-----------------------+-------------+\n|  3 | Concat Pooling     | 68.47                 | - 0.76      |\n+----+--------------------+-----------------------+-------------+\n|  4 | LSTM hidden=4096   | 69.10                 | - 0.13      |\n+----+--------------------+-----------------------+-------------+\n|  5 | LSTM hidden=1024   | 68.93                 | - 0.30      |\n+----+--------------------+-----------------------+-------------+\n|  6 | LSTM hidden=512    | 68.43                 | - 0.80      |\n+----+--------------------+-----------------------+-------------+\n|  7 | POS emb dim=100    | 68.99                 | - 0.24      |\n+----+--------------------+-----------------------+-------------+\n|  8 | POS emb dim=75     | 68.61                 | - 0.62      |\n+----+--------------------+-----------------------+-------------+\n|  9 | POS emb dim=50     | 69.33                 | + 0.10      |\n+----+--------------------+-----------------------+-------------+\n| 10 | POS emb dim=25     | 69.21                 | - 0.02      |\n+----+--------------------+-----------------------+-------------+\n| 11 | SGD optim lr=1     | 64.33                 | - 4.90      |\n+----+--------------------+-----------------------+-------------+\n| 12 | SGD optim lr=0.1   | 66.11                 | - 3.12      |\n+----+--------------------+-----------------------+-------------+\n| 13 | SGD optim lr=0.01  | 60.72                 | - 8.51      |\n+----+--------------------+-----------------------+-------------+\n| 14 | SGD optim lr=0.001 | 30.49                 | - 38.74     |\n+----+--------------------+-----------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The improvement from automatic AMR to gold AMR (+0.7 BLEU) is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy.",
        "evidence": "We also investigated whether our model can be further improved with an increase of AMR parsing accuracy. We involved two human assessors to evaluate the quality of the AMR parsing. The first one is the same as the one used for evaluating the performance of our system in the official release, and the second one is another one of our colleagues. It shows that the improvement from automatic AMR to gold AMR is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy.",
        "table": "+----+-------------+---------------+\n|    | AMR Anno.   | BLEU          |\n+====+=============+===============+\n|  0 | Automatic   | 16.8          |\n+----+-------------+---------------+\n|  1 | Gold        | [BOLD] *17.5* |\n+----+-------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "This is expected because SVM is a linear classifier that relies solely on the 1-gram, while neural classifiers like CNNs (row3) and LSTMs (row4) cannot learn non-linear function of n-grams when the n is larger than the number of word vector dimensions.",
        "evidence": "The results show that CNN-LSTM is more effective for sentiment task than SVM and its variants. This is expected because SVM can learn a linear classifier that relies on the 1-gram. While neural classifiers like CNN and LSTM can learn non-linear functions of n-grams, they cannot do so when the n is larger than the number of word vector dimensions. Our model that uses learned negation can adapt to this problem, and the results show that the learned negation can be useful even for a linear classifier that can only learn linear functions of n-grams.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "for example, if the evaluation begins by calculating all analogies using the relation ‘capital-common-countries’ then in analogy 1, there will be [15_1 + 15_2 + 6_2]/50 = 6.3 out of 50 answers found by 1—1, i.e., 12.6% of 50 is counted as the score.",
        "evidence": "It can be seen that the alternative approaches that aim to improve interpretability, have poor performance on analogy tasks. However, our proposed method has comparable performance with the original GloVe embeddings.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs.",
        "evidence": "Asbeck et al. From the table, we can see that DCGCN achieves comparable or better performance compared to recurrent graph encoders for all three measures, namely, #P, B and C.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The performance of each approach that interacts with the agenda-based user simulator is shown in [CONTINUE] Table 3.",
        "evidence": "The user simulator is a sequence-to-sequence model that takes as input the graph-structured belief state and interacts with the agent. We use the ground truth agenda for updating and updating state tracking and conversation state tracking. Although GDPL achieves a lower task success rate than GDPL-sess, GDPL achieves a higher task success rate than GDPL-discr. All three versions of GDPL outperform human in every metric. GDPL is rated higher at 94.97% in task inform and 83.90% in task success, while GDPL-discr has the highest task success rate.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Opinion distance methods generally outperform the competition on both ARI and Silhouette coefficient.",
        "evidence": "The ARI and Silhouette coefficient are two complementary measures which measure the quality of an image’s content through its visualization. The ARI is defined as follows: ARI = [1,2]∼N(document, image)∼N(document, image) (for Atheism, there is no ARI measure, since we use the statistic REL.) Silhouette coefficient = [1,2]∼N(document, image)∼N(document, image)∼N(document, image) For example, the monarchy has a ARI of 0.23, but the Silhouette coefficient is only 0.03. [CONTINUE] So, we simply use the ARI metric without the Silhouette coefficient, treating them as the same. Each word in the dataset is labeled with its corresponding ARI and Silhouette coefficient.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The complete model has significantly more parameters than the model without graph encoders (57.6M vs 61.7M).",
        "evidence": "The model without graph encoders, i.e., BiLSTM<> + BiLSTM, seems to have similar performance as the model with pre-trained embeddings, which confirms the results from the Reddit corpus. However, the model with pre-trained embeddings + GCN and <italic>GE> has significantly higher BLEU and METEOR scores than the model without graph encoders, indicating that explicitly modeling compositionality is crucial. The gain in BLEU score from <italic>GEb</italic> is smaller than that from <italic>get</italic>ge. We believe that this is because the city entity graph largely contains missing relations, which pre-trained language models can’t correctly capture.",
        "table": "+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|    | <bold>Model</bold>                                   |   <bold>BLEU</bold> |   <bold>METEOR</bold> | <bold>Size</bold>   |\n+====+======================================================+=====================+=======================+=====================+\n|  0 | biLSTM                                               |               22.5  |                 30.42 | 57.6M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|  1 | <italic>GEt</italic> + biLSTM                        |               26.33 |                 32.62 | 59.6M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|  2 | <italic>GEb</italic> + biLSTM                        |               26.12 |                 32.49 | 59.6M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|  3 | <italic>GEt</italic> + <italic>GEb</italic> + biLSTM |               27.37 |                 33.3  | 61.7M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "Unlike the above three models, Word2Sense does not use pretrained vectors.",
        "evidence": "The claim this model performs better than all the baselines lies with GloVe Pennington et al. et al. Word2Sense is not better than the same three baselines by large margins. It is due to the fact that it uses the entire word vector space as input, this space is significantly more sparse than GloVe and consequently the word vectors have better quality in general. Verb-143 is the worst of the three real word sense models, close to the Word2Sense model. This is mainly because the training data is small and we fixed the word embedding in fine-tuning step.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, coverage can compensate for much of the lost performance in each case.",
        "evidence": "The claim here is that coverage can compensate for much of the lost performance in each case. In particular, when compared against MQAN, the gains due to coverage are>73.84% for in-domain MultiNLI,>65.38% for out-of-domain SNLI, and>52.65% for out-of-domain SICK.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We found that rephrase disfluencies that contain content words are easier for the model to detect, compared to rephrases with function words only, and error decreases for longer disfluencies.",
        "evidence": "Overall, we see that disfluencies which include content words are easier for the model to detect than those containing function words. We observe that content-content disfluencies are slightly easier to detect than function-function disfluencies. This may be due to the fact that content words can provide useful information to the model during prediction. In addition, we observe that function words are easier for the model to detect also, compared to rephrases with function words only. The error decreases for longer disfluencies perhaps because longer contexts provide more information to the model.",
        "table": "+----+-------------------+----------------------------------------+----------------------------------------+\n|    | [BOLD] Type       | [BOLD] Reparandum Length  [BOLD] 1-2   | [BOLD] Reparandum Length  [BOLD] 3-5   |\n+====+===================+========================================+========================================+\n|  0 | content-content   | 0.61 (30%)                             | 0.58 (52%)                             |\n+----+-------------------+----------------------------------------+----------------------------------------+\n|  1 | content-function  | 0.77 (20%)                             | 0.66 (17%)                             |\n+----+-------------------+----------------------------------------+----------------------------------------+\n|  2 | function-function | 0.83 (50%)                             | 0.80 (32%)                             |\n+----+-------------------+----------------------------------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "We additionally find that supervised BLEU does not show a trade-off with Acc: for a single model type, higher Acc does not necessarily correspond to lower BLEU.",
        "evidence": "It should be emphasized that these results show that for any model type, higher accuracy does not necessarily correspond to lower BLEU. We also find that the supervised BLEU scores of some models (e.g., Fu-1, Multi-Decoder, Style Embedding) are markedly lower than the supervised BLEU scores of the untransferred models, showing that the use of a style classifier does not necessarily hurt performance in terms of supervised BLEU.",
        "table": "+----+----------------------+----------------------+----------------------+\n|    | Model                | BLEU                 | Acc∗                 |\n+====+======================+======================+======================+\n|  0 | fu-1                 | [EMPTY]              | [EMPTY]              |\n+----+----------------------+----------------------+----------------------+\n|  1 | Multi-decoder        | 7.6                  | 0.792                |\n+----+----------------------+----------------------+----------------------+\n|  2 | Style embed.         | 15.4                 | 0.095                |\n+----+----------------------+----------------------+----------------------+\n|  3 | simple-transfer      | simple-transfer      | simple-transfer      |\n+----+----------------------+----------------------+----------------------+\n|  4 | Template             | 18.0                 | 0.867                |\n+----+----------------------+----------------------+----------------------+\n|  5 | Delete/Retrieve      | 12.6                 | 0.909                |\n+----+----------------------+----------------------+----------------------+\n|  6 | yang2018unsupervised | yang2018unsupervised | yang2018unsupervised |\n+----+----------------------+----------------------+----------------------+\n|  7 | LM                   | 13.4                 | 0.854                |\n+----+----------------------+----------------------+----------------------+\n|  8 | LM + classifier      | [BOLD] 22.3          | 0.900                |\n+----+----------------------+----------------------+----------------------+\n|  9 | Untransferred        | [BOLD] 31.4          | 0.024                |\n+----+----------------------+----------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "When humans are asked to choose an option which they believe is more likely to be a correct causal conclusion, 80% select the correct label.",
        "evidence": "The claim-selection task was performed on the RoBERTa and BERT-large corpora. We perform a random sample of 1000 items and asked the subjects to select which of the two labels is more likely to be the correct causal conclusion. The RoBERTa-large results outperform all baselines by a large margin, with p-values of only 2.7. Moreover, when fine-tuned on the CoNLL-2019 data, the RoBERTa-large achieves an accuracy of 87.7, which is more than 11% better than the best baseline and 7% better than the previous SOTA.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions.",
        "evidence": "We observe that the main effect on SER is from cleaning the missed slots. In other words, the results in the bottom half of Table 2 and 3 show that cleaning the slots dramatically reduces the insertion and deletions. From the results, we can see that the introduction of the proposed pre-trained word vectors improves the performance of both models. However, the improvement is much stronger on the models designed for generation tasks, i.e., TGen and SC-LSTM, than on those designed for generation tasks.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "We believe that this is because ‘to’ is either a correct or an incorrect cue: to achieve high consistency, an MLM will more likely answer when the cue is “to”, and answer the other alternative, otherwise.",
        "evidence": "The MLM is more likely to answer when the cue is “To” than when the cue is “IN”, consistent with our previous analysis. We believe that this is because the cues are either a correct or an incorrect cue.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm, outperforming all the alternatives except Word2Vec baseline on average.",
        "evidence": "S5SS3SSS0Px1 Results: We run our experiment for once more and compared the obtained scores to the baseline algorithms. We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm. Moreover, our system outperforms all the alternatives on average by almost 5 percentage points.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "supports"
    },
    {
        "claim": "We observe an improvement in performance between PG-original and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model).",
        "evidence": "From the table, we can see that: 1) MMR improves the accuracy of PG-original and PG-MMR. We assume that this is because MMR learns domain-invariant feature space and thus the model can better utilize the sentiment information contained in the multi-label classification dataset. 2) The pre-trained PG-original and MMR are very close to each other. This indicates that our model is compatible with the pre-trained PG-original and strong enough to be applied on top of the PG-original. 3) Hi-MAP outperforms all the baselines, which indicates the effectiveness of our proposed model.",
        "table": "+----+------------------------------------------+--------------+--------------+---------------+\n|    | [BOLD] Method                            | [BOLD] R-1   | [BOLD] R-2   | [BOLD] R-SU   |\n+====+==========================================+==============+==============+===============+\n|  0 | First-1                                  | 26.83        | 7.25         | 6.46          |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  1 | First-2                                  | 35.99        | 10.17        | 12.06         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  2 | First-3                                  | 39.41        | 11.77        | 14.51         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  3 | LexRank Erkan and Radev ( 2004 )         | 38.27        | 12.70        | 13.20         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  4 | TextRank Mihalcea and Tarau ( 2004 )     | 38.44        | 13.10        | 13.50         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  5 | MMR Carbonell and Goldstein ( 1998 )     | 38.77        | 11.98        | 12.91         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  6 | PG-Original Lebanoff et al. ( 2018 )     | 41.85        | 12.91        | 16.46         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  7 | PG-MMR Lebanoff et al. ( 2018 )          | 40.55        | 12.36        | 15.87         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  8 | PG-BRNN Gehrmann et al. ( 2018 )         | 42.80        | 14.19        | 16.75         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  9 | CopyTransformer Gehrmann et al. ( 2018 ) | [BOLD] 43.57 | 14.03        | 17.37         |\n+----+------------------------------------------+--------------+--------------+---------------+\n| 10 | Hi-MAP (Our Model)                       | 43.47        | [BOLD] 14.89 | [BOLD] 17.41  |\n+----+------------------------------------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus.",
        "evidence": "The claim makes use of data from two different annotator corpora. In the black-aligned corpus, tweets are classified as containing sexism almost twice as frequently and 1.1 times as frequently as containing racism and sexism compared to those in the white-aligned corpus. Tweets in the black-aligned corpus are also classified as containing more instances of sexism than those in the white-aligned corpus.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "In comparison, GDPL is still comparable with ACER and PPO, but does not obtain a better match rate, and even achieves lower task success.",
        "evidence": "From this table, we see that GDPL does not significantly outperform the baseline methods, and in fact performs worse than ALDM even though GDPL achieves similar task success.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "For example, GCN+RC+LA (10) achieves a BLEU score of 52.9, which is better than GCN+RC+LA (9).",
        "evidence": "The results of the proposed model GCN+RC+LA (10) achieves a BLEU score of 52.9, which is better than GCN+RC+LA (9). Similar results can be seen in the comparison between GCN1 and GCN2. In particular, we can see that with the increase of the sentence length, the performance of GCN+RC+LA (9) improves significantly over GCN+RC+LA (2). In particular, we can see that with sentence length increases, the performance of GCN+RC+LA (9) is even better than GCN+RC+LA (10). These results demonstrate the effectiveness of our proposed model.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "All G2S models have lower entailment compared to S2S.",
        "evidence": "From this table, we see that the G2S models tend to have lower entailment scores than S2S models, which confirms our claim that GNN-based models can capture better semantics than RNN-based models. Moreover, the results in the table also show that our GNN-based models have comparable performance to the S2S models in the entailment task.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.4% F1 score decrease (A2−A1).",
        "evidence": "We performed the experiment on the baseline BiLSTM-CNN model. We can see in (a1) the dependency distance-based weight factor is critical for the model. Without this weight factor, the model degrades from precision to recall and the F1 score decreases significantly. In addition, the model trained with standard attention did not achieve good results.",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "The difference between accuracy on Easy and Hard is more pronounced for RoBERTa, suggesting a reliance on superficial cues.",
        "evidence": "The comparison between accuracy on Easy and Hard shows that RoBERTa relies on superficial cues more than BERT and Sasaki.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "In general, we found when the parameter budget is the same, shallower DCGCN models can obtain better results than the deeper ones.",
        "evidence": "We ran an experiment to investigate whether the parameter budget is the same across models. Specifically, we vary the parameter budget D in DCGCN(1) and DCGCN(4) and measure the change of BLEU scores when the parameter budget is the same. We observe that models with smaller parameter budgets (DCGCN(1) and DCGCN(3) can obtain better performance. With the same parameter budget, the shallower DCGCN models (DCGCN(2) and DCGCN(4) can outperform the deeper ones (DCGCN(1) and DCGCN(2)).",
        "table": "+----+----------------+-----+-------+-------------+-------------+\n|    | [BOLD] Model   |   D | #P    | B           | C           |\n+====+================+=====+=======+=============+=============+\n|  0 | DCGCN(1)       | 300 | 10.9M | 20.9        | 52.0        |\n+----+----------------+-----+-------+-------------+-------------+\n|  1 | DCGCN(2)       | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3 |\n+----+----------------+-----+-------+-------------+-------------+\n|  2 | DCGCN(2)       | 240 | 11.3M | 22.8        | 52.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  3 | DCGCN(4)       | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  4 | DCGCN(1)       | 420 | 12.6M | 22.2        | 52.4        |\n+----+----------------+-----+-------+-------------+-------------+\n|  5 | DCGCN(2)       | 300 | 12.5M | 23.8        | 53.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  6 | DCGCN(3)       | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1 |\n+----+----------------+-----+-------+-------------+-------------+\n|  7 | DCGCN(2)       | 360 | 14.0M | 24.2        | [BOLD] 54.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  8 | DCGCN(3)       | 300 | 14.0M | [BOLD] 24.4 | 54.2        |\n+----+----------------+-----+-------+-------------+-------------+\n|  9 | DCGCN(2)       | 420 | 15.6M | 24.1        | 53.7        |\n+----+----------------+-----+-------+-------------+-------------+\n| 10 | DCGCN(4)       | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8 |\n+----+----------------+-----+-------+-------------+-------------+\n| 11 | DCGCN(3)       | 420 | 18.6M | 24.5        | 54.6        |\n+----+----------------+-----+-------+-------------+-------------+\n| 12 | DCGCN(4)       | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------+-----+-------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other.",
        "evidence": "The results seem to show that the proposed CMOW method performs better than the 784-dimensional CBOW model. We observe that the performance of CMOW is almost on par with the human-annotated CBOW plots in the plot. This suggests that the proposed hierarchical models may be complementary to each other.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "However, we define that the “119.99” operator as: if RSI <= 119.99 then RSI meets the requirement.",
        "evidence": "We use the system presented in [wang-EtAl:2014:Long] for our comparison. We only consider the results of the system proposed by [wang-EtAl:2014:Long] to verify whether the proposed method can effectively improve the performance. However, there is still a large gap between our results and the ground truth.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The hybrid model does not yield scores close to or even above the better model of the two on all tasks.",
        "evidence": "The results show that the hybrid model does not perform close to or even above the better model of the two methods on all tasks. This is not surprising, given that the words that occupy a relatively large proportion of the input sequence are usually the ones that appear in the videos, which are modelled in a word-level fashion.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The performances of all models decrease as the diameters of the graphs increase.",
        "evidence": "The model improvements confirm the hypothesis that the model parameters become smaller as the diameter of the graphs increases. More specifically, the performance of G2S-GAT is slightly better than S2S when the diameter is 0 to 3 δ and from 0 to 7 δ. The model improves the performance of S2S when the diameter is greater than 4-8 δ, but the performance degrades when the diameter is less than 7-13 δ. This suggests that the GAT model can master the global structure of the graph better than the S2S model because the global structure can control the size of the features better.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "For BERT models, after fine-tuning on COPA, RoBERTa-large achieves the best performance on both Easy and Hard.",
        "evidence": "For both the BERT and RoBERTa models, fine-tuning on B-COPA improves performance on the Hard subset. The improvement is less pronounced for the RoBERTa-Large model, which is expected as the B-COPA dataset is already very large. Nevertheless, fine-tuning on B-COPA improves accuracy on the Easy and Hard subsets by more than 2.5% and 2.1%, respectively. Moreover, fine-tuning on B-COPA improves the performance on the Hard subset by more than 2% on both the BERT-Large and RoBERTa-Large models. These results show that BERT and RoBERTa are complementary to each other on this task.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc.",
        "evidence": "We can see that the best results are obtained by the multilingual model M6: M6+para and M6+lang. The results are slightly worse than the results achieved by the lexical-based model M0: Shen-1 under similar acc. This can be explained by the fact that the number of similar words in Shen-1 is much fewer than those in similar datasets. As a result, the lexical-based model is difficult to generalize.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.818 | 0.719        | 37.3        | 10.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.819 | 0.734        | 26.3        | 14.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.813 | 0.770        | 36.4        | 18.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.807 | 0.796        | 28.4        | 21.5        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.798 | 0.783        | 39.7        | 19.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785        | 27.1        | 20.3        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.805 | [BOLD] 0.817 | 43.3        | 21.6        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.818 | 0.805        | [BOLD] 29.0 | [BOLD] 22.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Interestingly, the error analysis on this dataset revealed that the BiLSTM model was unable to correctly classify one-word scopes.",
        "evidence": "The model’s performance in terms of F-score is slightly lower than the PCS model’s performance. This is primarily because the PCS model was trained and tested on the CoNLL-2009 dataset, which is more similar to the CoNLL-2009 dataset than to the CoNLL-2009 dataset.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The models have worse results when handling sentences with 20 or fewer tokens.",
        "evidence": "From the table, we see that the models GGNN and G2S-GAT have better performance than the S2S and G2S-GGNN in sentence length of 0-3 δ and 10-18 δ. Also, the models GGNN and G2S-GAT have better performance in sentence length of 0-7 δ and 14-20 δ than S2S and G2S-GGNN. The results show that the GGNN and GAT can handle short sentences better than the S2S and G2S.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously.",
        "evidence": "The first two rows demonstrate that, due to the joint training, our hybrid model can learn to pick up the best features from both CBOW and CMOW simultaneously. As observed, the performance of CBOW and CMOW decreases as the dimensions of the embeddings increase. This is expected as the embeddings themselves capture deeper linguistic relations than word embeddings, which is similar to the observation on the development set. Overall, we can see that thanks to joint training, our hybrid model can achieve further improvements over all embeddings, which demonstrates that the two methods can coexist harmoniously, and their differences in performance are significant.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Despite filtering out multiple hypernyms, the recall values for the Portuguese corpora are still relatively high.",
        "evidence": "The recall values for the Portuguese corpora are relatively high, which is expected considering the high size of the English one.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS |     TF |     DF | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+========+========+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1038 | 0.017  | 0.049  | 0.0641 | 0.0641 | 0.0613        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.041  | 0.027  | 0.027  | 0.1154        |   0.0661 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.6185        | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.6308        | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 |   0.5295 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  4 | R         | EN     | Europarl  | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0011        | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 |   0.0018 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0012        | 0.0008 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0016 |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0003        | 0.0009 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0017 |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  8 | F         | EN     | Europarl  | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0022        | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 |   0.0036 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0024        | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 |   0.0023 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0005        | 0.0018 | 0.0018 | 0.002  | 0.0021 | [BOLD] 0.0034 |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9.",
        "evidence": "According to the results, 1.1% absolute accuracy gain is achieved by using BERT in our approach. We believe that this is because BERT embedding is highly correlated with human judgement, which provides additional guidance to our model. 2.2% absolute accuracy gain is achieved by incorporating BERT into our model. We believe that this is because BERT embedding and normalization are very complementary to one another. 3.5% further improvement is achieved when BERT is incorporated. Overall, our model achieves an accuracy of 89.93%, which is 0.2% improvement over the best baseline (SRU) and 0.3% improvement over LSTM.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are not significantly associated with tweets that are not complaints.",
        "evidence": "Top unigrams and part-of-speech features (unigrams) for complaints and not complaints are presented in Table [CONTINUE] However, there are several interesting things to note. First, words referring to negative states, such as gratitude (thank, great, love) are not significantly associated with complaints. This is not surprising, since such words tend to refer to good things and are usually used in positive ways. Second, the presence of terms that show positive states in tweets that are not complaints are also not significant. For example, the presence of terms “good karma” and “goodwill” among top unigrams for complaints may suggest that the user is describing personal experiences, and these words do not lead to complaints. Third, about half of the items in the list of unigrams and part-of-speech features are not complaints. This is quite encouraging, since these items usually have good quality and are not complaints.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "RANDOM is indeed closer here to the expected 50% and other baselines are closer to gender-parity.",
        "evidence": "We see that the expected 50% gender-parity result is closer to gender-neutral than it is to gender-parity, not by a large margin, but closer. Unsurprisingly, token-level similarity and topical entity-based similarity are closer to gender-parity than other baselines.",
        "table": "+----+--------------------+-------------+-------------+-----------------------+-------------+\n|    | [EMPTY]            | M           | F           | B                     | O           |\n+====+====================+=============+=============+=======================+=============+\n|  0 | Random             | 47.5        | 50.5        | [ITALIC] 1.06         | 49.0        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  1 | Token Distance     | 50.6        | 47.5        | [ITALIC] 0.94         | 49.1        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  2 | Topical Entity     | 50.2        | 47.3        | [ITALIC] 0.94         | 48.8        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  3 | Syntactic Distance | 66.7        | 66.7        | [ITALIC]  [BOLD] 1.00 | 66.7        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  4 | Parallelism        | [BOLD] 69.3 | [BOLD] 69.2 | [ITALIC]  [BOLD] 1.00 | [BOLD] 69.2 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  5 | Parallelism+URL    | [BOLD] 74.2 | [BOLD] 71.6 | [ITALIC]  [BOLD] 0.96 | [BOLD] 72.9 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  6 | Transformer-Single | 59.6        | 56.6        | [ITALIC] 0.95         | 58.1        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  7 | Transformer-Multi  | 62.9        | 61.7        | [ITALIC] 0.98         | 62.3        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general.",
        "evidence": "From this table, we have confirmed that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns. In general, this is by far the majority category, where Google Translate has the highest probability for translating sentences with male pronouns. It also has the highest percentage of female pronouns in all the other categories it is exposed to.",
        "table": "+----+------------------------------------------------+--------------+------------+---------------+\n|    | Category                                       |   Female (%) |   Male (%) |   Neutral (%) |\n+====+================================================+==============+============+===============+\n|  0 | Office and administrative support              |       11.015 |     58.812 |        16.954 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  1 | Architecture and engineering                   |        2.299 |     72.701 |        10.92  |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  2 | Farming, fishing, and forestry                 |       12.179 |     62.179 |        14.744 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  3 | Management                                     |       11.232 |     66.667 |        12.681 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  4 | Community and social service                   |       20.238 |     62.5   |        10.119 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  5 | Healthcare support                             |       25     |     43.75  |        17.188 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  6 | Sales and related                              |        8.929 |     62.202 |        16.964 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  7 | Installation, maintenance, and repair          |        5.22  |     58.333 |        17.125 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  8 | Transportation and material moving             |        8.81  |     62.976 |        17.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  9 | Legal                                          |       11.905 |     72.619 |        10.714 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 10 | Business and financial operations              |        7.065 |     67.935 |        15.58  |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 11 | Life, physical, and social science             |        5.882 |     73.284 |        10.049 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 12 | Arts, design, entertainment, sports, and media |       10.36  |     67.342 |        11.486 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 13 | Education, training, and library               |       23.485 |     53.03  |         9.091 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 14 | Building and grounds cleaning and maintenance  |       12.5   |     68.333 |        11.667 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 15 | Personal care and service                      |       18.939 |     49.747 |        18.434 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 16 | Healthcare practitioners and technical         |       22.674 |     51.744 |        15.116 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 17 | Production                                     |       14.331 |     51.199 |        18.245 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 18 | Computer and mathematical                      |        4.167 |     66.146 |        14.062 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 19 | Construction and extraction                    |        8.578 |     61.887 |        17.525 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 20 | Protective service                             |        8.631 |     65.179 |        12.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 21 | Food preparation and serving related           |       21.078 |     58.333 |        17.647 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 22 | Total                                          |       11.76  |     58.93  |        15.939 |\n+----+------------------------------------------------+--------------+------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "PCS can detect 4,113 new scope relations, 833 fewer than with gold cues.",
        "evidence": "The claim prediction results are shown in Table 3. Our proposed method compares favourably against the baselines. [CONTINUE] Surprisingly, a simple PCA approach is able to achieve much better performance than the neural models, with F-score of 0.85 compared to 0.66 for in-scope and 0.72 compared to 0.72 for out-ofscope.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) underperforms ORACLE by a large margin.",
        "evidence": "Table. Our model (OURS) outperforms all existing methods by a large margin. It closely matches the performance of ORACLE with only a 0.40% difference. [CONTINUE] Specifically, all rationale-augmented approaches (RA-Trans, RA-SVM, and OURS) outperform the rationale-free baseline by a large margin. This confirms our hypothesis that human palate is highly subjective.",
        "table": "+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source            | Target      |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+===================+=============+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer aroma+palate | Beer look   | 74.41 |     74.83 |     74.94 |    72.75 |        76.41 | [BOLD] 79.53 |     80.29 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look+palate  | Beer aroma  | 68.57 |     69.23 |     67.55 |    69.92 |        76.45 | [BOLD] 77.94 |     78.11 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look+aroma   | Beer palate | 63.88 |     67.82 |     65.72 |    74.66 |        73.4  | [BOLD] 75.24 |     75.5  |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "supports"
    },
    {
        "claim": "GDPL outperforms three baselines significantly in all aspects (sign test, p-value < 0.01) except for the quality compared with ACER.",
        "evidence": "We can see that GDPL outperforms the three baselines significantly in all aspects (sign test, p-value < 0.01) except for the quality compared with ACER.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Lemma-based targets without POS disambiguation perform best on WN-N when dependency-based contexts are used; however, the difference to lemmatized and disambiguated targets is not statistically significant (p > .1).",
        "evidence": "The results show that lemmatized targets without POS disambiguation perform best on WN-N. [CONTINUE] This is consistent with the results using dependency-based contexts for VN. [CONTINUE] Lemmatized targets significantly outperform type targets for both types in terms of F-measure. [CONTINUE] POS disambiguation significantly improves performance for all cases with respect to VN (p > .1). Lemma-based targets without disambiguation outperform disambiguated targets on WN-N when dependency-based contexts are used. However, the difference to lemmatized and disambiguated targets is not statistically significant (p > .1).",
        "table": "+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|    | [EMPTY]      | WN-N P            | WN-N R            | WN-N F            | WN-V P            | WN-V R            | WN-V F            | VN P              | VN R              | VN F              |\n+====+==============+===================+===================+===================+===================+===================+===================+===================+===================+===================+\n|  0 | Context: w2  | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  1 | type         | .700              | .654              | .676              | .535              | .474              | .503              | .327              | .309              | .318              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  2 | x+POS        | .699              | .651              | .674              | .544              | .472              | .505              | .339              | .312              | .325              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  3 | lemma        | .706              | .660              | .682              | .576              | .520              | .547              | .384              | .360              | .371              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  4 | x+POS        | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  5 | Context: dep | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  6 | type         | .712              | .661              | .686              | .545              | .457              | .497              | .324              | .296              | .310              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  7 | x+POS        | .715              | .659              | .686              | .560              | .464              | .508              | .349              | .320              | .334              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  8 | lemma        | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591              | .512              | .548              | .408              | .371              | .388              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  9 | x+POS        | .722              | .666              | .693              | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "Another interesting fact in Table 1 is that the training throughput on the linear dataset does not scale better than the throughput on the balanced dataset, as the batch size increases.",
        "evidence": "As shown in Table 1, the training throughput of the linear dataset does not scale better than the throughput on the balanced dataset. Particularly, as the batch size increases, the throughput on the balanced dataset only increases by 7.6% while the throughput on the linear dataset only increases by 22.7%. This is very interesting, since the model is not trained to predict the features of the machine readable speech.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Negation can be either clearly express or be subtly used.",
        "evidence": "The primary source of disagreement in our sample of articles can be traced primarily to the use of negation cues. In news media, it is common that false negation cues are used in unclear, ambiguous or incomplete ways. In contrast, negation can be either clearly express or be subtly used. The average number of negation cues per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 4 shows that GDPL has the smallest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves more like the human.",
        "evidence": "This suggests that GDPL behaves more like the human compared to the baselines. GDPL has the smallest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves like the human..",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "supports"
    },
    {
        "claim": "This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations.",
        "evidence": "The summarization module of the DCGCN4 model shows that enriching the input graph with the global node and including the linear combination can help the model to learn better information aggregations and produce more expressive graph representations. [CONTINUE] From the table, we can see that after removing the linear combination, the performance of the summarization module drops 1.3% on B and 1.8% on C. This suggests that enriching the input graph with the global node and including the linear combination can help the model to learn better information aggregations and produce more expressive graph representations.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "MIL-ND does not achieve higher precision, recall, and F1 than MIL, and using its confidence at test time (τ MIL-ND, 'All' setting) was not beneficial in terms of precision and F1.",
        "evidence": "This does not mean that NER is not useful. Specifically, we see that Model 2 does not outperform MIL in terms of all F1 metrics. In fact, Model 2 performs even worse than MIL in terms of both precision and recall. Since we attribute this to the mismatch between training and test time, we believe that τMIL-ND, which uses the confidence at test time, was not effective in terms of precision and F1.",
        "table": "+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|    | System                     | All P        | All R        | All F1             | In  [ITALIC] E+ P   | In  [ITALIC] E+ R   | In  [ITALIC] E+ F1   |\n+====+============================+==============+==============+====================+=====================+=====================+======================+\n|  0 | Name matching              | 15.03        | 15.03        | 15.03              | 29.13               | 29.13               | 29.13                |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  1 | MIL (model 1)              | 35.87        | 35.87        | 35.87 ±0.72        | 69.38               | 69.38               | 69.38 ±1.29          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  2 | MIL-ND (model 2)           | 37.42        | [BOLD] 37.42 | 37.42 ±0.35        | 72.50               | [BOLD] 72.50        | [BOLD] 72.50 ±0.68   |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  3 | [ITALIC] τMIL-ND (model 2) | [BOLD] 38.91 | 36.73        | [BOLD] 37.78 ±0.26 | [BOLD] 73.19        | 71.15               | 72.16 ±0.48          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  4 | Supervised learning        | 42.90        | 42.90        | 42.90 ±0.59        | 83.12               | 83.12               | 83.12 ±1.15          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "Table 9: Performance of different models on the neural user simulator.",
        "evidence": "the agent’s success rate, informing and match rate, and the user simulator’s success rate. As it can be seen, GDPL outperforms the other methods on all metrics. In particular, GDPL has achieved 19.7% match rate, which is 2.7% higher than ACER and PPO.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "For Task B, all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings.",
        "evidence": "For Task B, all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings.",
        "table": "+----+--------------------------------------+--------------------+\n|    | [EMPTY]                              | Micro F1           |\n+====+======================================+====================+\n|  0 | Baseline                             | 0.709              |\n+----+--------------------------------------+--------------------+\n|  1 | W2V (<italic>d</italic>=50)          | 0.736              |\n+----+--------------------------------------+--------------------+\n|  2 | W2V (<italic>d</italic>=500)         | 0.753              |\n+----+--------------------------------------+--------------------+\n|  3 | S2V                                  | 0.748              |\n+----+--------------------------------------+--------------------+\n|  4 | S2V + W2V (<italic>d</italic>=50)    | 0.744              |\n+----+--------------------------------------+--------------------+\n|  5 | S2V + K + W2V(<italic>d</italic>=50) | 0.749              |\n+----+--------------------------------------+--------------------+\n|  6 | SIF (DE)                             | 0.759              |\n+----+--------------------------------------+--------------------+\n|  7 | SIF (DE-EN)                          | <bold>0.765</bold> |\n+----+--------------------------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "LSTM does significantly better than Word2Vec, especially for MEN-TR-3k (0.766 vs. 0.552, p-value<0.00001), RG65 (0.790 vs. 0.744, p-value<0.00001) and MTurk771 (0.682 vs. 0.650, p-value<0.00001).",
        "evidence": "The claim: We compare the performance of LSTM with GloVe, word2vec, PCA-VAE, SOV, Word2Sense and the baselines. It can be seen that LSTM performs much better than all the baselines.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "A potential reason is that the RL agent has only learned limited useful signals in some small, low-quality data; therefore, other summarisation signals including aspect modelling, coverage modelling and salience modelling are missing.",
        "evidence": "This could be because the RL agent has learned limited useful signals in some small, low-quality data. Since its vocabularies are limited, its summarisation signal generator is not able to effectively use aspect modelling, coverage modelling and salience modelling, which are missing in our work.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Our proposed method outperforms Pretrained Word2Sense embeddings, despite the latter having the advantage of training on a larger corpus.",
        "evidence": "Our proposed method appears to outperform the pretrained Word2Sense embeddings on five out of the eight tasks. The results also show that, although the word2sense embeddings have benefited from training on a larger corpus, our method still performs better than the latter.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "evidence": "The ARI and Silhouette coefficients scores of both OD methods (OD-D2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.. [CONTINUE] We evaluate our OD methods on a random sample of 1,413 tweets, covering topics from diverse domains. Opinion polling averages (OS) are used for evaluation. The best performing baseline is TF-IDF, followed by SENT2Vec and Doc2Vec. OD-w2v outperforms the others by a large margin, with some very close results to the best baseline.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005).",
        "evidence": "Our results are shown in Table. Our model Candela significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < .0005). Specifically, candela obtains a 21% relative improvement compared to the best retrieval model (H&W Hao and Wang We also have strong results without the PSG component, which demonstrates the effectiveness of our design.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "based on the analysis results, we conclude that the dialog states were successfully retained in the model policy and user simulator, but the source of error lies in the action selection with respect to informability and matching.",
        "evidence": "Overall, the results show that the informability of the dialog states was retained in the model policy and user simulator, but the source of error lies in the action selection with respect to informability and matching.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, the sdp information has a clear positive impact on all the relation types (Table 1).",
        "evidence": "The claim That SDP has a clear positive impact on all the relation types Our results showed that incorporating the dependency trees with SDP can significantly improve the performance of most relations. Compared with the baseline model, the improvement of SDP depends on the type of relation. For the comparison, the best F1 score without SDP is significantly better than the one with SDP. Moreover, macro-averaged F1 scores indicate that SDP has a clear positive impact on most relations types, but the improvement of Macro-averaged is only statistically significant (p-value <0.01). This may indicate that SDP is useful for some relations, but that it is not generalized to all relations.",
        "table": "+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|    | [BOLD] Relation   |   [BOLD] best F1 (in 5-fold) without sdp |   [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.   |\n+====+===================+==========================================+=======================================+================+\n|  0 | USAGE             |                                    60.34 |                                 80.24 | + 19.90        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  1 | MODEL-FEATURE     |                                    48.89 |                                 70    | + 21.11        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  2 | PART_WHOLE        |                                    29.51 |                                 70.27 | +40.76         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  3 | TOPIC             |                                    45.8  |                                 91.26 | +45.46         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  4 | RESULT            |                                    54.35 |                                 81.58 | +27.23         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  5 | COMPARE           |                                    20    |                                 61.82 | + 41.82        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  6 | macro-averaged    |                                    50.1  |                                 76.1  | +26.00         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally stronger: [CONTINUE] as in Eq.",
        "evidence": "As can be seen, the results for the essay level are generally better than for the F1 level. This can be explained by the fact that the space-augmented models are generally better able to represent the dependency between paragraphs.",
        "table": "+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]                                         | C-F1 100%          | C-F1 50%           | R-F1 100%          | R-F1 50%           | F1 100%            | F1 50%             |\n+====+=================================================+====================+====================+====================+====================+====================+====================+\n|  0 | Y-3                                             | 49.59              | 65.37              | 26.28              | 37.00              | 34.35              | 47.25              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  1 | Y-3:Y<italic>C</italic>-1                       | 54.71              | 66.84              | 28.44              | 37.35              | 37.40              | 47.92              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  2 | Y-3:Y<italic>R</italic>-1                       | 51.32              | 66.49              | 26.92              | 37.18              | 35.31              | 47.69              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  3 | Y-3:Y<italic>C</italic>-3                       | <bold>54.58</bold> | 67.66              | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold> |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  4 | Y-3:Y<italic>R</italic>-3                       | 53.31              | 66.71              | 26.65              | 35.86              | 35.53              | 46.64              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  5 | Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95              | <bold>67.84</bold> | 27.90              | 39.71              | 36.54              | 50.09              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  6 | Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55              | 67.60              | 28.30              | 38.26              | 37.26              | 48.86              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "In contrast, our proposed classifier can almost precisely identify the one-word scope without any syntactic information.",
        "evidence": "The results obtained by our system show that the neural classifier almost perfectly identifies the one-word scopes without any syntactic information.. This is very different from the results obtained by the neural models that are trained on syntactic information. Whereas, our system relies on syntactic information to make predictions and can almost perfectly predict the one-word scopes without any syntactic information.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "As shown in Table 8, the S2S baseline outperforms the G2S approaches.",
        "evidence": "As shown, the S2S baseline is on par with the G2S approaches.. Gold is the state-of-the-art model.",
        "table": "+----+----------------------+----------------------+---------------------+\n|    | <bold>Model</bold>   | <bold>ADDED</bold>   | <bold>MISS</bold>   |\n+====+======================+======================+=====================+\n|  0 | S2S                  | 47.34                | 37.14               |\n+----+----------------------+----------------------+---------------------+\n|  1 | G2S-GIN              | 48.67                | 33.64               |\n+----+----------------------+----------------------+---------------------+\n|  2 | G2S-GAT              | 48.24                | 33.73               |\n+----+----------------------+----------------------+---------------------+\n|  3 | G2S-GGNN             | 48.66                | 34.06               |\n+----+----------------------+----------------------+---------------------+\n|  4 | GOLD                 | 50.77                | 28.35               |\n+----+----------------------+----------------------+---------------------+\n|  5 | [EMPTY]              | [EMPTY]              | [EMPTY]             |\n+----+----------------------+----------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "We suspect that there are not enough data to pretrain the models and that the thread classification task used to pretrain the HAN models may not be sophisticated enough to learn effective thread vectors.",
        "evidence": "This may indicate that there is not enough data to pretrain the HAN models. In particular, we observe that the HAN models may not be sophisticated enough to learn effective thread vectors. This may indicate that the thread classification task used to pretrain the HAN models may not be sophisticated enough to learn effective thread vectors.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "This strongly indicates that there is a superficial cue that affects model performance, but this cue is not captured by Word frequency",
        "evidence": "Word frequency We run experiments for all models on the training set and find that using word frequency does not significantly help the results. This indicates that there is a superficial cue that affects model performance.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "We also have competitive results to Guo et al.",
        "evidence": "As shown, we also provide the results to Guo et al. It shows that our G2S-GAT model outperforms Guo et al. In particular, the G2S-GGNN outperforms the state-of-the-art G2S-GIN model Guo et al. Moreover, we also compare our results with other recent competitive results. It shows that our domain-adaptive model G2S-GAT is superior to all the baselines.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DocSub achieved better values of precision, but lower values of recall.",
        "evidence": "We see that for each language pair, DocSub was able to achieve better values of precision, but lower values of recall.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Despite performing slightly worse than sparsemax under automatic metrics, TVMAX outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2.",
        "evidence": "Despite performing slightly worse than sparsemax under automatic metrics, TVMAX outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2. [CONTINUE] The results show that TVMAX is able to achieve better performance than sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2..",
        "table": "+----+-----------+-------------+-----------------------+\n|    | [EMPTY]   | caption     | attention relevance   |\n+====+===========+=============+=======================+\n|  0 | softmax   | 3.50        | 3.38                  |\n+----+-----------+-------------+-----------------------+\n|  1 | sparsemax | 3.71        | 3.89                  |\n+----+-----------+-------------+-----------------------+\n|  2 | TVmax     | [BOLD] 3.87 | [BOLD] 4.10           |\n+----+-----------+-------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "We have 116,674 tweets, with an average length of 22.3 tokens.",
        "evidence": "First, we extracted all negation cues using fastText. We then removed all false negation cues and only kept the tweets with at least three negation symbols. Then, we calculated the average token length of each tweet. The average tweet length is 22.3 tokens, calculated by averaging the number of tokens in each tweet. We compared the number of false negation cues and true negation cues in the table with the average token length of tweets. Overall, we have 116,674 tweets, with an average length of 22.3 tokens.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU).",
        "evidence": "Here we compare our model with GNMT and GRU, the current state-of-the-art models on the GLUE benchmark. For a fair comparison, we set the batch size of our model to 1.1. It can be seen that the translation quality of LRN is slightly better than that of GRU by 0.02 BLEU.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.",
        "evidence": "This shows that the coverage mechanism improves the model performance. To further illustrate the effect of the coverage mechanism, we remove the encoder modules and the decoder modules. Specifically, we can see that the performance drops by 1.7/2.4 points for B and C. This shows the effectiveness of the coverage mechanism.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "However, models trained using linguistic features on the training data obtain significantly higher predictive accuracy.",
        "evidence": "NNs and RNNs trained on text only result in poor predictive accuracy. This is expected as the text is not natural language. However, models trained on syntactic or semantic features obtained significantly higher predictive accuracy than neural networks. MLP and LSTM models obtained similar predictive accuracy to sentiment models. This suggests that syntactic information may be effective when used to improve the classification accuracy of unsupervised models. We leave further exploration of this phenomenon to future work.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] As the results of applying the co-occurrence baseline (ρ = 0) shows (Table 2), the semantic relations in this data are strongly concentrated within a sentence boundary, especially for the relation of RecurLink, with an F1 of 1.0.",
        "evidence": "As can be seen from the table, the semantic relations in the PubMed-based annotations are strongly concentrated within a sentence boundary. Especially for the relation Recurlink, the co-occurrence baseline achieves an F1-score of 1.0, which is significantly higher than the other approaches. The machine learning approaches based on BoC lexical features effectively complement the deficiency of cross-sentence relation extraction.",
        "table": "+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|    | Relation type                 |   Count | Intra-sentential co-occ.  [ITALIC] ρ=0   | Intra-sentential co-occ.  [ITALIC] ρ=5   |   Intra-sentential co-occ.  [ITALIC] ρ=10 | BoC(Wiki-PubMed-PMC) LR   | BoC(Wiki-PubMed-PMC) SVM   | BoC(Wiki-PubMed-PMC) ANN   |\n+====+===============================+=========+==========================================+==========================================+===========================================+===========================+============================+============================+\n|  0 | TherapyTiming(TP,TD)          |     428 | [BOLD] 0.84                              | 0.59                                     |                                      0.47 | 0.78                      | 0.81                       | 0.78                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  1 | NextReview(Followup,TP)       |     164 | [BOLD] 0.90                              | 0.83                                     |                                      0.63 | 0.86                      | 0.88                       | 0.84                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  2 | Toxicity(TP,CF/TR)            |     163 | [BOLD] 0.91                              | 0.77                                     |                                      0.55 | 0.85                      | 0.86                       | 0.86                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  3 | TestTiming(TN,TD/TP)          |     184 | 0.90                                     | 0.81                                     |                                      0.42 | 0.96                      | [BOLD] 0.97                | 0.95                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  4 | TestFinding(TN,TR)            |     136 | 0.76                                     | 0.60                                     |                                      0.44 | [BOLD] 0.82               | 0.79                       | 0.78                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  5 | Threat(O,CF/TR)               |      32 | 0.85                                     | 0.69                                     |                                      0.54 | [BOLD] 0.95               | [BOLD] 0.95                | 0.92                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  6 | Intervention(TP,YR)           |       5 | [BOLD] 0.88                              | 0.65                                     |                                      0.47 | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  7 | EffectOf(Com,CF)              |       3 | [BOLD] 0.92                              | 0.62                                     |                                      0.23 | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  8 | Severity(CF,CS)               |      75 | [BOLD] 0.61                              | 0.53                                     |                                      0.47 | 0.52                      | 0.55                       | 0.51                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n|  9 | RecurLink(YR,YR/CF)           |       7 | [BOLD] 1.0                               | [BOLD] 1.0                               |                                      0.64 | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 10 | RecurInfer(NR/YR,TR)          |      51 | 0.97                                     | 0.69                                     |                                      0.43 | [BOLD] 0.99               | [BOLD] 0.99                | 0.98                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 11 | GetOpinion(Referral,CF/other) |       4 | [BOLD] 0.75                              | [BOLD] 0.75                              |                                      0.5  | -                         | -                          | -                          |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 12 | Context(Dis,DisCont)          |      40 | [BOLD] 0.70                              | 0.63                                     |                                      0.53 | 0.60                      | 0.41                       | 0.57                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 13 | TestToAssess(TN,CF/TR)        |      36 | 0.76                                     | 0.66                                     |                                      0.36 | [BOLD] 0.92               | [BOLD] 0.92                | 0.91                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 14 | TimeStamp(TD,TP)              |     221 | [BOLD] 0.88                              | 0.83                                     |                                      0.5  | 0.86                      | 0.85                       | 0.83                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 15 | TimeLink(TP,TP)               |      20 | [BOLD] 0.92                              | 0.85                                     |                                      0.45 | 0.91                      | [BOLD] 0.92                | 0.90                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+\n| 16 | Overall                       |    1569 | 0.90                                     | 0.73                                     |                                      0.45 | 0.92                      | [BOLD] 0.93                | 0.91                       |\n+----+-------------------------------+---------+------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------+----------------------------+----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input.",
        "evidence": "Ch4S3SS4SSSx3 Ch4S4SS1SSSx2 -Ch5S3SS1SSSx4 Ch5S3SS1SSSx7 -Ch5S3SS2SSSx3 Ch5S3SS2SSSx6 Ch5S3SS3SSSx4 Ch5S3SS3SSSx5 Ch5S4SS2SSSx2 -Ch5S4SS3SSSx3 -Ch5T13 Our models are significantly better than all the baselines when not using PSG. When removing the passage embedding, the BLEU-2 score of our comparison models dropped by 8.3% and METEOR by 2.5%. Only removing system retrieval results in a slight decrease, and we believe this is because the ground-truth passage is not important. In all three retrieval tasks, our model is better than the baselines. Comparing our results with human judgements, we still have a huge gap to human judgements. Among our models, the one with system retrieval has better results even without using PSG. It demonstrates the effectiveness of our PRNSFM.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "between all three systems, GloVe ranks last, followed by the original implementation of this model, and finally the optimized implementation.",
        "evidence": "To the best of our knowledge, GloVe is the top performing system in the seen test sets. However, when it comes to unseen questions, word2vec is always better than GloVe. Between all three systems, GloVe is always on par. In the following section, we analyze the proposed system further.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "For Yelp, M0 has better Acc and PP than M1 at comparable semantic similarity.",
        "evidence": "We see that M6 has the best performance at semantic similarity across all the measures. As for accuracy, M6+para+lang has the highest accuracy and distinct-2 score at 21.6%, well above the second best M1 system (M0: shen-1). Similar results can be seen for the word similarity measures as well.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.818 | 0.719        | 37.3        | 10.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.819 | 0.734        | 26.3        | 14.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.813 | 0.770        | 36.4        | 18.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.807 | 0.796        | 28.4        | 21.5        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.798 | 0.783        | 39.7        | 19.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785        | 27.1        | 20.3        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.805 | [BOLD] 0.817 | 43.3        | 21.6        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.818 | 0.805        | [BOLD] 29.0 | [BOLD] 22.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "on the other hand, neither the distance nor syntactic feature plays an important role in entity coreference performance, which indicates that the relation types of entities provide valuable information for cross-document entity coreference resolution.",
        "evidence": "The results show that the relation type of entities is highly relevant for entity coreference resolution, which indicates that the relation type of entities can provide valuable information for cross-document entity coreference resolution. On the other hand, the distance nor syntactic feature plays an important role in entity coreference resolution.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] In general terms, the results displayed in table 1 show that the rejection method can reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain.",
        "evidence": "We compared the results of the rejection method to the results of applying a pre-trained black-box classification system to a new domain. It can be seen that the rejection method reduced the error of the output predictions when applying a pre-trained black-box classification system to a new domain. In general, the results displayed in the table show that the rejection method reduced the error of the output predictions when applying a pre-trained black-box classification system to a new domain.",
        "table": "+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|    | [EMPTY]                              | [BOLD] BB source acc.   | [BOLD] BB target acc.   | [BOLD] Non-reject. acc. (10/20/30%)   | [BOLD] Class. quality (10/20/30%)   | [BOLD] Reject. quality (10/20/30%)   |\n+====+======================================+=========================+=========================+=======================================+=====================================+======================================+\n|  0 | [BOLD] Apply Yelp BB to SST-2        | 89.18±0.08%             | 77.13±0.52%             | 82.43±0.22% 88.19±0.50% 93.60±0.16%   | 80.40±0.39% 83.11±0.80% 83.05±0.23% | 6.03±0.45 6.04±0.51 4.97±0.07        |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|  1 | [BOLD] Apply SST-2 BB to Yelp        | 83.306±0.18%            | 82.106±0.88%            | 87,98±0.18% 92.13±0.38% 94.19±0.33%   | 85.49±0.88% 84.53±0.38% 78.99±0.46% | 8.30±1.63 5.72±0.27 3.73±0.10        |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|  2 | [BOLD] Apply Electronics BB to Music | 86.39±0.22%             | 90.38±0.13%             | 95.04±0.43% 96.45±0.35% 97.26±0.31%   | 90.67±0.88% 83.93±0.67% 75.77±0.54% | 10.7±1.65 4.82±0.35 3.25±0.14        |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+\n|  3 | [BOLD] Apply Music BB to Electronics | 93.10±0.02%             | 79.85±0.0%              | 83.26±0.41% 87.06±0.55% 90.50±0.29%   | 79.97±0.74% 79.93±0.87% 76.81±0.41% | 4.1±0.55 3.80±0.35 3.32±0.09         |\n+----+--------------------------------------+-------------------------+-------------------------+---------------------------------------+-------------------------------------+--------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3.",
        "evidence": "We see that in the first layer, the accuracy of POS and semantic tagging increases linearly in the objective. However, the tag accuracy of BLEU drops linearly in the same layer. This means our drop-off in prediction is not statistically significant. We leave further exploration of this phenomenon to future work.",
        "table": "+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|    | [ITALIC] k           | Ar                   | Es                   | Fr                   | Ru                   | Zh                   | En                   |\n+====+======================+======================+======================+======================+======================+======================+======================+\n|  0 | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  1 | 0                    | 88.0                 | 87.9                 | 87.9                 | 87.8                 | 87.7                 | 87.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  2 | 1                    | 92.4                 | 91.9                 | 92.1                 | 92.1                 | 91.5                 | 89.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  3 | 2                    | 91.9                 | 91.8                 | 91.8                 | 91.8                 | 91.3                 | 88.3                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  4 | 3                    | 92.0                 | 92.3                 | 92.1                 | 91.6                 | 91.2                 | 87.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  5 | 4                    | 92.1                 | 92.4                 | 92.5                 | 92.0                 | 90.5                 | 86.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  6 | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  7 | 0                    | 81.9                 | 81.9                 | 81.8                 | 81.8                 | 81.8                 | 81.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  8 | 1                    | 87.9                 | 87.7                 | 87.8                 | 87.9                 | 87.7                 | 84.5                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  9 | 2                    | 87.4                 | 87.5                 | 87.4                 | 87.3                 | 87.2                 | 83.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 10 | 3                    | 87.8                 | 87.9                 | 87.9                 | 87.3                 | 87.3                 | 82.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 11 | 4                    | 88.3                 | 88.6                 | 88.4                 | 88.1                 | 87.7                 | 82.1                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 12 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 13 | [EMPTY]              | 32.7                 | 49.1                 | 38.5                 | 34.2                 | 32.1                 | 96.6                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "the distribution of dialogue sessions can be seen in Fig.",
        "evidence": "We only show the results of the most frequent classes, i.e., Full and Other. We can see that the proportion of Full dialogues is significantly higher than that of the other classes.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "The error reduction over the best baseline is 15.08% on average.",
        "evidence": "Our model (OURS) shows the best performance on all the three tasks. The error reduction over the best baseline is 15.08% on average.",
        "table": "+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source                               | Target            |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+======================================+===================+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer look + Beer aroma + Beer palate | Hotel location    | 78.65 |     79.09 |     79.28 |    80.42 |        82.1  | [BOLD] 84.52 |     85.43 |\n+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look + Beer aroma + Beer palate | Hotel cleanliness | 86.44 |     86.68 |     89.01 |    86.95 |        87.15 | [BOLD] 90.66 |     92.09 |\n+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look + Beer aroma + Beer palate | Hotel service     | 85.34 |     86.61 |     87.91 |    87.37 |        86.4  | [BOLD] 89.93 |     92.42 |\n+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "supports"
    },
    {
        "claim": "The system does not perform well on synthetic dataset with a minimum of 80% P@1 and 98% P@10.",
        "evidence": "The system performs well on P@k, but is not good at P@k and P@10. It needs more training data for fair comparisons.",
        "table": "+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|    | [BOLD] Language   | [BOLD] # Test   | [BOLD] P@1   | [BOLD] P@3   | [BOLD] P@5   | [BOLD] P@10   | [BOLD] MRR   |\n+====+===================+=================+==============+==============+==============+===============+==============+\n|  0 | [BOLD] Language   | [BOLD] Samples  | [BOLD] P@1   | [BOLD] P@3   | [BOLD] P@5   | [BOLD] P@10   | [BOLD] MRR   |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  1 | Bengali           | 140000          | 91.30        | 97.83        | 98.94        | 99.65         | 94.68        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  2 | Czech             | 94205           | 95.84        | 98.72        | 99.26        | 99.62         | 97.37        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  3 | Danish            | 140000          | 85.84        | 95.19        | 97.28        | 98.83         | 90.85        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  4 | Dutch             | 140000          | 86.83        | 95.01        | 97.04        | 98.68         | 91.32        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  5 | English           | 140000          | 97.08        | 99.39        | 99.67        | 99.86         | 98.27        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  6 | Finnish           | 140000          | 97.77        | 99.58        | 99.79        | 99.90         | 98.69        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  7 | French            | 140000          | 86.52        | 95.66        | 97.52        | 98.83         | 91.38        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  8 | German            | 140000          | 87.58        | 96.16        | 97.86        | 99.05         | 92.10        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n|  9 | Greek             | 30022           | 84.95        | 94.99        | 96.88        | 98.44         | 90.27        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 10 | Hebrew            | 132596          | 94.00        | 98.26        | 99.05        | 99.62         | 96.24        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 11 | Hindi             | 140000          | 82.19        | 93.71        | 96.28        | 98.30         | 88.40        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 12 | Indonesian        | 140000          | 95.01        | 98.98        | 99.50        | 99.84         | 97.04        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 13 | Italian           | 140000          | 89.93        | 97.31        | 98.54        | 99.38         | 93.76        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 14 | Marathi           | 140000          | 93.01        | 98.16        | 99.06        | 99.66         | 95.69        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 15 | Polish            | 140000          | 95.65        | 99.17        | 99.62        | 99.86         | 97.44        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 16 | Portuguese        | 140000          | 86.73        | 96.29        | 97.94        | 99.10         | 91.74        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 17 | Romanian          | 140000          | 95.52        | 98.79        | 99.32        | 99.68         | 97.22        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 18 | Russian           | 140000          | 94.85        | 98.74        | 99.33        | 99.71         | 96.86        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 19 | Spanish           | 140000          | 85.91        | 95.35        | 97.18        | 98.57         | 90.92        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 20 | Swedish           | 140000          | 88.86        | 96.40        | 98.00        | 99.14         | 92.87        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 21 | Tamil             | 140000          | 98.05        | 99.70        | 99.88        | 99.98         | 98.88        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 22 | Telugu            | 140000          | 97.11        | 99.68        | 99.92        | 99.99         | 98.38        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 23 | Thai              | 12403           | 98.73        | 99.71        | 99.78        | 99.85         | 99.22        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+\n| 24 | Turkish           | 140000          | 97.13        | 99.51        | 99.78        | 99.92         | 98.33        |\n+----+-------------------+-----------------+--------------+--------------+--------------+---------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "These experiments show that the number of factors giving the best performance does not vary depending on the underlying data distribution.",
        "evidence": "These experiments show that the performance of our classifier does not vary significantly depending on the underlying data distribution. In fact, the F1 score of our classifier on the NYT10 dataset is slightly higher than the F1 score on the NYT11 dataset. We define the number of factors giving the best performance as M. We observe that the performance of our classifier increases with the number of factors in the data distribution.",
        "table": "+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|    |   [ITALIC] m |   NYT10 Prec. |   NYT10 Rec. | NYT10 F1     |   NYT11 Prec. |   NYT11 Rec. | NYT11 F1     |\n+====+==============+===============+==============+==============+===============+==============+==============+\n|  0 |            1 |         0.541 |        0.595 | [BOLD] 0.566 |         0.495 |        0.621 | 0.551        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  1 |            2 |         0.521 |        0.597 | 0.556        |         0.482 |        0.656 | 0.555        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  2 |            3 |         0.49  |        0.617 | 0.547        |         0.509 |        0.633 | 0.564        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  3 |            4 |         0.449 |        0.623 | 0.522        |         0.507 |        0.652 | [BOLD] 0.571 |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  4 |            5 |         0.467 |        0.609 | 0.529        |         0.488 |        0.677 | 0.567        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which is lower than the previous state-of-the-art.",
        "evidence": "Table. Our model achieves a joint goal accuracy of 48.79%, which is lower than the previous state-of-the-art..",
        "table": "+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|    | [BOLD] DST Models                      | [BOLD] Joint Acc. WoZ 2.0   | [BOLD] Joint Acc. MultiWoZ   | [BOLD] ITC               |\n+====+========================================+=============================+==============================+==========================+\n|  0 | Baselines Mrksic et al. ( 2017 )       | 70.8%                       | 25.83%                       | [ITALIC] O( [ITALIC] mn) |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  1 | NBT-CNN Mrksic et al. ( 2017 )         | 84.2%                       | -                            | [ITALIC] O( [ITALIC] mn) |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  2 | StateNet_PSI Ren et al. ( 2018 )       | [BOLD] 88.9%                | -                            | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  3 | GLAD Nouri and Hosseini-Asl ( 2018 )   | 88.5%                       | 35.58%                       | [ITALIC] O( [ITALIC] mn) |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  4 | HyST (ensemble) Goel et al. ( 2019 )   | -                           | 44.22%                       | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  5 | DSTRead (ensemble) Gao et al. ( 2019 ) | -                           | 42.12%                       | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  6 | TRADE Wu et al. ( 2019 )               | -                           | 48.62%                       | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  7 | COMER                                  | 88.6%                       | [BOLD] 48.79%                | [ITALIC] O(1)            |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+",
        "label": "refutes"
    },
    {
        "claim": "BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU gain over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU gain over the approach described in Freitag and Al-Onaizan (2016).",
        "evidence": "This claim makes use of the fact that the EWC-adapted models provide better translation quality than the strong uniform EWC ensemble, and BI+IS is able to give a 2.4 / 10.2 overall BLEU gain over the approach described in Freitag and Al-Onaizan (2016).",
        "table": "+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|    | [BOLD] Language pair   | [BOLD] Model type   |   [BOLD] Oracle model |   [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS   |\n+====+========================+=====================+=======================+================================================+================================================+\n|  0 | es-en                  | Unadapted           |                  36.4 |                                           34.7 | 36.6                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  1 | es-en                  | No-reg              |                  36.6 |                                           34.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  2 | es-en                  | EWC                 |                  37   |                                           36.3 | [BOLD] 37.2                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  3 | en-de                  | Unadapted           |                  36.4 |                                           26.8 | 38.8                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  4 | en-de                  | No-reg              |                  41.7 |                                           31.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  5 | en-de                  | EWC                 |                  42.1 |                                           38.6 | [BOLD] 42.0                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation.",
        "evidence": "We now proceed to analyze the impact of the language modeling loss in our model. Generally, we find that adding the language modeling loss improves accuracy, sometimes at the cost of semantic preservation. For example, M3 has worse accuracy than M2, but substantially better accuracy than M4 and M5 if we add the loss from M4 to M6. Similarly, we find that the loss in M6 tends to be greater than that of M3 or M7, sometimes at the cost of semantic preservation.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.818 | 0.719        | 37.3        | 10.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.819 | 0.734        | 26.3        | 14.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.813 | 0.770        | 36.4        | 18.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.807 | 0.796        | 28.4        | 21.5        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.798 | 0.783        | 39.7        | 19.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785        | 27.1        | 20.3        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.805 | [BOLD] 0.817 | 43.3        | 21.6        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.818 | 0.805        | [BOLD] 29.0 | [BOLD] 22.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "When the model focuses on “nele” and “type”, it learns the semantic meaning of them, thus enabling the prediction of triples that have “nele” and “type”",
        "evidence": "The model focuses on “nele” and “type”. It means that the model can make better prediction of triples that have “nele” and “type”. We observe that the positive effect of our model design is that: (1) For target prediction, PCNN+ATT (1) can achieve better results than Rank+ExAtt (0.584) and PCNN+ATT (m) (0.317) by considering only the target word and ignoring the semantic meaning; (2) Our model achieves the best result if we focus on the “type” and “nele”. It means that the model can make better prediction of triples that have “nele” and “type”.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods, except for the Portuguese Europarl corpus, where DocSub had the highest value.",
        "evidence": "This confirms that, in general, this ranking better than the traditional one. Note, however, that the Portuguese corpus has the highest values of f-measure.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model.",
        "evidence": "We again show the results from our models compared to the results from the original KCP. We can see that pre-clustering of documents to topics is highly effective, leading to substantial improvements over the KCP by 4.6 points in CoNLL F1 score. Our joint model also outperforms all the base [CONTINUE] The results show that pre-clustering of documents to topics is highly effective, leading to improvements of 4.6 points over the KCP baseline. These results are very similar to the ones reported by \\citeauthorkenyon-dean et al. \\shortcitekcp.",
        "table": "+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>                                      | R       | MUC P   | <italic>F</italic>1   | B3 P    | CEAF-<italic>e</italic> P   | CoNLL <italic>F</italic>1   |\n+====+=========================================================+=========+=========+=======================+=========+=============================+=============================+\n|  0 | <bold>Baselines</bold>                                  | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  1 | Cluster+Lemma                                           | 75.5    | 79.9    | 73.6                  | 85      | 71.7                        | 76.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  2 | CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | -       | 75      | 64                    | 78      | -                           | 73                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  3 | KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 71      | 71      | 69                    | 67      | 67                          | 69                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  4 | Cluster+KCP                                             | 77.4    | 79.3    | 71.5                  | 87.2    | 66.4                        | 73.6                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  5 | <bold>Model Variants</bold>                             | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  6 | Disjoint                                                | 80.3    | 83.6    | 75.9                  | 86      | 71.9                        | 78.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  7 | Joint                                                   | 81      | 84.5    | 77.3                  | 85.1    | 73.8                        | <bold>79.5</bold>           |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Also, the average human rating for Refresh is not significantly higher (p (cid:28) 0.01) than ExtAbsRL.",
        "evidence": "The human rating for Refresh is significantly higher than for ExtAbsRL. Interestingly, the average human rating for Refresh is lower than for ExtAbsRL. We believe this is because the average human rating for Refresh is rather low (p (cid:28) 0.01), because the context-switch has few interactions with users.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "refutes"
    },
    {
        "claim": "This is because word representation learning approaches, like OIWE-IPG and SOV, do not consider the semantic distance learning issue, which has a significant impact on word similarity task.",
        "evidence": "The reason is that sparse word representation methods (i.e., GloVe and word2vec) perform worse than the other algorithms which are related to semantic distance learning. This is because the sparse word representation methods do not consider the semantic distance learning issue, which has a significant impact on word similarity task. Word2Sense outperforms our algorithms on 11 out of 12 datasets.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity.",
        "evidence": "We see that M6 has the best performance at similarity, closely followed by M0. As for accuracy, M6+para+lang has the highest accuracy and distinct-2 score. It is worth noting that M1, M2, M3, and M4 are similar to each other at the semantic similarity level. This is because they only differ in the posterior probability distribution. M6+para+lang gets the best result in content preservation, while M4, M5 and M6 do not.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.818 | 0.719        | 37.3        | 10.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.819 | 0.734        | 26.3        | 14.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.813 | 0.770        | 36.4        | 18.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.807 | 0.796        | 28.4        | 21.5        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.798 | 0.783        | 39.7        | 19.2        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785        | 27.1        | 20.3        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.805 | [BOLD] 0.817 | 43.3        | 21.6        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.818 | 0.805        | [BOLD] 29.0 | [BOLD] 22.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, our data augmentation technique (NO-TRANSLATIONS) had a significant impact on the final score, reducing it by 0.84 points.",
        "evidence": "The results are consistent with the intuition: our data augmentation technique (no-translations) had a significant impact on the final score, reducing it by 0.84 points..",
        "table": "+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|    | ID LSTM-800        |   5-fold CV 70.56 | Δ 0.66   | Single model 67.54   | Δ 0.78   | Ensemble 67.65   | Δ 0.30   |\n+====+====================+===================+==========+======================+==========+==================+==========+\n|  0 | LSTM-400           |             70.5  | 0.60     | [BOLD] 67.59         | 0.83     | [BOLD] 68.00     | 0.65     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  1 | IN-TITLE           |             70.11 | 0.21     | [EMPTY]              | [EMPTY]  | 67.52            | 0.17     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  2 | [BOLD] SUBMISSION  |             69.9  | –        | 66.76                | –        | 67.35            | –        |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  3 | NO-HIGHWAY         |             69.72 | −0.18    | 66.42                | −0.34    | 66.64            | −0.71    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  4 | NO-OVERLAPS        |             69.46 | −0.44    | 65.07                | −1.69    | 66.47            | −0.88    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  5 | LSTM-400-DROPOUT   |             69.45 | −0.45    | 65.53                | −1.23    | 67.28            | −0.07    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  6 | NO-TRANSLATIONS    |             69.42 | −0.48    | 65.92                | −0.84    | 67.23            | −0.12    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  7 | NO-ELMO-FINETUNING |             67.71 | −2.19    | 65.16                | −1.60    | 65.42            | −1.93    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "both (Nguyen et al., 2016) and ours pre-compute a vocabulary of top-K possible responses, which are used as the only acceptable responses.",
        "evidence": "Comparing DAMD with HDSA, we can see that our responses are relatively more diverse. For example, we can see that the diversity of our responses is comparable to HDSA’s responses. Our model also yields more acceptable responses in all cases, while HDSA’s responses are more diverse only in the good response category. This can be explained by the fact that the content and structure of the dialogue between two speakers are highly similar. HDSA leverages the whole context to provide an informative response, while our model relies on the word embedding matrix to learn a global response. This shows the advantage of using our pre-compiled vocabulary of more diverse responses.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "The full model gives 25.5 BLEU points on the AMR15 dev set.",
        "evidence": "These alignments were post-processed to show some of the properties of the model. First, we showed that the baseline model, DCGCN4, gives 25.5 BLEU points, which is fairly high given the relatively small amount of annotated data. We then experimented with some of the model’s effects. First, we removed the dense blocks from the baseline model, and observed a significant drop in performance. This suggests that the baseline model suffers from the data sparsity problem. Next, we replaced the base model with the DCGCN4 model, and observed a slight improvement in performance. Finally, we removed the dense blocks, and observed that the model with only 2, 3, and 4 dense blocks now gives 23.2 BLEU points.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "supports"
    },
    {
        "claim": "The semantic threshold for OD-d2v is set at 0.3 while for OD-w2v is set at 0.6.",
        "evidence": "This shows that the semantic threshold for OD-D2V is lower than for OD-w2v. This is expected as the difference in average number of words is lower for OD-d2v than for OD-w2v. [CONTINUE] We also observe that Doc2vec and TF-IDF perform better than expected in terms of ARI, whereas BERT outperforms them in terms of Sil.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "This suggests that graph encoders based on gating mechanisms are not as effective as other models in text generation models.",
        "evidence": "This result shows that graph encoders based on gating mechanisms are not as effective as other models in text generation models. It was expected since the G2S models did not outperform the state-of-the-art models, and the best results of the G2S models are gathered into the table. As shown, the graph encoders built by Guo et al. (2017) and Sang et al. (2018) are better than the Song et al. (2018) model, but the best result of the G2S models is obtained by Guo et al. Moreover, we compared our proposed models G2S-GAT with the state-of-the-art text generation models, including Back et al. (2018) and Cao et al. (2019). The results show that the graph encoders based on gating mechanisms are not as effective as other models in text generation models.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "However, when gold PP attachment are used, we note a large potential improve [CONTINUE] ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach.",
        "evidence": "We use the oracle parser as the parser for this work. The use of gold PP annotations as features is shown to be effective when used in combination with our RPCG approach. In particular, when the baseline model is simply applied, the gain in performance is relatively small. This is because the baseline model has very weak parsing capabilities and does not benefit much from the use of gold PP annotations. By adding gold PP annotations, we note a large potential improve in the parsing accuracies. For example, the accuracy difference between RBG and oracle parser increases from 88.51% to 89.59% when gold PP annotations are used. This improvement is statistically significant (p<0.05, t-test), and it confirms that adding PP annotations as features is an effective approach..",
        "table": "+----+-------------------+-------------------+-------------------+\n|    | [BOLD] System     |   [BOLD] Full UAS |   [BOLD] PPA Acc. |\n+====+===================+===================+===================+\n|  0 | RBG               |             94.17 |             88.51 |\n+----+-------------------+-------------------+-------------------+\n|  1 | RBG + HPCD (full) |             94.19 |             89.59 |\n+----+-------------------+-------------------+-------------------+\n|  2 | RBG + LSTM-PP     |             94.14 |             86.35 |\n+----+-------------------+-------------------+-------------------+\n|  3 | RBG + OntoLSTM-PP |             94.3  |             90.11 |\n+----+-------------------+-------------------+-------------------+\n|  4 | RBG + Oracle PP   |             94.6  |             98.97 |\n+----+-------------------+-------------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "The human evaluation shows that our mirrored instances are not as difficult as the original ones (see Table 3).",
        "evidence": "The human evaluation shows that our mirrored instances are not as difficult as the original ones.",
        "table": "+----+---------------+------------+-----------------------------+\n|    | Dataset       |   Accuracy |   Fleiss’ kappa  [ITALIC] k |\n+====+===============+============+=============================+\n|  0 | Original COPA |        100 |                       0.973 |\n+----+---------------+------------+-----------------------------+\n|  1 | Balanced COPA |         97 |                       0.798 |\n+----+---------------+------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "However, the drop in performance on the QA-SRL task, from which the model's weights are initialized, is much smaller with BIDAF (ELMO) than MQAN, and this corroborates the idea that contextualized ELMo representations, which benefit from general pre-training and are transferred to the task using the fine-tuning paradigm, are more amenable for achieving cross-dataset generalization.",
        "evidence": "We hypothesize that contextualized ELMo representations, which benefit from general pre-training and are transferred to the task using the fine-tuning paradigm, are more amenable for achieving cross-dataset generalization than MQAN. We tested this on the QA-SRL task, which requires knowledge of the order by which the questions are formulated. We trained BiDAF (ELMo) on in-domain SQuAD and QA-SRL data, and then fine-tuned it on the SQuAD training data. The results show that the relative drop in performance on the QA-SRL task is much smaller with BiDAF (ELMo) than MQAN, and this corroborates the idea that contextualized ELMo representations, which benefit from general pre-training and are transferred to the task using the fine-tuning paradigm, are more amenable for achieving cross-dataset generalization.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The PRKGC model gives considerably good results, which indicates the non-triviality of RC-QEDE.",
        "evidence": "As shown, the PRKGC model performs much better than the baselines in terms of all evaluation metrics. Especially, we can see that the PRKGC model improves the precision in all evaluation metrics. This indicates that the PRKGC model generates questions that are of high quality. Moreover, the PRKGC+NS model performs even better than the PRKGC model, which shows that the PRKGC model can handle RC-QEDE well.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "We hypothesize that the gating mechanism cannot better capture long-distance dependencies between nodes far apart in the graph.",
        "evidence": "From the table, we see that our G2S-GAT and S2S models cannot better capture long-distance dependencies than the current state-of-the-art baselines. For example, in the most recent test set of \\newciteLDC2015E86, the G2S-GAT model achieves a BLEU score of 23.42 ± 0.16 comparing to the state-of-the-art G2S model Guo et al. Moreover, in the most recent test set of \\newciteLDC2017T10, the G2S-GGNN model achieves a BLEU score of 27.87, better than the state-of-the-art G2S-GAT model that achieves a BLEU score of 26.72 ± 0.20. This result suggests that the current gating mechanism cannot capture long-distance dependencies between nodes far apart in the graph. The G2S-GGNN model achieves the best performance in all the three settings.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under BLEU (around 3 points) is greater than that under Meteor (around 5 points).",
        "evidence": "From the table, we can see that under BLEU, the gap between Seq2seq and Dual2seq is greater than that under Meteor (around 5 points) almost by a factor of 1. Dual2seq is better than all the other systems under all three metrics, which proves the effectiveness of the proposed method. When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq is smaller than that under Meteor (around 5 points) by a factor of 2.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Without knowledge of the input systems, the score of MUC-B1, which most closely follows the MUC scoring methodology (Vilain et al., 1995), was higher than MUC-B1.",
        "evidence": "We believe that this is due to the fact that CoNLL does not consider the word order or the position of the words. These two aspects are important for the prediction of the next word in a sentence. The superiority of the lemma matrix approach over the disjoint approach can be explained by the fact that it does not model the word order or the position dependencies. Instead, it only uses the information from the lemma label. The joint approach improves all three measures.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "At the same time, the distributional information embedded into the network appears to have acted as a stabilizing force.",
        "evidence": "The distributional information appears to have acted as a stabilizing force in the task of participant selection. Specifically, the mean and standard deviation values of the participants increased from 80/88/82/78/97 to 85/6.9/24.4% which is consistent with the results of the automatic evaluation. At the same time, the distributional information seems to have acted as a stabilizing force in the network.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We suspect that two reasons for the performance drop on booking hotels are 1) the vocabularies of booking hotels are more similar to that of others than of booking flights or restaurants, making it easier to say keywords unintentionally and 2) users tend to vary more in asking details on booking a hotel,",
        "evidence": "The three methods all outperform ACER and ALDM without any usage of any in-domain resources. We suspect that two reasons for the performance drop can be explained. First, the vocabulary of booking hotels is more similar to that of booking flights and restaurants. This makes it easier for the models to miss out on important words. Second, users tend to vary more in asking details on booking a hotel. We conjecture that this is due to the fact that most hotels do not have standard language spans. In most cases, the entities and attributes we mentioned are all out of the user memory, making it easier for the models to find the words.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "The gap has become larger when the threshold becomes smaller, since there is much more noises when the score becomes smaller, our capsule net and word-level attention models are more robust to these noises.",
        "evidence": "This indicates that our capsule-level attention model is more robust to its surroundings while the word-level attention model is more robust to the noise. From the results, we can see that the gap has become larger when the threshold becomes smaller. Since there is much more noise when the score becomes smaller, our capsule net and word-level attention models are more robust to these noises.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "The CS-ONLY-DISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions.",
        "evidence": "The claim that the cs-only-discriminative model is able to prioritize the gold sentence better than all other models is true. Under both conditions, the fine-tuned cs-only-discriminative model outperforms all other models.",
        "table": "+----+-----------------+--------------+--------------+--------------+--------------+\n|    | [EMPTY]         | dev CS       | dev mono     | test CS      | test mono    |\n+====+=================+==============+==============+==============+==============+\n|  0 | CS-only-LM      | 45.20        | 65.87        | 43.20        | 62.80        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  1 | Fine-Tuned-LM   | 49.60        | 72.67        | 47.60        | 71.33        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  2 | CS-only-disc    | [BOLD] 75.60 | 70.40        | 70.80        | 70.53        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  3 | Fine-Tuned-disc | 70.80        | [BOLD] 74.40 | [BOLD] 75.33 | [BOLD] 75.87 |\n+----+-----------------+--------------+--------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "For example, we take the triple (nele, type, nele).",
        "evidence": "The PCNN+ATT model performs worse than the rank+exatt model. We attribute this to the fact that the words generated by our model are well suited for hierarchical attention, since they are high-level features which can capture more detailed semantics at the pair level.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "Increasing the window size to 10 increases the F1 score marginally (A3−A4).",
        "evidence": "Yes, increasing the window size to 10 increases the F1 score marginally (A3−A4).",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "In contrast, the noise-aware model requires more iterations to converge.",
        "evidence": "The results show that the alignment-aware model requires fewer iterations to converge compared to the soft alignment model. This is expected as the model is able to learn a more robust feature space where the alignment is not only uniform but also more adjustable.",
        "table": "+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|    | Method                | En→It best   | En→It avg    |   En→It iters | En→De best   | En→De avg    |   En→De iters | En→Fi best   | En→Fi avg    |   En→Fi iters | En→Es best   | En→Es avg    |   En→Es iters |\n+====+=======================+==============+==============+===============+==============+==============+===============+==============+==============+===============+==============+==============+===============+\n|  0 | Artetxe et al., 2018b | [BOLD] 48.53 | 48.13        |           573 | 48.47        | 48.19        |           773 | 33.50        | 32.63        |           988 | 37.60        | 37.33        |           808 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|  1 | Noise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 |           471 | [BOLD] 49.67 | [BOLD] 48.89 |           568 | [BOLD] 33.98 | [BOLD] 33.68 |           502 | [BOLD] 38.40 | [BOLD] 37.79 |           551 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms.",
        "evidence": "Although the number of terms in the vocabulary of the generative model is not unlimited, it can be used to automatically generate a list of terms. For example, we can understand the term “DepthDepth” of the TF model as having 789 terms with different values of term frequency. We can also understand it as having 211 terms that share the same value of term frequency with other terms. From the table, we can see that DocSub, DFS, and HLUST all tend to frequency-wise higher than the other embedding models, and our TF model is able to learn good embeddings even with unseen terms.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 957    | 1,000  | 1,000  | 1,000 | 1,000 | 836      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 44     | 1      | 1      | 1     | 1     | 43       | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,588  | 1,025  | 1,028  | 1,185 | 1,103 | 1,184    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 21     | 921    | 901    | 788   | 835   | 8        | 15       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 921    | 901    | 788   | 835   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 11.82  | 921    | 901    | 788   | 835   | 3.05     | 8.46     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 1.78   | 1      | 1      | 1     | 1     | 2.62     | 1.77     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 20     | 2      | 3      | 4     | 3     | 88       | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.99   | 1.03   | 1.03   | 1.19  | 1.10  | 4.20     | 2.38     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 476    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 164    | 2      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 521    | 1,029  | 1,331  | 3,025 | 3,438 | 3,802    | 1,009    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 16     | 915    | 658    | 454   | 395   | 118      | 12       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 913    | 658    | 454   | 395   | 110      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 5.82   | 914    | 658    | 454   | 395   | 112.24   | 5.95     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.75   | 1      | 1      | 1     | 1     | 1.05     | 2.02     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 25     | 2      | 77     | 13    | 12    | 66       | 98       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.83   | 1.03   | 1.36   | 3.03  | 3.44  | 6.64     | 2.35     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] For both datasets, our approach substantially outperforms the baselines.",
        "evidence": "As shown, for both datasets, our approach substantially outperforms the baselines. For example, on LDC2015e86, G2S-Gin has a slightly better BLEU score than S2S by 0.93 ± 0.20. Meanwhile, G2S-GGNN has a better METEOR score than all the baselines by at least 1.53% over G2S-Gin. These results show that our approach is very effective for the generation of fake news.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "We additionally find that supervised BLEU shows a trade-off with Acc: for a single model type, higher Acc generally corresponds to lower BLEU.",
        "evidence": "For comparison, we also show support for the proposition that accuracy is a trade-off between transferability and generation quality. Tab. The results support our hypothesis, with our untransferred model achieving 31.4 BLEU, a significant improvement over the unsupervised counterpart (31.4 BLEU, error reduction of 14.7%) and close to the state-of-the-art (0.9 BLEU) of \\newciteyang2018unsupervised.",
        "table": "+----+----------------------+----------------------+----------------------+\n|    | Model                | BLEU                 | Acc∗                 |\n+====+======================+======================+======================+\n|  0 | fu-1                 | [EMPTY]              | [EMPTY]              |\n+----+----------------------+----------------------+----------------------+\n|  1 | Multi-decoder        | 7.6                  | 0.792                |\n+----+----------------------+----------------------+----------------------+\n|  2 | Style embed.         | 15.4                 | 0.095                |\n+----+----------------------+----------------------+----------------------+\n|  3 | simple-transfer      | simple-transfer      | simple-transfer      |\n+----+----------------------+----------------------+----------------------+\n|  4 | Template             | 18.0                 | 0.867                |\n+----+----------------------+----------------------+----------------------+\n|  5 | Delete/Retrieve      | 12.6                 | 0.909                |\n+----+----------------------+----------------------+----------------------+\n|  6 | yang2018unsupervised | yang2018unsupervised | yang2018unsupervised |\n+----+----------------------+----------------------+----------------------+\n|  7 | LM                   | 13.4                 | 0.854                |\n+----+----------------------+----------------------+----------------------+\n|  8 | LM + classifier      | [BOLD] 22.3          | 0.900                |\n+----+----------------------+----------------------+----------------------+\n|  9 | Untransferred        | [BOLD] 31.4          | 0.024                |\n+----+----------------------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "RoBERTa, due to its optimizations and higher training data, outperforms the other models by a significant margin, indicating the large potential for models trained on much larger data",
        "evidence": "A comparison between BERT-large and RoBERTa shows that RoBERTa, due to its optimizations and higher training data, outperforms all other models by a significant margin.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Overall results show that ATR achieves the best performance and consumes the least training time.",
        "evidence": "A5SS1SSS0Px4 Impact of ATR pretraining. We compare ATR to 6 different models, including LSTM, GRU, SRU, and LRN. From the table, we can see that ATR achieves the best performance with a negligible increase in training time compared to all other models. We also observe that BERT significantly reduces the training time compared to vanilla LSTM and GRU. From these results, we can conclude that ATR is significantly faster than all other models with a negligible increase in training time.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "Therefore, our method covers most contexts where “to” is an",
        "evidence": "The In context is easy to understand — and this is another cue. The In context, the positive class is obvious and our method achieves an accuracy of 77.7%. For the negative class, the class is difficult to predict — and our method only achieves 36.6% accuracy. Therefore, our method seems to be effective for the difficult contexts.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "GDPL achieves extremely high performance in the task success on account of the substantial improvement in inform F1 and match rate over the baselines.",
        "evidence": "We compared GDPL with three baselines. It confirms that GDPL achieves extremely high performance in the task success on account of the substantial improvement in inform F1 and match rate over the baselines. By comparing the results of GDPL with GDPL-sess and GDPL-discr, we can observe that GDPL has better performance on task success than GDPL-sess and GDPL-discr. We also observe that GDPL is comparable to human on task success.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "2018b; Dong et\\xa0al.",
        "evidence": "In this section, we present the experiments and results of jointly training with the classification model in Narayan et al. For the tasks, we only train the network for 3 epochs as suggested by Narayan et al. We use the Adam optimizer with a learning rate of 0.0001, clipping the gradients which have a norm larger than 1.0. We train the network for 15 epochs, use the vanilla SGD optimizer with a learning rate of 0.0005, and take the evaluated reward as the final reward. Compared with all these previous methods, our model obtains the best results on all of the metrics. Particularly, our model only needs 0.2% absolute improvement on ROUGE-1,2,L and 0.5% absolute improvement on ROUGE-2,L.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] In Librispeech + DEMAND, minimizing DCE (15.8%) and FSEGAN (14.9%) achieves a lower WER than acoustic supervision (15.6%) and multi-task learning (14.4%).",
        "evidence": "We then experimented with the data on which the best performance was achieved. Overall, we found that multi-task learning, i.e., minimizing DCE and FSEGAN, achieved lower WER than acoustic supervision (15.6%) and multi-task learning (14.4%). We also observed that acoustic supervision (15.6%) outperformed minimizing DCE (15.8%) in terms of WER, and multi-task learning (14.4%) achieved lower WER than minimizing DCE (15.8%) and FSEGAN (14.9%).",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 17.3        | 0.828        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 19.5        | 0.722        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 15.8        | [BOLD] 0.269 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 14.9        | 0.291        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 15.6        | 0.330        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 5.7         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005) with borderline significance for WN-N and WN-V (p ≈ .05).",
        "evidence": "window-based W2 contexts pos disambiguation yields significantly better F scores on lemmatized targets for WN-N and WN-V, with borderline significance for VN. [CONTINUE] POS disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005) with borderline significance for WN-N and WN-V (p ≈ .05),",
        "table": "+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|    | [EMPTY]      | WN-N P            | WN-N R            | WN-N F            | WN-V P            | WN-V R            | WN-V F            | VN P              | VN R              | VN F              |\n+====+==============+===================+===================+===================+===================+===================+===================+===================+===================+===================+\n|  0 | Context: w2  | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  1 | type         | .700              | .654              | .676              | .535              | .474              | .503              | .327              | .309              | .318              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  2 | x+POS        | .699              | .651              | .674              | .544              | .472              | .505              | .339              | .312              | .325              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  3 | lemma        | .706              | .660              | .682              | .576              | .520              | .547              | .384              | .360              | .371              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  4 | x+POS        | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  5 | Context: dep | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  6 | type         | .712              | .661              | .686              | .545              | .457              | .497              | .324              | .296              | .310              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  7 | x+POS        | .715              | .659              | .686              | .560              | .464              | .508              | .349              | .320              | .334              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  8 | lemma        | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591              | .512              | .548              | .408              | .371              | .388              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  9 | x+POS        | .722              | .666              | .693              | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "In Italian, we get an increase of 91.67% of the gap with respect to English.",
        "evidence": "We see that the difference between German and Italian is not very large, close to 0.05. In Italian, we get an increase of 91.67% of the gap with respect to English. In German, we get a reduction of 91.67%.",
        "table": "+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|    | [EMPTY]          |   Italian Original |   Italian Debiased |   Italian English | Italian Reduction   |   German Original |   German Debiased |   German English | German Reduction   |\n+====+==================+====================+====================+===================+=====================+===================+===================+==================+====================+\n|  0 | Same Gender      |              0.442 |              0.434 |             0.424 | –                   |             0.491 |             0.478 |            0.446 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  1 | Different Gender |              0.385 |              0.421 |             0.415 | –                   |             0.415 |             0.435 |            0.403 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  2 | difference       |              0.057 |              0.013 |             0.009 | [BOLD] 91.67%       |             0.076 |             0.043 |            0.043 | [BOLD] 100%        |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] To validate Acc, human annotators were asked to judge the style of 100 transferred sentences [CONTINUE] We then compute the percentage of machine and human judgments that match.",
        "evidence": "To validate the transfer transfer claim, we asked human annotators to judge the style of 100 transferred sentences. We then compute the percentage of machine and human judgments that match. The results show that humans agree 100% among the transferred sentences. This is a strong signal that the transfer claim and ground truth contexts agree.",
        "table": "+----+----------+---------------------------------------------------------------------------+--------+--------+\n|    | Metric   | Method of validation                                                      |   Yelp |   Lit. |\n+====+==========+===========================================================================+========+========+\n|  0 | Acc      | % of machine and human judgments that match                               |  94    |  84    |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  1 | Sim      | Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation |   0.79 |   0.75 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  2 | PP       | Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency       |   0.81 |   0.67 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+",
        "label": "supports"
    },
    {
        "claim": "Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively.",
        "evidence": "As can be seen, the proposed DCGCN models achieve 20.5 and 13.1 BLEU points more than the best baseline models, achieving the best results for the English-German and English-Czech datasets, respectively. More importantly, we can see that the number of model parameters in the ensemble DCGCN models is much smaller than other baselines, which is expected because the models are designed for word prediction only.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "HDSA shows the effectiveness of explicitly capturing intent and dialog history.",
        "evidence": "The results of applying our domain-adaptive delexcalization and domain-aware belief span modeling are summarized in Table 5. [CONTINUE] The first group of Table 5 shows the results of our domain-aware belief span modeling and multi-action data augmentation. [CONTINUE] The relative performance between HDSA and DAMD significantly improves with the addition of multi-action data augmentation. [CONTINUE] HDSA without multi-action data augmentation (Table 6) achieves the best performance.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived.",
        "evidence": "We show that for all questions, our embeddings perform better than GloVe in all scenarios. In particular, the performance of our embeddings in the “All” questions subset is always higher than that of GloVe, and the performance of embeddings in the “Concept Word” questions subset is the best. However, the performance of embeddings in the “All” questions subset is only slightly improved by our embeddings when the lexical resource from which the concept wordgroups were derived is selected. The results demonstrate that a reasonable selection of the lexical resource is sufficient for the semantic features captured by embeddings, and that our embeddings can be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "supports"
    },
    {
        "claim": "Overall results show that LRN achieves competitive performance but consumes the least training time.",
        "evidence": "LSR achieves competitive performance while consumes the least training time compared to the other models. We also combine the two techniques to further evaluate the time efficiency of our method. In particular, we use base accuracy as well as BLEU score as our evaluation metric. From the table, we can see that: (1) LSTM and GRU achieve competitive results with significantly shorter training time than LRSN. The results show that our method is capable of further reducing the training time with no loss in accuracy and BLEU score. (2) Compared to ATR and SRU, LRSN achieves a further 3% improvement on accuracy with BLEU score and significantly lower training time. We (3) find that our method is capable of further improving the performance at the cost of significantly longer training time.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "Interestingly, G2S-GGNN has better performance among our models.",
        "evidence": "From the table, we see that G2S-GGNN has better performance than our G2S-GIN and S2S models on all datasets. For example, in LDC2015e86, G2S-GGNN beats S2S by about 1.12 BLEU points with less than 1.08 error rate. Moreover, in G2S-GAT, we achieve better results than all the other baselines. G2S-GAT achieves 27.87 BLEU points, which is over 2.7 points better than the best baseline. As an example, in LDC2017T10, G2S-GGNN beats S2S by about 1.13 BLEU points. Therefore, our proposed model G2S-GGNN has better performance than all the baselines.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "with respect to  the efficiency criteria, in which task the dialog systems take shorter time to reach the successful termination in an average and the total dialog time is shorter when averaged across all dialog sessions, the trend shows that all dialog methods have a strong tendency to increase dialog latency with time.",
        "evidence": "The results show that all dialog methods have a strong tendency to increase dialog latency with time. All ACER, PPO and ALDM dialog systems take significantly longer time to complete than ACER and PPO in terms of success rate, while ALDM dialog systems take significantly shorter time compared to PPO. In comparison, PPO and ALDM take very similar time compared to ACER, which demonstrates the superiority of PPO over ALDM.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "The mechanism successfully alleviates the over-fitting issue caused by the imbalanced two tasks’ sizes.",
        "evidence": "We can see that our method successfully alleviates the over-fitting issue caused by the imbalanced two tasks’ sizes. The results show that our method improves the model’s performance in both tasks, especially for the in-domain evaluation dataset named SQuAD and the out-of-domain evaluation dataset named QA-SRL.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Surprisingly, GDPL even outperforms human in completing the task, and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate.",
        "evidence": "We compared GDPL with the baselines to get insights into how well they perform on the task. Surprisingly, GDPL achieves a comparable human performance, i.e., its average dialog turns are close to those of humans. GDPL is also better than GDPL-sess in terms of inform F1 and match rate, though GDPL has a slightly worse match rate than GDPL-discr",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "HAN models outperform both LogReg and SVM using the current set of features.",
        "evidence": "We see that HAN models (logistic regression and SVM) outperform all other models except HAN when using the current set of features. HAN models with pre-trained word embeddings (pretrainU) outperform HAN while using only sentence embeddings (pretrain) in terms of all ROUGE scores. When we add sentence embeddings as input to the HAN models (pretrainu), they outperform all HAN models while performing better than using them without them (sum-basic and lexRank). It shows that including sentence embeddings gives an advantage for summarization tasks. With respect to the two aggregated scores, ROUGE measures the summary quality and it negatively affects the BLEU score. This suggests that sentence embeddings and word embeddings capture complementary information which HAN models can take advantage of to improve summarization performance. It is worth mentioning that the current set of features is only sufficient to assess the summarization performance of individual models while HAN can be used in combination with any of these other features.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Table 1: In all language pairs, the best correlation is not achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans.",
        "evidence": "We experiment with our proposed metrics and baselines on all language pairs. The best result is achieved by averaging all of the embeddings from BERT pretrained on MNLI and using MAEans as the aggregator. We experiment with two variations of our metrics: WMD-1 and WMD-2 are only used for word-overview. We also experiment with combining the embeddings from multiple BERT layers, i.e., WMD-1/2+BERT+MNLI+PAGEANS. It can be seen that MAEans aggregates all of the embeddings from BERT pretrained on MNLI, while ELMo does not support this scenario. In all cases, our proposed metrics outperform both baselines by a large margin. This validates the effectiveness of our proposed metrics.",
        "table": "+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|    | Setting    | Metrics                      | <bold>Direct Assessment</bold> cs-en   | <bold>Direct Assessment</bold> de-en   | <bold>Direct Assessment</bold> fi-en   | <bold>Direct Assessment</bold> lv-en   | <bold>Direct Assessment</bold> ru-en   | <bold>Direct Assessment</bold> tr-en   | <bold>Direct Assessment</bold> zh-en   | <bold>Direct Assessment</bold> Average   |\n+====+============+==============================+========================================+========================================+========================================+========================================+========================================+========================================+========================================+==========================================+\n|  0 | Baselines  | METEOR++                     | 0.552                                  | 0.538                                  | 0.720                                  | 0.563                                  | 0.627                                  | 0.626                                  | 0.646                                  | 0.610                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  1 | Baselines  | RUSE(*)                      | 0.624                                  | 0.644                                  | 0.750                                  | 0.697                                  | 0.673                                  | 0.716                                  | 0.691                                  | 0.685                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  2 | Baselines  | BERTScore-F1                 | 0.670                                  | 0.686                                  | 0.820                                  | 0.710                                  | 0.729                                  | 0.714                                  | 0.704                                  | 0.719                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  3 | Sent-Mover | Smd + W2V                    | 0.438                                  | 0.505                                  | 0.540                                  | 0.442                                  | 0.514                                  | 0.456                                  | 0.494                                  | 0.484                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  4 | Sent-Mover | Smd + ELMO + PMeans          | 0.569                                  | 0.558                                  | 0.732                                  | 0.525                                  | 0.581                                  | 0.620                                  | 0.584                                  | 0.595                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  5 | Sent-Mover | Smd + BERT + PMeans          | 0.607                                  | 0.623                                  | 0.770                                  | 0.639                                  | 0.667                                  | 0.641                                  | 0.619                                  | 0.652                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  6 | Sent-Mover | Smd + BERT + MNLI + PMeans   | 0.616                                  | 0.643                                  | 0.785                                  | 0.660                                  | 0.664                                  | 0.668                                  | 0.633                                  | 0.667                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  7 | Word-Mover | Wmd-1 + W2V                  | 0.392                                  | 0.463                                  | 0.558                                  | 0.463                                  | 0.456                                  | 0.485                                  | 0.481                                  | 0.471                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  8 | Word-Mover | Wmd-1 + ELMO + PMeans        | 0.579                                  | 0.588                                  | 0.753                                  | 0.559                                  | 0.617                                  | 0.679                                  | 0.645                                  | 0.631                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n|  9 | Word-Mover | Wmd-1 + BERT + PMeans        | 0.662                                  | 0.687                                  | 0.823                                  | 0.714                                  | 0.735                                  | 0.734                                  | 0.719                                  | 0.725                                    |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n| 10 | Word-Mover | Wmd-1 + BERT + MNLI + PMeans | 0.670                                  | 0.708                                  | <bold>0.835</bold>                     | <bold>0.746</bold>                     | <bold>0.738</bold>                     | 0.762                                  | <bold>0.744</bold>                     | <bold>0.743</bold>                       |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+\n| 11 | Word-Mover | Wmd-2 + BERT + MNLI + PMeans | <bold>0.679</bold>                     | <bold>0.710</bold>                     | 0.832                                  | 0.745                                  | 0.736                                  | <bold>0.763</bold>                     | 0.740                                  | <bold>0.743</bold>                       |\n+----+------------+------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our proposed method does not outperform GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set.",
        "evidence": "We present the results from our experiments on semantic analogy test set and syntactic test set. GloVe embeddings outperform our proposed word embeddings in both accuracy and syntactic test set. However, word2sense model performs slightly better in syntactic test set among our methods. It can be interpreted that our proposed method does not outperform the other algorithms in semantic analogy test set, however, it performs well in syntactic test set.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "refutes"
    },
    {
        "claim": "Also in cross-document coreference, it achieves the best joint results, except for the CEAF metric.",
        "evidence": "The claim-premise coreference can be defined as follows. First, we assume that lemma standardization on entities and relations is available. Then, we link each claim to a single entity (for cross-document coreference) using the lemma standardization. The conclusion is that our method achieves better performance than the disjoint baseline (p<0.01, except for the CoNLL F1 score, which is a little lower than the best joint result).",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "BI and IS both individually outperform the oracle for all domains, [CONTINUE] With adaptive decoding, we can assume that a uniform ensemble will always perform better than a single model for any potentially unknown domain.",
        "evidence": "This could be because adaptive decoding improves the model’s ability to generalize to unseen domains. uniform decoding does not generalize well to unknown domains.",
        "table": "+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|    | [BOLD] Decoder configuration   | [BOLD] es-en  [BOLD] Health   | [BOLD] es-en  [BOLD] Bio   | [BOLD] en-de  [BOLD] News   | [BOLD] en-de  [BOLD] TED   | [BOLD] en-de  [BOLD] IT   |\n+====+================================+===============================+============================+=============================+============================+===========================+\n|  0 | Oracle model                   | 35.9                          | 36.1                       | 37.8                        | 24.1                       | 39.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  1 | Uniform                        | 33.1                          | 36.4                       | 21.9                        | 18.4                       | 38.9                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  2 | Identity-BI                    | 35.0                          | 36.6                       | 32.7                        | 25.3                       | 42.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  3 | BI                             | 35.9                          | 36.5                       | 38.0                        | 26.1                       | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  4 | IS                             | [BOLD] 36.0                   | 36.8                       | 37.5                        | 25.6                       | 43.3                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  5 | BI + IS                        | [BOLD] 36.0                   | [BOLD] 36.9                | [BOLD] 38.4                 | [BOLD] 26.4                | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results show that coverage information does not improve the generalization of both examined models across various NLI datasets.",
        "evidence": "This claim is true. The results show that incorporating coverage information does not improve the generalization of both examined models across various NLI datasets. The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Further, contrary to intuition, the sob emoji contributes less than cry, despite representing a stronger emotion.",
        "evidence": "We see that the sob emoji contributes less than cry, despite representing a stronger emotion. The difference is more significant for the “no-emoji” categories, where the sob emoji contributes 3.67% of the total number of emojis, while the cry emoji contributes 12.07%.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Our text classifiers for identifying negation cues and finding the negation scope are built on top of a BERT classifier",
        "evidence": "The claim text is bombarded with negation phrases one by one. A false negation is generated when the first false negation is matched with a positive negation. We observe that false negation cues are shorter on average than true negation cues. The average sentence length of false negation cues is 13.6, while the average tweet length is 22.3. Finding the negation scope is also significantly harder. It only takes a single false negation to get a prediction of the wrong negation.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "we see that in most cases fine-tuning on B-COPA does not help the models’ performance, only their robustness.",
        "evidence": "In the first experiment, we fine-tune the models on B-COPA and evaluate their performance on the Easy and Hard subsets. We see that fine-tuning on B-COPA does not improve the models’ performance. In fact, it slightly degrades their performance on the Hard subset. The reason is that fine-tuning on B-COPA decreases the accuracy of these models on the overall COPA set. As the BERT-Large and RoBERTa-Large models are already fine-tuned, fine-tuning on B-COPA on the overall COPA set does not improve their performance.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, CMOW generally outperforms CBOW embeddings.",
        "evidence": "The overall improvement is roughly in the range of +3.1%− +7.2%, though there is a more varied variation depending on the corpus.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "The results in the table suggest that cleaning the missing slots did not provide more complex training examples.",
        "evidence": "The results show that the simple TGen model can still perform well even if the slots are missing, which supports our claim. For example, when the slots for add are missing, the drop in performance from 15.94 to 12.09% on the original test set is only because of the wrong add. In addition, even if the LSTM is trained better than the generative model (TGen−), the performance on par with the generative model is not sufficient. This shows that LSTM cannot leverage the information from the slot filling decoder better than the generative model.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Lemmatized targets generally perform better, with the boost being more pronounced on SimVerb.",
        "evidence": "In this section, we provide an additional experiment using lemmatized targets for comparison. Lemmatized targets generally perform better than their corresponding targets in all cases, with the boost being more pronounced on SimVerb. This difference appears to be related to the fact that SimVerb is significantly more verb-like than W2.",
        "table": "+----+----------------+----------------------+-----------------------+\n|    | Context: w2    | Context: w2 SimLex   | Context: w2 SimVerb   |\n+====+================+======================+=======================+\n|  0 | target         | all                  | V                     |\n+----+----------------+----------------------+-----------------------+\n|  1 | type           | .348                 | .307                  |\n+----+----------------+----------------------+-----------------------+\n|  2 | x + POS        | .350                 | .279                  |\n+----+----------------+----------------------+-----------------------+\n|  3 | lemma          | <bold>.351</bold>    | .400                  |\n+----+----------------+----------------------+-----------------------+\n|  4 | x + POS        | .345                 | <bold>.406</bold>     |\n+----+----------------+----------------------+-----------------------+\n|  5 | * type         | .339                 | .277                  |\n+----+----------------+----------------------+-----------------------+\n|  6 | * type MFit-A  | .385                 | -                     |\n+----+----------------+----------------------+-----------------------+\n|  7 | * type MFit-AR | .439                 | .381                  |\n+----+----------------+----------------------+-----------------------+\n|  8 | Context: dep-W | Context: dep-W       | Context: dep-W        |\n+----+----------------+----------------------+-----------------------+\n|  9 | type           | .362                 | .314                  |\n+----+----------------+----------------------+-----------------------+\n| 10 | x + POS        | .359                 | .287                  |\n+----+----------------+----------------------+-----------------------+\n| 11 | lemma          | <bold>.379</bold>    | .401                  |\n+----+----------------+----------------------+-----------------------+\n| 12 | x + POS        | .366                 | <bold>.431</bold>     |\n+----+----------------+----------------------+-----------------------+\n| 13 | * type         | .376                 | .313                  |\n+----+----------------+----------------------+-----------------------+\n| 14 | * type MFit-AR | .434                 | .418                  |\n+----+----------------+----------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "When redundancy removal was applied to LogReg, it produces significant improvement.",
        "evidence": "This table shows that: (1) The HAN models outperform all the baseline models by around 2% in terms of ROUGE-1 and ROUGE-2. This indicates that redundancy removal is able to improve the quality of the summary. (2) Our HAN models outperform all the baselines except MEAD. This is because MEAD is a relatively complex model which requires a large number of human-labeled training data. (3) The proposed HAN models, when applied to LogReg, achieves the best performance. This shows that redundancy removal is able to improve the quality of summary.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "We found that innovations are not helpful in both early and late fusion frameworks, and late fusion does not perform better on average.",
        "evidence": "We compared the text-only and the raw baselines in the scenario of early fusion. We also observed that the text-only baseline is much worse than the raw baseline. This suggests that information from distributed contextual embedding is not helpful in creating fusion errors. On the other hand, we see that the late fusion framework performs better than the early fusion framework. This suggests that information from distributed contextual embedding is helpful in both early and late fusion frameworks.",
        "table": "+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|    | [EMPTY]   | [BOLD] Model             | [BOLD] dev mean   | [BOLD] dev best   | [BOLD] test mean   | [BOLD] test best   | [ITALIC] α   |\n+====+===========+==========================+===================+===================+====================+====================+==============+\n|  0 | single    | text                     | 86.54             | 86.80             | 86.47              | 86.96              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  1 | single    | raw                      | 35.00             | 37.33             | 35.78              | 37.70              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  2 | single    | innovations              | 80.86             | 81.51             | 80.28              | 82.15              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  3 | early     | text + raw               | 86.46             | 86.65             | 86.24              | 86.53              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  4 | early     | text + innovations       | 86.53             | 86.77             | 86.54              | 87.00              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  5 | early     | text + raw + innovations | 86.35             | 86.69             | 86.55              | 86.44              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  6 | late      | text + raw               | 86.71             | 87.05             | 86.35              | 86.71              | 0.2          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  7 | late      | text + innovations       | [BOLD] 86.98      | [BOLD] 87.48      | [BOLD] 86.68       | [BOLD] 87.02       | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  8 | late      | text + raw + innovations | 86.95             | 87.30             | 86.60              | 86.87              | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model cc.el.300 has outperformed all the other models apart from the case of the Syntactic category when we included the out-of-vocabulary (oov) terms [CONTINUE] where the model gr def had the best performance.",
        "evidence": "Model CC.EL.300 has outperformed all the other models apart from the case of the syntactic category when we included the out-of-vocabulary (oov) terms [CONTINUE] where the model GR def had the best performance. Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model CC.EL.300 has outperformed all the other models apart from the case of the syntactic category when we included the out-of-vocabulary (oov) terms.",
        "table": "+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|    | Category Semantic   | Category no oov words   | gr_def 58.42%   | gr_neg10 59.33%   | cc.el.300  [BOLD] 68.80%   | wiki.el 27.20%   | gr_cbow_def 31.76%   | gr_d300_nosub 60.79%   | gr_w2v_sg_n5 52.70%   |\n+====+=====================+=========================+=================+===================+============================+==================+======================+========================+=======================+\n|  0 | [EMPTY]             | with oov words          | 52.97%          | 55.33%            | [BOLD] 64.34%              | 25.73%           | 28.80%               | 55.11%                 | 47.82%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  1 | Syntactic           | no oov words            | 65.73%          | 61.02%            | [BOLD] 69.35%              | 40.90%           | 64.02%               | 53.69%                 | 52.60%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  2 | [EMPTY]             | with oov words          | [BOLD] 53.95%   | 48.69%            | 49.43%                     | 28.42%           | 52.54%               | 44.06%                 | 43.13%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  3 | Overall             | no oov words            | 63.02%          | 59.96%            | [BOLD] 68.97%              | 36.45%           | 52.04%               | 56.30%                 | 52.66%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+\n|  4 | [EMPTY]             | with oov words          | 53.60%          | 51.00%            | [BOLD] 54.60%              | 27.50%           | 44.30%               | 47.90%                 | 44.80%                |\n+----+---------------------+-------------------------+-----------------+-------------------+----------------------------+------------------+----------------------+------------------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "Replacing the attention normalizing function with softmax operation increases the F1 score marginally (A3−A5).",
        "evidence": "We could replace the attention normalizing function with softmax operation (A3−A5) and increase the F1 score by (A3−A5). However, the F1 score is slightly decreased (A3−A5) compared to the case of fine-tuning without normalization (A4−A5). The Softmax operation does not improve the F1 score over the baseline (A2−A3) in the way that the attention normalization performs marginally better (A3−A5) in comparison to fine-tuning without normalization (A4−A5), but it achieves a better F1 score (A3−A6) compared to the baseline (A2−A3).",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "For each model we report both perplexity and accuracy (except for discriminative training, where perplexity is not valid), where each of them is reported according to the best performing model on that measure (on the dev set).",
        "evidence": "This table shows that the fine-tuned model with discriminative training (“Only-Disc”) performs the best, and the language-only models (“English-Only-LM”) are the worst. Unsurprisingly, the language-only models are the worst, which is expected since LM’s performance is closely related to that of the language model.",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] A notable exception is the \"Seanad Abolition\" dataset, where TF-IDF performs relatively better than WMD, Sent2vec and Doc2vec.",
        "evidence": "We observe that OD significantly outperforms all the baselines, with TF-IDF performing relatively better than WMD, Sent2vec and Doc2vec. One notable exception is the \"Seanad Abolition\" dataset, where TF-IDF performs relatively better than WMD and Doc2vec. We believe that this is due to the fact that the text in the \"Seanad Abolition\" dataset is well compressed, with few relevant entities.",
        "table": "+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|    | Methods   | Seanad Abolition ARI   | Seanad Abolition  [ITALIC] Sil   | Video Games ARI   | Video Games  [ITALIC] Sil   | Pornography ARI   | Pornography  [ITALIC] Sil   |\n+====+===========+========================+==================================+===================+=============================+===================+=============================+\n|  0 | TF-IDF    | 0.23                   | 0.02                             | -0.01             | 0.01                        | -0.02             | 0.01                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  1 | WMD       | 0.09                   | 0.01                             | 0.01              | 0.01                        | -0.02             | 0.01                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  2 | Sent2vec  | -0.01                  | -0.01                            | 0.11              | 0.06                        | 0.01              | 0.02                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  3 | Doc2vec   | -0.01                  | -0.03                            | -0.01             | 0.01                        | 0.02              | -0.01                       |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  4 | BERT      | 0.03                   | -0.04                            | 0.08              | 0.05                        | -0.01             | 0.03                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  5 | OD-parse  | 0.01                   | -0.04                            | -0.01             | 0.02                        | 0.07              | 0.05                        |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+\n|  6 | OD        | [BOLD] 0.54            | [BOLD] 0.31                      | [BOLD] 0.56       | [BOLD] 0.42                 | [BOLD] 0.41       | [BOLD] 0.41                 |\n+----+-----------+------------------------+----------------------------------+-------------------+-----------------------------+-------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Results presented in Table 7 show that the domain adaptation approach does not significantly boost F1 (t-test, p>0.5) and ROC AUC (0.012).",
        "evidence": "Results presented in Table 7 show that the domain adaptation approach does not significantly boost F1 and ROC AUC. [CONTINUE] The domain adaptation models outperform the most frequent class baseline, however, our model still substantially outperforms this baseline. [CONTINUE] We believe that the strong correlation between the domain adaptation and the strong results of the baseline model – i.e. lr-all features – shows that this approach does not generalize well.",
        "table": "+----+---------------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model                    | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+=================================+==============+=============+==============+\n|  0 | Most Frequent Class             | 64.2         | 39.1        | 0.500        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  1 | LR-All Features – Original Data | 80.5         | 78.0        | 0.873        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  2 | Dist. Supervision + Pooling     | 77.2         | 75.7        | 0.853        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  3 | Dist. Supervision + EasyAdapt   | [BOLD] 81.2  | [BOLD] 79.0 | [BOLD] 0.885 |\n+----+---------------------------------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "when the best model for each dataset is deployed in either setting, our program ablation does best and the non-ablated tree does slightly worse but still significantly outperforms the baseline sentence model (t = 12.7, p = 5.0×10-36).",
        "evidence": "Ablation Study. We perform ablation study to measure the effectiveness of our program. In the first setting, we compare the non-ablated model against the baseline sentence model. The non-ablated model is slightly worse but still significantly outperforms the baseline sentence model (t = 12.7, p = 5.0×10-36).. This indicates that the model is able to build upon the strong signal provided by the strong training and development sets it receives. In the second setting, we compare the model to the best model that is trained using the large training set. We again see that the non-ablated model outperforms the baseline sentence model, but this is more pronounced than in the “t = 12.7, p = 5.0×10-33” setting.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "In German, we get a reduction of 100%.",
        "evidence": "We show the results for Italian and German, the best results in each language. In Italian, we find a reduction of 91.67% of the GloVe embedding similarity score. In German, we get a reduction of 100%.",
        "table": "+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|    | [EMPTY]          |   Italian Original |   Italian Debiased |   Italian English | Italian Reduction   |   German Original |   German Debiased |   German English | German Reduction   |\n+====+==================+====================+====================+===================+=====================+===================+===================+==================+====================+\n|  0 | Same Gender      |              0.442 |              0.434 |             0.424 | –                   |             0.491 |             0.478 |            0.446 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  1 | Different Gender |              0.385 |              0.421 |             0.415 | –                   |             0.415 |             0.435 |            0.403 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  2 | difference       |              0.057 |              0.013 |             0.009 | [BOLD] 91.67%       |             0.076 |             0.043 |            0.043 | [BOLD] 100%        |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "Although these four models have the same number of layers, dense connections allow the model to achieve much better performance.",
        "evidence": "Although these four models have the same number of layers, the dense blocks allow the model to achieve much better performance. Examples in Fig. These results show that when compared with the -{3, 4} dense blocks, the BLEU score drops by 1.8 points and the CIDEr score drops by 1.6 points. Similar results can be seen when comparing with the DCGCN4. We believe that the highway connections between the individual layers allows the model to focus on particular words, and therefore the model outperforms the baseline by a large margin.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "supports"
    },
    {
        "claim": "The relatively low accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are not well-equipped to perform this task \"out-of-the-box\".",
        "evidence": "The relatively low accuracies of BERT-Large, RoBERTa-Large and BERT-*-NSP show that these pretrained models are not well-equipped to perform this task \"out-of-the-box\"..",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "Longer sentences pose additional challenges to the models.",
        "evidence": "The claim makes use of the following information: longer graphs pose additional challenges to the models. Firstly, the sizes of the graph DNNs and G2S models are greater than Gaussian networks, which means that the model must take into account longer context to properly capture the semantics of words. Secondly, the out-degrees of G2S-GAT and G2S-GGNN are smaller than those of S2S, which means that the models are able to extract features and processes longer sequences better. Finally, the gap between the out-degrees of G2S-GAT and S2S is smaller than that of G2S-GGNN, which suggests that the GAT model can better represent the semantics of words and processes longer sentences.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] As a result, the folding technique performs better than the recursive approach for the training task.",
        "evidence": "Specifically, we fold the data and evaluate the performance of the recursive approach against the PLM objective. As expected, as a result, the folding technique performs better than the recursive approach for the training task.. The speedups range from less than 1/s (1 minute for batch size 1) to more than 100% for a batch size of 25.",
        "table": "+----+--------------+--------------------------------------+-------------------------------------+\n|    | Batch size   | Throughput (instances/s) Inference   | Throughput (instances/s) Training   |\n+====+==============+======================================+=====================================+\n|  0 | Batch size   | Fold                                 | Fold                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  1 | 1            | 16.5                                 | 9.0                                 |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  2 | 10           | 52.2                                 | 37.5                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  3 | 25           | 61.6                                 | 54.7                                |\n+----+--------------+--------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The largest gain is by 4% on the CoordInv task.",
        "evidence": "The results agree with our intuition. While our performance gain on CoordInv task is relatively small compared to other similarity measures, we see that the subjNum and CoordInv tasks are improved. An average gain of 6.1% on CoordInv and 10.8% on CoordInv respectively over the CBOW/400 model, which is greater than the difference between our implementation of CBOW and CMOW.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs.",
        "evidence": "The results show that as the graph size increases, encoding a bigger graph (containing more information) is harder than encoding a smaller graph (containing smaller information). We can see the increase in performance gap between the two architectures is significant for all the models.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The inferior score on attention relevance shows that TVMAX is worse at selecting the relevant features and its output is less interpretable.",
        "evidence": "Here, we compare the comparison results between TVMAX and its variants sparsemax, softmax, and sparsemax. The superior score on attention relevance shows that TVMAX is worse at selecting the relevant features and its output is less interpretable.",
        "table": "+----+-----------+-------------+-----------------------+\n|    | [EMPTY]   | caption     | attention relevance   |\n+====+===========+=============+=======================+\n|  0 | softmax   | 3.50        | 3.38                  |\n+----+-----------+-------------+-----------------------+\n|  1 | sparsemax | 3.71        | 3.89                  |\n+----+-----------+-------------+-----------------------+\n|  2 | TVmax     | [BOLD] 3.87 | [BOLD] 4.10           |\n+----+-----------+-------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "Row (1)-(7) show each model with different representations on the original dataset.",
        "evidence": "Our model is able to achieve comparable or better performance than the state-of-the-art without using any additional reward.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give significantly different results for both 1 DCGCN block and 2 DCGCN blocks.",
        "evidence": "To verify our claim that the more GCN layers we use, the better the performance of our model, we conduct experiments with different numbers of DCGCN blocks. Firstly, we observe that the performance of our model improves as the number of GCGCN blocks increases from 1 to 2. Particularly, we observe that the performance of our model with m=6 is the best. The same phenomenon can be observed with n=3. Adding more GCGCN blocks, however, does not lead to consistent performance improvement. We think that the reason may be that too many GCGCN layers can cause overfitting. For example, if three GCGCN blocks are stacked, the performance of one block is slightly better than the other. However, increasing the number of GCN blocks, either by increasing the number of GCN layers or increasing the number of GCN layers, is likely to make the model overfitted.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    |   [ITALIC] Block |   [ITALIC] n |   [ITALIC] m | B           | C           |\n+====+==================+==============+==============+=============+=============+\n|  0 |                1 |            1 |            1 | 17.6        | 48.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 |                1 |            1 |            2 | 19.2        | 50.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 |                1 |            2 |            1 | 18.4        | 49.1        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 |                1 |            1 |            3 | 19.6        | 49.4        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 |                1 |            3 |            1 | 20.0        | 50.5        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 |                1 |            3 |            3 | 21.4        | 51.0        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 |                1 |            3 |            6 | 21.8        | 51.7        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 |                1 |            6 |            3 | 21.7        | 51.5        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 |                1 |            6 |            6 | 22.0        | 52.1        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 |                2 |            3 |            6 | [BOLD] 23.5 | 53.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 |                2 |            6 |            3 | 23.3        | [BOLD] 53.4 |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 |                2 |            6 |            6 | 22.0        | 52.1        |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "(2017).8 Overall both BERT (76.5%) and RoBERTa (87.7%) do not outperform the best previous model (71.4%) on Hard instances without superficial cues.",
        "evidence": "(Sasaki et al., et al. Obviously, word frequency counts are not enough for this task. The claim that BERT and RoBERTa can achieve better performance without word frequency counts is true. However, BERT and RoBERTa still do not outperform the best previous model by large margins.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "In particular, we see that hate speech and harassment are relatively easy to detect.",
        "evidence": "Here, we compare the performance of our method to other state-of-the-art approaches. In particular, we see that hate speech and harassment are relatively easy to detect.. our method has an F1 score of 0.88 compared to 0.65 and 0.71 for W. & H. and R. & S.",
        "table": "+----+--------------------+-----------+-------------+----------+------+\n|    | Dataset            | Class     |   Precision |   Recall |   F1 |\n+====+====================+===========+=============+==========+======+\n|  0 | [ITALIC] W. & H.   | Racism    |        0.73 |     0.79 | 0.76 |\n+----+--------------------+-----------+-------------+----------+------+\n|  1 | [EMPTY]            | Sexism    |        0.69 |     0.73 | 0.71 |\n+----+--------------------+-----------+-------------+----------+------+\n|  2 | [EMPTY]            | Neither   |        0.88 |     0.85 | 0.86 |\n+----+--------------------+-----------+-------------+----------+------+\n|  3 | [ITALIC] W.        | Racism    |        0.56 |     0.77 | 0.65 |\n+----+--------------------+-----------+-------------+----------+------+\n|  4 | [EMPTY]            | Sexism    |        0.62 |     0.73 | 0.67 |\n+----+--------------------+-----------+-------------+----------+------+\n|  5 | [EMPTY]            | R. & S.   |        0.56 |     0.62 | 0.59 |\n+----+--------------------+-----------+-------------+----------+------+\n|  6 | [EMPTY]            | Neither   |        0.95 |     0.92 | 0.94 |\n+----+--------------------+-----------+-------------+----------+------+\n|  7 | [ITALIC] D. et al. | Hate      |        0.32 |     0.53 | 0.4  |\n+----+--------------------+-----------+-------------+----------+------+\n|  8 | [EMPTY]            | Offensive |        0.96 |     0.88 | 0.92 |\n+----+--------------------+-----------+-------------+----------+------+\n|  9 | [EMPTY]            | Neither   |        0.81 |     0.95 | 0.87 |\n+----+--------------------+-----------+-------------+----------+------+\n| 10 | [ITALIC] G. et al. | Harass.   |        0.41 |     0.19 | 0.26 |\n+----+--------------------+-----------+-------------+----------+------+\n| 11 | [EMPTY]            | Non.      |        0.75 |     0.9  | 0.82 |\n+----+--------------------+-----------+-------------+----------+------+\n| 12 | [ITALIC] F. et al. | Hate      |        0.33 |     0.42 | 0.37 |\n+----+--------------------+-----------+-------------+----------+------+\n| 13 | [EMPTY]            | Abusive   |        0.87 |     0.88 | 0.88 |\n+----+--------------------+-----------+-------------+----------+------+\n| 14 | [EMPTY]            | Spam      |        0.5  |     0.7  | 0.58 |\n+----+--------------------+-----------+-------------+----------+------+\n| 15 | [EMPTY]            | Neither   |        0.88 |     0.77 | 0.82 |\n+----+--------------------+-----------+-------------+----------+------+",
        "label": "refutes"
    },
    {
        "claim": "For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments but significantly different Sim scores.",
        "evidence": "At the same time, we can see that the human judgments of semantic preservation (M2>M0, M7>M0, M6 and M7 have very similar human ratings, while the differences in fluency (M0, M6, M7) are very large. For example, on the Yelp dataset, we see that M2 has the highest human agreement for Taste of M1, while M7 has the highest for Taste of M2. Also, we can see that the properties of both semantic preservation and fluency are very different from each other.",
        "table": "+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|    | Dataset    | Models A   | Models B   |   Transfer quality A>B |   Transfer quality B>A |   Transfer quality Tie | Semantic preservation A>B   | Semantic preservation B>A   |   Semantic preservation Tie |   Semantic preservation ΔSim | Fluency A>B   | Fluency B>A   |   Fluency Tie |   Fluency ΔPP |\n+====+============+============+============+========================+========================+========================+=============================+=============================+=============================+==============================+===============+===============+===============+===============+\n|  0 | [EMPTY]    | M0         | M2         |                    9   |                    6   |                   85.1 | 1.5                         | [BOLD] 25.4                 |                        73.1 |                        -0.05 | 10.4          | [BOLD] 23.9   |          65.7 |           0.9 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  1 | Yelp       | M0         | M7         |                    9.6 |                   14.7 |                   75.8 | 2.5                         | [BOLD] 54.5                 |                        42.9 |                        -0.09 | 4.6           | [BOLD] 39.4   |          56.1 |           8.3 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  2 | Yelp       | M6         | M7         |                   13.7 |                   11.6 |                   74.7 | 16.0                        | 16.7                        |                        67.4 |                         0.01 | 10.3          | 20.0          |          69.7 |          14.3 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  3 | [EMPTY]    | M2         | M7         |                    5.8 |                    9.3 |                   84.9 | 8.1                         | [BOLD] 25.6                 |                        66.3 |                        -0.04 | 14.0          | [BOLD] 26.7   |          59.3 |           7.4 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  4 | Literature | M2         | M6         |                    4.2 |                    6.7 |                   89.2 | 16.7                        | 20.8                        |                        62.5 |                         0.01 | [BOLD] 40.8   | 13.3          |          45.8 |         -13.3 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  5 | Literature | M6         | M7         |                   15.8 |                   13.3 |                   70.8 | [BOLD] 25.0                 | 9.2                         |                        65.8 |                         0.03 | 14.2          | 20.8          |          65   |          14.2 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "The interpolation weight α for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction.",
        "evidence": "The results show that the interpolation weight α for the late fusion experiments is high, which further indicates that innovation features are useful in overall prediction.",
        "table": "+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|    | [EMPTY]   | [BOLD] Model             | [BOLD] dev mean   | [BOLD] dev best   | [BOLD] test mean   | [BOLD] test best   | [ITALIC] α   |\n+====+===========+==========================+===================+===================+====================+====================+==============+\n|  0 | single    | text                     | 86.54             | 86.80             | 86.47              | 86.96              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  1 | single    | raw                      | 35.00             | 37.33             | 35.78              | 37.70              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  2 | single    | innovations              | 80.86             | 81.51             | 80.28              | 82.15              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  3 | early     | text + raw               | 86.46             | 86.65             | 86.24              | 86.53              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  4 | early     | text + innovations       | 86.53             | 86.77             | 86.54              | 87.00              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  5 | early     | text + raw + innovations | 86.35             | 86.69             | 86.55              | 86.44              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  6 | late      | text + raw               | 86.71             | 87.05             | 86.35              | 86.71              | 0.2          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  7 | late      | text + innovations       | [BOLD] 86.98      | [BOLD] 87.48      | [BOLD] 86.68       | [BOLD] 87.02       | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  8 | late      | text + raw + innovations | 86.95             | 87.30             | 86.60              | 86.87              | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The performances of all models increase as the diameters of the graphs increase.",
        "evidence": "The claims are shown in Table 1. As the size of the graphs increases, the performance of all models shows improvement. G2S-GAT models outperform S2S and G2S-GGNN models almost on all sizes of graphs. Also, the gap between G2S-GAT and S2S is smaller on small graphs (0-3 δ) than on large graphs (0-50 δ). This is because the information from G2S-GAT is richer. Moreover, the gap between G2S-GGNN and G2S-GAT is smaller on small graphs (0-3 δ) than on large graphs (0-20 δ).",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] however, GRU yields the best BLEU score of 26.28, outperforming oLRN (+0.45 BLEU).",
        "evidence": "With the same set of parameters, GRU achieves the best BLEU score, outperforming GNMT and LRN by 0.45 and 0.99 BLEU respectively. It is worth mentioning that GRU is trained on much more data than the other models.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better.",
        "evidence": "There is a lot of noise in the audio-visual retrieval datasets, but this is not the case with paraphrase retrieval. Generally, models that only use the visual modality as a retrieval mechanism (Audio2Vec-u and Audio2Vec-c) are much worse at paraphrase detection than paraphrase retrieval tasks. The best result with paraphrase retrieval is achieved by SegMatch, but this is very close to VGGS.",
        "table": "+----+-------------+-----------------+---------------+------------+\n|    | [EMPTY]     | Recall@10 (%)   | Median rank   | RSAimage   |\n+====+=============+=================+===============+============+\n|  0 | VGS         | 27              | 6             | 0.4        |\n+----+-------------+-----------------+---------------+------------+\n|  1 | SegMatch    | [BOLD] 10       | [BOLD] 37     | [BOLD] 0.5 |\n+----+-------------+-----------------+---------------+------------+\n|  2 | Audio2vec-U | 5               | 105           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  3 | Audio2vec-C | 2               | 647           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  4 | Mean MFCC   | 1               | 1,414         | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  5 | Chance      | 0               | 3,955         | 0.0        |\n+----+-------------+-----------------+---------------+------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no main effect on SER from cleaning the missed slots, with only slight reductions in insertions and deletions.",
        "evidence": "The experiment with the best performance for each metric (SUM) is shown in Tab. However, the results in the bottom half of Tab. The reason for this trend is discussed in the introduction. From the results, we can see that sentence generation optimized for performance of the target task is crucial to achieve acceptable results. For example, even if we train the model using only the generated slot candidates, the performance of the model after manually cleaning the missed slots is very poor compared to training using the generated slot candidates from the original test set. This shows the main effect of cleaning the missed slots is to reduce the insertion errors, while maintaining good performance on other metrics. In addition, even if we train the model using only the generated slot candidates, the performance of the model further improves after manually cleaning the missing slots.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold decreases recall and increases precision (differences are statistically significant with t-test, p < 0.05).",
        "evidence": "This is a strong signal that the cosinebased threshold function is more amenable to similarity measures. In particular, the results show that it leads to larger improvements in recall and smaller improvements in precision (differences are statistically significant with t-test, p < 0.05) for the Gigaword and NOW datasets.",
        "table": "+----+-----------+--------------------+--------------------+-----------------+-------------+\n|    | [EMPTY]   | [BOLD] Algorithm   |   [BOLD] Precision |   [BOLD] Recall | [BOLD] F1   |\n+====+===========+====================+====================+=================+=============+\n|  0 | Giga      | Baseline           |               0.19 |            0.51 | 0.28        |\n+----+-----------+--------------------+--------------------+-----------------+-------------+\n|  1 | Giga      | Threshold          |               0.46 |            0.41 | [BOLD] 0.41 |\n+----+-----------+--------------------+--------------------+-----------------+-------------+\n|  2 | NOW       | Baseline           |               0.26 |            0.53 | 0.34        |\n+----+-----------+--------------------+--------------------+-----------------+-------------+\n|  3 | NOW       | Threshold          |               0.42 |            0.41 | [BOLD] 0.41 |\n+----+-----------+--------------------+--------------------+-----------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "AME outperforms the FME model, confirming the importance of word embeddings adaptation.",
        "evidence": "The results show that AME outperforms the FME model in all the settings. For instance, in the case of R@1, AME outperforms the FME model by 2.1 percent and by 3.8 percent in R@5 and R@10. This observation confirms that word embeddings adaptation is crucial in improving the model’s performance.",
        "table": "+----+-----------+---------------+---------------+----------------+---------------+---------------+----------------+\n|    | [EMPTY]   | EN → DE R@1   | EN → DE R@5   | EN → DE R@10   | DE → EN R@1   | DE → EN R@5   | DE → EN R@10   |\n+====+===========+===============+===============+================+===============+===============+================+\n|  0 | FME       | 51.4          | 76.4          | 84.5           | 46.9          | 71.2          | 79.1           |\n+----+-----------+---------------+---------------+----------------+---------------+---------------+----------------+\n|  1 | AME       | [BOLD] 51.7   | [BOLD] 76.7   | [BOLD] 85.1    | [BOLD] 49.1   | [BOLD] 72.6   | [BOLD] 80.5    |\n+----+-----------+---------------+---------------+----------------+---------------+---------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "On the other hand, ACER is still subject to trainability limitation due to the lacking of expressivity power in DSTC models.",
        "evidence": "AlldM can outperform ACER in all cases, which is consistent with the results of the automatic evaluation. However, unlike PPO, ACER is still subject to trainability limitation due to the lacking of expressivity power in DSTC models.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] We observed no advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs similarly to either a 4 layer or a 2 layer SRU with a comparable number of parameters.",
        "evidence": "The main conclusion from this table is that there is no advantage to using a hierachical encoder. The number of parameters of the models is much larger than if we used a 4 layer encoder. In order to ensure that this is not an over-representation of the number of parameters, we trained each model with the same number of parameters as the base model, and evaluated on the test set. We see that a 2 layer LSTM performs similarly to either a 4 layer or a 2 layer SRU with a comparable number of parameters.",
        "table": "+----+----------------------------+---------------------+------------------------------+------------------------+\n|    | [BOLD] Model               | [BOLD] Parameters   | [BOLD] Validation AUC@0.05   | [BOLD] Test AUC@0.05   |\n+====+============================+=====================+==============================+========================+\n|  0 | Base                       | 8.0M                | [BOLD] 0.871                 | 0.816                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  1 | 4L SRU → 2L LSTM           | 7.3M                | 0.864                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  2 | 4L SRU → 2L SRU            | 7.8M                | 0.856                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  3 | Flat → hierarchical        | 12.4M               | 0.825                        | 0.559                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  4 | Cross entropy → hinge loss | 8.0M                | 0.765                        | 0.693                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  5 | 6.6M → 1M examples         | 8.0M                | 0.835                        | 0.694                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  6 | 6.6M → 100K examples       | 8.0M                | 0.565                        | 0.417                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  7 | 200 → 100 negatives        | 8.0M                | 0.864                        | 0.647                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  8 | 200 → 10 negatives         | 8.0M                | 0.720                        | 0.412                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+",
        "label": "supports"
    },
    {
        "claim": "The single DCGCN model achieves a BLEU score of 30.4 and a CHRF++ score of 59.6, outperforming the ensemble approach based on combining five DCGCN models initialized with different random seeds.",
        "evidence": "As can be seen, our single DCGCN model achieves a BLEU score of 30.4 and a chrF++ score of 59.6, outperforming the ensemble approach based on combining five GCN models initialized with different random seeds. We attribute this to the fact that the diversity of possible answers is so small (only 19.1%), which is sufficient for the single model to achieve good performance. The ensemble approach requires five GCN models to achieve the same performance, which demonstrates the effectiveness of the hierarchical structure proposed by GCN models.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "The proposed method does not outperform the original embeddings and performs worse than the SOV.",
        "evidence": "The results show that the proposed method is not better than the word2vec embeddings and performs worse than the sov embeddings.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "This superior confirms the effectiveness of our approach.",
        "evidence": "Figure 2: A conversation example between human (H) and agents (A) with the same target and starting utterance in CWC dataset.Keywords predicted by the agents or mentioned by human are highlighted in bold. The target achieved at the end of a conversation is underlined. judge whether the current conversation context contains the end target. We set a maximum number of turns as 8 to prevent from an endless conversation that can not reach the target. We use the success rate of reaching the targets (Succ.) and the average number of turns taken to achieve the target (#Turns) as our evaluation criterion. Table 6 shows the results of 5-category conversations simulated in our approach. Although the average number of turns of our approach is slightly more than Kernel, our system obtains the highest success rate, significantly improving over other approaches. We conduct user study for a more thorough on our CWC dataset. We use the DialCrowd e et al. 2018) to set up user study interfaces and kinds of user studies as below. The first study sure system performance in terms of two key amely transition smoothness and target achieveonstruct 50 test cases, each of which has a target g utterance. In each case, a user will chat with a lected agent. If the agent thinks the conversation the given target or the maximum number of conrns, the agent will inform and show the target to e DialCrowd will ask the user to judge whether reached and rate the transition smoothness of tion with a smoothness score. The smoothness s from 1 (very bad) to 5 (very good). All agents ur DKRN agent outpe Table 6 shows the results of the second study outperforms the comparison agents with a large example, compared to Kernel, 62% users deter agent is better on CWC dataset. This superior confirms the effectiveness of our approach. Qualitative Study To more intuitively demonstrate the superior per our agent, two conversations in",
        "table": "+----+----------------+------------------+---------------+-----------------+--------------+\n|    | System         | TGPC Succ. (%)   |   TGPC #Turns | CWC Succ. (%)   | CWC #Turns   |\n+====+================+==================+===============+=================+==============+\n|  0 | Retrieval      | 7.16             |          4.17 | 0               | -            |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  1 | Retrieval-Stgy | 47.80            |          6.7  | 44.6            | 7.42         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  2 | PMI            | 35.36            |          6.38 | 47.4            | 5.29         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  3 | Neural         | 54.76            |          4.73 | 47.6            | 5.16         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  4 | Kernel         | 62.56            |          4.65 | 53.2            | 4.08         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  5 | DKRN (ours)    | [BOLD] 89.0      |          5.02 | [BOLD] 84.4     | 4.20         |\n+----+----------------+------------------+---------------+-----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Among all the baselines, GDPL does not obtain the most preference against PPO.",
        "evidence": "According to the results, GDPL does not obtain the most preference against PPO. Among all the baselines, GDPL has achieved 19% and 19% wins in terms of efficiency and quality against ALDM, respectively. In comparison, PPO has obtained 13% and 13% wins in terms of efficiency and quality.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "The DCGCN models do not achieve the highest BLEU points on the En-De and En-Cs tasks, respectively.",
        "evidence": "As can be seen from the table, the DCGCN models achieve the best performance in comparison with the baseline models of both Seq2Seq and GGN2Seq. In particular, the DCGCN model outperforms the best performing baseline model of Seq2Seqb by 29.7, 19.0, 45.8, 28.3, 13.1 BLEU points for English-German, English-Czech and English-German respectively. More importantly, the results show that the DCGCN model achieves the best performance in terms of BLEU points as well. These results outperform the baselines by a large margin without ensembling.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Longer sentences do not pose additional challenges to the models.",
        "evidence": "The results show that the proposed GNN and G2S models significantly outperform the S2S and G2S-GAT models on the larger models. For example, on the smaller model G2S-GGNN, the model sizes are smaller 0-20 and 50-240, which shows that the proposed GNN and G2S models are robust to the change in model size. Likewise, the models show improved performance on the smaller models. G2S-GAT obtains the best performance on all models except for the S2S and G2S-GAT, which are slightly worse than the G2S models. Unlike the results on the smaller models, the degradation in performance on the larger models is much smaller. This shows that the proposed GNN and G2S models are robust to the change in model size.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "our model achieves a slightly better performance in AUC than the baseline models and the proposed model works better.",
        "evidence": "The claims are supported or falsified based on the information provided. From the table, we can see that our model achieves a slightly better performance in AUC than the baseline models. This might imply that our proposed model works better..",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "This suggests that our models are not capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.",
        "evidence": "This indicates that our models are not capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences. The results show that the greatest improvement of our model over S2S and G2S-GIN is obtained when the structural information is kept in the graph generating outputs instead of the word embeddings. Moreover, the experimental results also show that the information from the graph generation model also extends to the semantics of the reference sentences.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Adding either the global node or the linear combination improves the baseline models with only dense connections.",
        "evidence": "Baseline models We can see that adding either the global node or the linear combination improves the baseline models with only dense connections. The results of adding the linear combination are better than adding either of the global nodes or the linear combination. This may be because the dot product and the linear combination can be viewed as two linear transformers. Complementing the global node with the linear combination is worse than adding either of the global nodes alone. We believe that the global node and the linear combination provide complementary information that is beneficial for the summarization task.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "We showed that it is not possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks.",
        "evidence": "We showed that it is not possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks. We keep the same architecture as that in the baseline model.",
        "table": "+----+------------------------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34                         | [BOLD] Eval set %   | [BOLD] #param   |\n+====+==========================================+=====================+=================+\n|  0 | Baseline (No SA)Anderson et al. ( 2018 ) | 55.00               | 0M              |\n+----+------------------------------------------+---------------------+-----------------+\n|  1 | SA (S: 1,2,3 - B: 1)                     | 55.11               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  2 | SA (S: 1,2,3 - B: 2)                     | 55.17               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  3 | [BOLD] SA (S: 1,2,3 - B: 3)              | [BOLD] 55.27        | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "The amount of resources is insufficient for executing forward computations, and therefore our framework does not outperform the folding technique for the inference task with up to 4.93x faster throughput.",
        "evidence": "Our framework does not outperform the folding technique for the inference task with up to 4.93x faster throughput..",
        "table": "+----+--------------+--------------------------------------+-------------------------------------+\n|    | Batch size   | Throughput (instances/s) Inference   | Throughput (instances/s) Training   |\n+====+==============+======================================+=====================================+\n|  0 | Batch size   | Fold                                 | Fold                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  1 | 1            | 16.5                                 | 9.0                                 |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  2 | 10           | 52.2                                 | 37.5                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  3 | 25           | 61.6                                 | 54.7                                |\n+----+--------------+--------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "We observed an advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs worse than either a 4 layer or a 2 layer SRU with a comparable number of parameters.",
        "evidence": "Finally, we see that a 2 layer LSTM performs consistently better than either a 4 layer or a 2 layer SRU model with a comparable number of parameters.",
        "table": "+----+----------------------------+---------------------+------------------------------+------------------------+\n|    | [BOLD] Model               | [BOLD] Parameters   | [BOLD] Validation AUC@0.05   | [BOLD] Test AUC@0.05   |\n+====+============================+=====================+==============================+========================+\n|  0 | Base                       | 8.0M                | [BOLD] 0.871                 | 0.816                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  1 | 4L SRU → 2L LSTM           | 7.3M                | 0.864                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  2 | 4L SRU → 2L SRU            | 7.8M                | 0.856                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  3 | Flat → hierarchical        | 12.4M               | 0.825                        | 0.559                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  4 | Cross entropy → hinge loss | 8.0M                | 0.765                        | 0.693                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  5 | 6.6M → 1M examples         | 8.0M                | 0.835                        | 0.694                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  6 | 6.6M → 100K examples       | 8.0M                | 0.565                        | 0.417                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  7 | 200 → 100 negatives        | 8.0M                | 0.864                        | 0.647                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  8 | 200 → 10 negatives         | 8.0M                | 0.720                        | 0.412                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The second row in Table 3 shows the test accuracy of a system trained without sense priors [CONTINUE] and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet.",
        "evidence": "The second row shows the results for a system trained without sense priors, and the third row shows the results for a system trained with sense priors. The effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially makes them type level representations. This results in a slight drop in accuracy for the system trained without sense priors, and a slight increase in accuracy for the system trained with sense priors.",
        "table": "+----+----------------+-------------------+\n|    | [BOLD] Model   |   [BOLD] PPA Acc. |\n+====+================+===================+\n|  0 | full           |              89.7 |\n+----+----------------+-------------------+\n|  1 | - sense priors |              88.4 |\n+----+----------------+-------------------+\n|  2 | - attention    |              87.5 |\n+----+----------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Hashtags also have a [CONTINUE] positive effect on classification performance, however it is less significant.",
        "evidence": "There is one silver lining: hashtags seem to have a slightly positive effect on classification performance. However, the difference is less significant than for emoji.",
        "table": "+----+-----------+------------------+----------------------+\n|    | [EMPTY]   | [BOLD] Present   | [BOLD] Not Present   |\n+====+===========+==================+======================+\n|  0 | Emoji     | 4805 (76.6%)     | 23952 (68.0%)        |\n+----+-----------+------------------+----------------------+\n|  1 | Hashtags  | 2122 (70.5%)     | 26635 (69.4%)        |\n+----+-----------+------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "Each participant evaluates 3 dialog sessions of each model.",
        "evidence": "In this section, we evaluate the performance of the systems against the strong claim from ACER and PPO. Each participant evaluates 3 dialog sessions of each model. We can see that ACER outperforms PPO in all aspects (sign test, p-value < 0.01)",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our model (OURS) obtains substantial gains in accuracy over the baselines across all three target aspects.",
        "evidence": "Our model obtains substantial gains over the baselines across all three target aspects. It outperforms Trans on all three datasets. It is worth noting that the oracle baseline (which is solely trained on the beer review data)",
        "table": "+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source            | Target      |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+===================+=============+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer aroma+palate | Beer look   | 74.41 |     74.83 |     74.94 |    72.75 |        76.41 | [BOLD] 79.53 |     80.29 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look+palate  | Beer aroma  | 68.57 |     69.23 |     67.55 |    69.92 |        76.45 | [BOLD] 77.94 |     78.11 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look+aroma   | Beer palate | 63.88 |     67.82 |     65.72 |    74.66 |        73.4  | [BOLD] 75.24 |     75.5  |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "supports"
    },
    {
        "claim": "With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.",
        "evidence": "This claim can only be answered by comparing the model’s performances against the baselines. From the table, we can see that when the coverage mechanism is disabled, the B and C scores drop by 1.7/2.4 points for B and 2.1 points for C respectively. This shows the effectiveness of the coverage mechanism.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "Note that the effectiveness of P1 and P2 are not necessarily additive, as combining P1 and P2 does not always perform the best.",
        "evidence": "We can see that P1 and P2 are not always additive. In fact, we find that P2 performs slightly better than P1 on HIACLE, while the results on the CoAtt and RVA datasets are very close.",
        "table": "+----+----------+--------------+----------+----------+--------+\n|    | Model    | LF           |   HCIAE  |   CoAtt  |   RvA  |\n+====+==========+==============+==========+==========+========+\n|  0 | baseline | 57.21        |    56.98 |    56.46 |  56.74 |\n+----+----------+--------------+----------+----------+--------+\n|  1 | +P1      | 61.88        |    60.12 |    60.27 |  61.02 |\n+----+----------+--------------+----------+----------+--------+\n|  2 | +P2      | 72.65        |    71.5  |    71.41 |  71.44 |\n+----+----------+--------------+----------+----------+--------+\n|  3 | +P1+P2   | [BOLD] 73.63 |    71.99 |    71.87 |  72.88 |\n+----+----------+--------------+----------+----------+--------+",
        "label": "refutes"
    },
    {
        "claim": "We observe that for the NYT10 dataset, m = 4 gives the highest F1 score.",
        "evidence": "We evaluate the F1 score of our system for the NYT10 and NYT11 datasets in Table. We find that for the NYT10 dataset, m = 4 gives the highest F1 score. This is because multiple annotators have labeled the same documents as before M = 4. These results show that our system can do well on multiple annotators. Also, we find that the performance of our system improves as the number of annotators increases.",
        "table": "+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|    |   [ITALIC] m |   NYT10 Prec. |   NYT10 Rec. | NYT10 F1     |   NYT11 Prec. |   NYT11 Rec. | NYT11 F1     |\n+====+==============+===============+==============+==============+===============+==============+==============+\n|  0 |            1 |         0.541 |        0.595 | [BOLD] 0.566 |         0.495 |        0.621 | 0.551        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  1 |            2 |         0.521 |        0.597 | 0.556        |         0.482 |        0.656 | 0.555        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  2 |            3 |         0.49  |        0.617 | 0.547        |         0.509 |        0.633 | 0.564        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  3 |            4 |         0.449 |        0.623 | 0.522        |         0.507 |        0.652 | [BOLD] 0.571 |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  4 |            5 |         0.467 |        0.609 | 0.529        |         0.488 |        0.677 | 0.567        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency.",
        "evidence": "S5SS1SSS0Px4 Results From the table, we can see that the systems trained on the original data or with cleaned added slots clearly perform worse than the systems trained on the original data.",
        "table": "+----+------------------------+--------------+---------------+----------------+----------------+\n|    | [BOLD] Training data   |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] Disfl |\n+====+========================+==============+===============+================+================+\n|  0 | Original               |            0 |            22 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  1 | Cleaned added          |            0 |            23 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  2 | Cleaned missing        |            0 |             1 |              0 |              2 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  3 | Cleaned                |            0 |             0 |              0 |              5 |\n+----+------------------------+--------------+---------------+----------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "The Word2Vec embeddings appear to perform better than our method on the random test, although we suspect that the difference is marginal.",
        "evidence": "From the table we can see that the Word2Vec embeddings appear to perform better than our method on the random test, although the difference is marginal. Note that the dataset is relatively small in terms of the number of questions.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Although SFN requires a large portion of training data to achieve superior performance, we find that combining large amounts of multi-action parallel data can significantly improve the model’s performance.",
        "evidence": "Although we find that our model does not require a large portion of training data to achieve superior performance, we find that combining large amounts of multi-action parallel data can significantly improve the model’s performance.. Specifically, in Table 7, we show the results of applying our multi-action data augmentation on top of the otherwise baseline seq2seq model. [CONTINUE] Moreover, [CONTINUE] even without using any model-generated data, our augmented models perform the best, indicating the potential of our augmentation pipeline.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold increases recall and decreases precision (differences are statistically significant with t-test, p < 0.05).",
        "evidence": "This is a strong signal that the cosinebased thresholding algorithm is more amenable to similarity measures. In particular, the results show that it increases recall and decreases precision at the same time. For the Gigaword dataset, the differences are statistically significant with t-test, p < 0.05, but at the NOW dataset the differences are only significant with t-test, p < 0.01.",
        "table": "+----+-----------+--------------------+--------------------+-----------------+-------------+\n|    | [EMPTY]   | [BOLD] Algorithm   |   [BOLD] Precision |   [BOLD] Recall | [BOLD] F1   |\n+====+===========+====================+====================+=================+=============+\n|  0 | Giga      | Baseline           |               0.19 |            0.51 | 0.28        |\n+----+-----------+--------------------+--------------------+-----------------+-------------+\n|  1 | Giga      | Threshold          |               0.46 |            0.41 | [BOLD] 0.41 |\n+----+-----------+--------------------+--------------------+-----------------+-------------+\n|  2 | NOW       | Baseline           |               0.26 |            0.53 | 0.34        |\n+----+-----------+--------------------+--------------------+-----------------+-------------+\n|  3 | NOW       | Threshold          |               0.42 |            0.41 | [BOLD] 0.41 |\n+----+-----------+--------------------+--------------------+-----------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Overall, all of the implementations can improve the performances of base models.",
        "evidence": "Our experiments showed that adding the adversarial loss (P1) can improve the performance of the base models significantly. We leave the results of the implementations for future work.",
        "table": "+----+---------+------------+-------+------------------+------------------+------------------+------------------+--------------+\n|    | Model   |   baseline |    QT |   S  [ITALIC] R0 |   S  [ITALIC] R1 |   S  [ITALIC] R2 |   S  [ITALIC] R3 | D            |\n+====+=========+============+=======+==================+==================+==================+==================+==============+\n|  0 | LF      |      57.21 | 58.97 |            67.82 |            71.27 |            72.04 |            72.36 | 72.65        |\n+----+---------+------------+-------+------------------+------------------+------------------+------------------+--------------+\n|  1 | LF +P1  |      61.88 | 62.87 |            69.47 |            72.16 |            72.85 |            73.42 | [BOLD] 73.63 |\n+----+---------+------------+-------+------------------+------------------+------------------+------------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks except WC.",
        "evidence": "The observations are similar to those of Yao et al. The performance of the hybrid model is poor compared to CMOW or CBOW, but that of the hybrid model is only worse than Com. The main reason is that the vocabulary of Com is relatively small compared to the vocabulary of CBOW and thus the character-based model can capture more of the semantic information. As a result, the performance of the hybrid model is only worse when the depth is relatively large than 400.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best.",
        "evidence": "We note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 achieves the best..",
        "table": "+----+----------+--------------+----------+----------+--------+\n|    | Model    | LF           |   HCIAE  |   CoAtt  |   RvA  |\n+====+==========+==============+==========+==========+========+\n|  0 | baseline | 57.21        |    56.98 |    56.46 |  56.74 |\n+----+----------+--------------+----------+----------+--------+\n|  1 | +P1      | 61.88        |    60.12 |    60.27 |  61.02 |\n+----+----------+--------------+----------+----------+--------+\n|  2 | +P2      | 72.65        |    71.5  |    71.41 |  71.44 |\n+----+----------+--------------+----------+----------+--------+\n|  3 | +P1+P2   | [BOLD] 73.63 |    71.99 |    71.87 |  72.88 |\n+----+----------+--------------+----------+----------+--------+",
        "label": "supports"
    },
    {
        "claim": "Despite achieving high performance in the task success, GDPL does not show substantial improvement in inform F1 and match rate over the baselines.",
        "evidence": "We can see that GDPL achieves a higher inform F1 and match rate than the baselines, GDPL-sess also performs better than GDPL-discr. However, GDPL does not achieve high task success, which we attribute to the fact that the baselines can answer the questions very well. For example, ACER and PPO can get a high inform F1 and match rate, but they fail to do so in the majority of cases. Although GDPL achieves some improvement in task success, we believe the baselines are still very strong, and there is still a large gap to human performance.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "In most setups our best case is not better than the former best case.",
        "evidence": "In most setups our system is not better than the former best case. In the translation case, we obtained a 0.15 BLEU improvement in the translation tasks and a 0.43 BLEU improvement in the translation tasks. In the case of the noise-aware alignment, we improved the translation accuracy by 0.18 BLEU points in the translation tasks and by 0.19 BLEU points in the translation tasks.",
        "table": "+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|    | Method                | En→It best   | En→It avg    |   En→It iters | En→De best   | En→De avg    |   En→De iters | En→Fi best   | En→Fi avg    |   En→Fi iters | En→Es best   | En→Es avg    |   En→Es iters |\n+====+=======================+==============+==============+===============+==============+==============+===============+==============+==============+===============+==============+==============+===============+\n|  0 | Artetxe et al., 2018b | [BOLD] 48.53 | 48.13        |           573 | 48.47        | 48.19        |           773 | 33.50        | 32.63        |           988 | 37.60        | 37.33        |           808 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|  1 | Noise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 |           471 | [BOLD] 49.67 | [BOLD] 48.89 |           568 | [BOLD] 33.98 | [BOLD] 33.68 |           502 | [BOLD] 38.40 | [BOLD] 37.79 |           551 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] As we can observe, it seems that clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected.",
        "evidence": "First, as expected, clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment).",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Selective attention mechanisms like sparsemax and especially TVMAX do not reduce repetition, as measured by the REP metric reported in Table 1.",
        "evidence": "As it can be seen, the results show that sparsemax and especially TVMAX do not reduce repetition. [CONTINUE] Moreover, we observe that TVMAX is slightly better than softmax in terms of SPICE and CIDEr, but worse in terms of BLEU4 and the REP metric. Thus, these results clearly indicate that the use of selective attention mechanisms does not degrade the quality of the caption generation.",
        "table": "+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|    | [EMPTY]   | MSCOCO spice   | MSCOCO cider   | MSCOCO rouge [ITALIC] L   | MSCOCO bleu4   | MSCOCO meteor   | MSCOCO rep↓   | Flickr30k spice   | Flickr30k cider   | Flickr30k rouge [ITALIC] L   | Flickr30k bleu4   | Flickr30k meteor   | Flickr30k rep↓   |\n+====+===========+================+================+===========================+================+=================+===============+===================+===================+==============================+===================+====================+==================+\n|  0 | softmax   | 18.4           | 0.967          | 52.9                      | 29.9           | 24.9            | 3.76          | 13.5              | 0.443             | 44.2                         | 19.9              | 19.1               | 6.09             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  1 | sparsemax | [BOLD] 18.9    | [BOLD] 0.990   | [BOLD] 53.5               | [BOLD] 31.5    | [BOLD] 25.3     | 3.69          | [BOLD] 13.7       | [BOLD] 0.444      | [BOLD] 44.3                  | [BOLD] 20.7       | [BOLD] 19.3        | 5.84             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  2 | TVmax     | 18.5           | 0.974          | 53.1                      | 29.9           | 25.1            | [BOLD] 3.17   | 13.3              | 0.438             | 44.2                         | 20.5              | 19.0               | [BOLD] 3.97      |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "From the table, we can see that our JMEE framework does not achieve the best F1 scores for both trigger classification and argument-related subtasks among all the compared methods.",
        "evidence": "From the table, we can see that our JMEE framework does not achieve the best F1 scores for both trigger classification and argument-related subtasks among all the compared methods.",
        "table": "+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|    | [BOLD] Method   | [BOLD] Trigger  [BOLD] Identification (%)   | [BOLD] Trigger  [BOLD] Classification (%)   | [BOLD] Argument  [BOLD] Identification (%)   | [BOLD] Argument  [BOLD] Role (%)   |\n+====+=================+=============================================+=============================================+==============================================+====================================+\n|  0 | [BOLD] Method   | [ITALIC] F1                                 | [ITALIC] F1                                 | [ITALIC] F1                                  | [ITALIC] F1                        |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  1 | Cross-Event     | [EMPTY]                                     | 68.8                                        | 50.3                                         | 44.6                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  2 | JointBeam       | 70.4                                        | 67.5                                        | 56.8                                         | 52.7                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  3 | DMCNN           | 73.5                                        | 69.1                                        | 59.1                                         | 53.5                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  4 | PSL             | [EMPTY]                                     | 69.4                                        | [EMPTY]                                      | [EMPTY]                            |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  5 | JRNN            | 71.9                                        | 69.3                                        | 62.8                                         | 55.4                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  6 | dbRNN           | [EMPTY]                                     | 71.9                                        | 67.7                                         | 58.7                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  7 | [BOLD] JMEE     | [BOLD] 75.9                                 | [BOLD] 73.7                                 | [BOLD] 68.4                                  | [BOLD] 60.3                        |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset.",
        "evidence": "Scaling to E2E. From the results we can see that the SC-LSTM seems to have trouble scaling to the E2E dataset. In the original setup, for both WOMs and SER, TGen was significantly better than SC-LSTM. After cleaning the data, we can see that TGen+ was always better than TGen−. Similar to the experiment on the development set, we can see that the ranking mechanism for TGen is effective for WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "our system also receives the highest rating in 70% of test cases.",
        "evidence": "Table 4 shows the average human rating for Refresh (2.52), compared to the human rating for ExtAbsRL (1.66). Our system also receives the highest rating in 70% of test cases.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] EWC models do not perform as well as uniform ensembling, as evidenced by the fact that in some cases, uniform ensembling outperforms the oracle.",
        "evidence": "Uniform ensembling was expected to perform better than the EWC models. After all, the EWC models were trained by predicting always the oracle label. In the table, we see that Uniform Ensembling even outperforms the oracle in 5 out of 8 cases.",
        "table": "+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|    | [BOLD] Decoder configuration   | [BOLD] es-en  [BOLD] Health   | [BOLD] es-en  [BOLD] Bio   | [BOLD] en-de  [BOLD] News   | [BOLD] en-de  [BOLD] TED   | [BOLD] en-de  [BOLD] IT   |\n+====+================================+===============================+============================+=============================+============================+===========================+\n|  0 | Oracle model                   | 35.9                          | 37.8                       | 37.8                        | 27.0                       | 57.0                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  1 | Uniform                        | 36.0                          | 36.4                       | [BOLD] 38.9                 | 26.0                       | 43.5                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  2 | BI + IS                        | [BOLD] 36.2                   | [BOLD] 38.0                | 38.7                        | [BOLD] 26.1                | [BOLD] 56.4               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "GDPL is better at booking flights and restaurants than finding hotels, even though its SLU precision is comparable to other agents.",
        "evidence": "The claim that GDPL works better than the other methods is interesting. Specifically, the way in which GDPL retrieves data is clearly better than the way PPO or ALDM take the data. In particular, the way in which GDPL retrieves data is much better than PPO or ALDM, but GDPL does not use any target-side VQA data, and thus is not able to maximize the Slu precision like PPO. By contrast, the way in which ACER and GNPL take the VQA data is clearly bad, as GNPL is able to get much better results than ACER and ALDM.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our model outperforms PG-MMR when trained and tested on the Multi-News dataset.",
        "evidence": "Our model outperforms PG-MMR when trained and tested on the Multi-News dataset. This dataset consists of 308 news articles, comprising 1,099 sentences. The scores for ROUGE-1, ROUGE-2, and ROUGE-SU are the same as those for MULTI-NEWS. We use the “first-3” and “first-2” scores from the MULTI-NEWS dataset, which are the summaries of the first sentence. We can see from the table that our model also outperforms PG-MMR, which is a very strong baseline. The other two baselines, CopyTransformer and PG-BRNN, are based on neural networks. They have different network architectures from the same architecture. Our model is also competitive with these baselines.",
        "table": "+----+------------------------------------------+--------------+--------------+---------------+\n|    | [BOLD] Method                            | [BOLD] R-1   | [BOLD] R-2   | [BOLD] R-SU   |\n+====+==========================================+==============+==============+===============+\n|  0 | First-1                                  | 26.83        | 7.25         | 6.46          |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  1 | First-2                                  | 35.99        | 10.17        | 12.06         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  2 | First-3                                  | 39.41        | 11.77        | 14.51         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  3 | LexRank Erkan and Radev ( 2004 )         | 38.27        | 12.70        | 13.20         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  4 | TextRank Mihalcea and Tarau ( 2004 )     | 38.44        | 13.10        | 13.50         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  5 | MMR Carbonell and Goldstein ( 1998 )     | 38.77        | 11.98        | 12.91         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  6 | PG-Original Lebanoff et al. ( 2018 )     | 41.85        | 12.91        | 16.46         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  7 | PG-MMR Lebanoff et al. ( 2018 )          | 40.55        | 12.36        | 15.87         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  8 | PG-BRNN Gehrmann et al. ( 2018 )         | 42.80        | 14.19        | 16.75         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  9 | CopyTransformer Gehrmann et al. ( 2018 ) | [BOLD] 43.57 | 14.03        | 17.37         |\n+----+------------------------------------------+--------------+--------------+---------------+\n| 10 | Hi-MAP (Our Model)                       | 43.47        | [BOLD] 14.89 | [BOLD] 17.41  |\n+----+------------------------------------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet shows much better performance.",
        "evidence": "It can be seen that our model ZSGNet performs much better than other models on the \"Clothing\" and \"Bodyparts\" classes.",
        "table": "+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|    | Method               | Overall      | people       | clothing     | bodyparts    | animals      | vehicles     | instruments   | scene        | other        |\n+====+======================+==============+==============+==============+==============+==============+==============+===============+==============+==============+\n|  0 | QRC - VGG(det)       | 60.21        | 75.08        | 55.9         | 20.27        | 73.36        | 68.95        | 45.68         | 65.27        | 38.8         |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  1 | CITE - VGG(det)      | 61.89        | [BOLD] 75.95 | 58.50        | 30.78        | [BOLD] 77.03 | [BOLD] 79.25 | 48.15         | 58.78        | 43.24        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  2 | ZSGNet - VGG (cls)   | 60.12        | 72.52        | 60.57        | 38.51        | 63.61        | 64.47        | 49.59         | 64.66        | 41.09        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  3 | ZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87        | [BOLD] 66.18 | [BOLD] 45.27 | 73.79        | 71.38        | [BOLD] 58.54  | [BOLD] 66.49 | [BOLD] 45.53 |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The coverage mechanism is not effective in our models.",
        "evidence": "From this table, we can see that the coverage mechanism is not effective in our models. The ablation baselines -linear combination and global node aggregation -both perform worse than DCGCN4, which proves that the hierarchical structure does not work well in our model. Among all the baselines, the global node aggregation performs the worst, which shows the necessity of aggregating node embeddings via graph attention. The comparison between different modules shows that the combination of linear combination and global node alone is the worst case. It indicates that the hierarchical structure is not necessary for the summarization task.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "Similarly, manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably).",
        "evidence": "The Local baseline is run with all Wikipedia features, and the Local-Manual system is similar to the Wiki-Local system but trained using the manual features. The results show that the manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably). The F-measure of the local system is 63.93%, while the F-measures of the local and Wiki-Manual systems are 62.96% and 66.80%, respectively. This means that the manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably)..",
        "table": "+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|    | System                | Accuracy         | Precision       | Recall          | F-Measure        |\n+====+=======================+==================+=================+=================+==================+\n|  0 | Local                 | 63.97%           | 64.27%          | 64.50%          | 63.93%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  1 | Manual                | 64.25%           | [BOLD] 70.84%∗∗ | 48.50%          | 57.11%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  2 | Wiki                  | 67.25%           | 66.51%          | 69.50%          | 67.76%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  3 | Local-Manual          | 65.75%           | 67.96%          | 59.50%          | 62.96%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  4 | Wiki-Local            | 67.40%           | 65.54%          | 68.50%          | 66.80%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  5 | Wiki-Manual           | 67.75%           | 70.38%          | 63.00%          | 65.79%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  6 | [ITALIC] Our Approach | [BOLD] 69.25%∗∗∗ | 68.76%          | [BOLD] 70.50%∗∗ | [BOLD] 69.44%∗∗∗ |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, the slightly increased invalid response percentage [CONTINUE] We also observe our DAMD model outperforms HDSA in both diversity and appropriateness scores.",
        "evidence": "We observe that the perplexity improvement is mainly due to the increased number of valid responses or correct responses. However, the slightly increased invalid response percentage indicates that our model can generate more diverse and appropriate responses. We also observe our DAMD model achieves comparable diversity and appropriateness scores against the HDSA model.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 respectively compared to the scores of 0.01, -0.01 and 0.07 by OD-parse.",
        "evidence": "distribuThe claim-premise of OD is that: (1) OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 compared to the scores of 0.01, -0.01 and 0.07 by OD-parse.. We observe that compared to OD-parse, OD is much more accurate. (2) On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 compared to the scores of 0.01, -0.01 and 0.07 by OD-parse. This is because of the fact that the number of games in the datasets is relatively small compared to the number of sentences in the claim-premise. We also observe that compared to Jensen-Shannon divergence, the change in the predicted score from OD to Jensen-Shannon is more significant. (3) For the \"Video Games\" and \"Pornography\" datasets, OD is much better than Jensen-Shannon divergence. This is because the \"Video Games\" dataset is much smaller than the \"Pornography\" dataset.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] We notice small improvements relative to the baseline showing that self-attention alone does improve the VQA task.",
        "evidence": "We use the ResNet-34 English captioning model trained on the VQA dataset Anderson et al. Using self-attention, we notice small improvements relative to the baseline showing that self-attention alone does improve the VQA task. We use S: 1,2,3 - B:1,2,3 which is the same as the architecture used in Anderson et al.",
        "table": "+----+------------------------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34                         | [BOLD] Eval set %   | [BOLD] #param   |\n+====+==========================================+=====================+=================+\n|  0 | Baseline (No SA)Anderson et al. ( 2018 ) | 55.00               | 0M              |\n+----+------------------------------------------+---------------------+-----------------+\n|  1 | SA (S: 1,2,3 - B: 1)                     | 55.11               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  2 | SA (S: 1,2,3 - B: 2)                     | 55.17               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  3 | [BOLD] SA (S: 1,2,3 - B: 3)              | [BOLD] 55.27        | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "We don’t evaluate RoBERTa on the 100 instance subset of COPA due to its tendency to pick superficial cues.",
        "evidence": "The results only apply to models trained on 100 instances. The table shows that pretrained B-COPA images (B-COPA) slightly outperforms original COPA images (B-COPA 50%). Moreover, fine-tuning B-COPA images with our retrieval approach improves performance on the B-COPA dataset by 1.6 points (2.1 vs. 2.2) on the overall accuracy, and by 1.1 points (1.1 vs. 0.8) on the hard subset. The results are consistent with the claim on the hard COPA dataset. Moreover, B-COPA (B-COPA 50%) outperforms B-COPA (B-COPA 74.7) by 1.0 point (1.1 vs. 0.4) on the overall accuracy. RoBERTa, on the other hand, appears to be better at B-COPA 50% due to its tendency to pick superficial cues. On the overall accuracy, B-COPA (B-COPA 50%) outperforms B-COPA (COPA 74.3) by 1.0 point (1.9 vs. 2.0). Further, there is a larger gap between B-COPA (B-COPA 50%) and B-COPA (COPA 83.9) on the hard subset, which could be due to the small size of B-COPA (500 instances) compared to 74.7% of the original COPA.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the results for the UD representation are comparable to the two others.",
        "evidence": "Results of our experiments. We evaluated the impact of the number of feature maps, the learning rate, the dropout rate and the hyper-parameters on the F1 score of the ud representation. The results are shown in Tab. In particular, we observe that the ud representation performs the best when the number of feature maps is set to 1000, which is the same as the size of the feature maps used by \\newciteConll08. Regarding the hyper-parameters, we also observe that the dropout rate and the learning rate dropout are important for the good performance of the ud representation.",
        "table": "+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|    | [BOLD] Representation   | [BOLD] Hyper parameters Filter size   |   [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func.   |   [BOLD] Hyper parameters L2 Reg. |   [BOLD] Hyper parameters Learning rate |   [BOLD] Hyper parameters Dropout Prob. |   [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values   |\n+====+=========================+=======================================+=============================================+============================================+===================================+=========================================+=========================================+==================================================+==================================================+\n|  0 | CoNLL08                 | 4-5                                   |                                        1000 | Softplus                                   |                           11.5    |                                0.00113  |                                    1    |                                            73.34 | 74.49                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  1 | SB                      | 4-5                                   |                                         806 | Sigmoid                                    |                            0.0813 |                                0.00179  |                                    0.87 |                                            72.83 | [BOLD] 75.05                                     |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  2 | UD v1.3                 | 5                                     |                                         716 | Softplus                                   |                            1.66   |                                0.000963 |                                    1    |                                            68.93 | 69.57                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Lemmatized targets generally do not perform better, with the boost being more pronounced on SimVerb.",
        "evidence": "Dep-W and SimVerb are significantly better captured by lemmatized targets than all other targets. The boost is particularly pronounced on SimVerb, where the lemmatized targets perform worse.",
        "table": "+----+----------------+----------------------+-----------------------+\n|    | Context: w2    | Context: w2 SimLex   | Context: w2 SimVerb   |\n+====+================+======================+=======================+\n|  0 | target         | all                  | V                     |\n+----+----------------+----------------------+-----------------------+\n|  1 | type           | .348                 | .307                  |\n+----+----------------+----------------------+-----------------------+\n|  2 | x + POS        | .350                 | .279                  |\n+----+----------------+----------------------+-----------------------+\n|  3 | lemma          | <bold>.351</bold>    | .400                  |\n+----+----------------+----------------------+-----------------------+\n|  4 | x + POS        | .345                 | <bold>.406</bold>     |\n+----+----------------+----------------------+-----------------------+\n|  5 | * type         | .339                 | .277                  |\n+----+----------------+----------------------+-----------------------+\n|  6 | * type MFit-A  | .385                 | -                     |\n+----+----------------+----------------------+-----------------------+\n|  7 | * type MFit-AR | .439                 | .381                  |\n+----+----------------+----------------------+-----------------------+\n|  8 | Context: dep-W | Context: dep-W       | Context: dep-W        |\n+----+----------------+----------------------+-----------------------+\n|  9 | type           | .362                 | .314                  |\n+----+----------------+----------------------+-----------------------+\n| 10 | x + POS        | .359                 | .287                  |\n+----+----------------+----------------------+-----------------------+\n| 11 | lemma          | <bold>.379</bold>    | .401                  |\n+----+----------------+----------------------+-----------------------+\n| 12 | x + POS        | .366                 | <bold>.431</bold>     |\n+----+----------------+----------------------+-----------------------+\n| 13 | * type         | .376                 | .313                  |\n+----+----------------+----------------------+-----------------------+\n| 14 | * type MFit-AR | .434                 | .418                  |\n+----+----------------+----------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "The hybrid model is not able to repair this deficit, increasing the difference to 8%.",
        "evidence": "The result shows that the hybrid model is not able to repair the deficit. We observe significant degradation of performance, i.e., the CBOW and CMOW models lost 14.6% and 26.5% accuracy points respectively. In comparison, the hybrid model is able to repair the deficit by losing only 4.5% and 1.5% accuracy points.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | STS12       | STS13       | STS14       | STS15       | STS16       |\n+====+===========+=============+=============+=============+=============+=============+\n|  0 | CBOW      | 43.5        | [BOLD] 50.0 | [BOLD] 57.7 | [BOLD] 63.2 | 61.0        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW      | 39.2        | 31.9        | 38.7        | 49.7        | 52.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 49.6 | 46.0        | 55.1        | 62.4        | [BOLD] 62.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +14.6%      | -8%         | -4.5%       | -1.5%       | +1.8%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +26.5%      | +44.2%      | +42.4       | +25.6%      | +19.0%      |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "They are 553 true positives, 48 false positives, and 5 false negatives.",
        "evidence": "The claim-premise matrix has a very high number of false positives, which justifies the call for further study. However, the low overall precision suggests that there are a lot of false negatives in the dataset.",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, the main improvement of SER comes from training on cleaned data with up to 94% error reduction without the ranker and 97% with.11 just cleaning the training data has a much less dramatic effect than using a semantic control mechanism, such as the reranker (4.27% vs. 0.97% SER).",
        "evidence": "S4SS3SSS0Px1 Results: As it can be seen, cleaning the training data has a much dramatic effect than using a semantic control mechanism such as the reranker. From the results, we can see that training on cleaned data can improve the BLEU score by up to 94% error reduction without the ranker and 97% with. However, the main improvement comes from the introduction of the semantic control mechanism, i.e. the drop in SER from 15.94% to 31.51%.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Though the improvement is slim, it is encouraging to continue researching into visual modulation",
        "evidence": "We trained the ResNet-34 visual modulation model with our proposed visual modulation model. We explored different hyper-parameters, namely, S: 3 - B: 3, S: 3 - B: 4, and S: 3 - B: 6. We observed that the improvement is slim. It is encouraging to continue researching into visual modulation.",
        "table": "+----+-----------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34            | [BOLD] Eval set %   | [BOLD] #param   |\n+====+=============================+=====================+=================+\n|  0 | SA (S: 3 - M: 1)            | 55.25               | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  1 | [BOLD] SA (S: 3 - B: 3)     | [BOLD] 55.42        | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  2 | SA (S: 3 - B: 4)            | 55.33               | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  3 | SA (S: 3 - B: 6)            | 55.31               | } 0.082M        |\n+----+-----------------------------+---------------------+-----------------+\n|  4 | SA (S: 3 - B: 1,3,5)        | 55.45               | } 0.245M        |\n+----+-----------------------------+---------------------+-----------------+\n|  5 | [BOLD] SA (S: 3 - B: 2,4,6) | [BOLD] 55.56        | } 0.245M        |\n+----+-----------------------------+---------------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "PPO agent obtains the highest ratio of successful turns, but GDPL outperforms other agents on SLU precision.",
        "evidence": "From the table, we can see that GDPL obtains the highest ratio of successful turns among the agents. It also outperforms other agents on SLU precision.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "Consequently, with an 8% i is substantially more linguistically informed than CBOW.",
        "evidence": "The main reason for this is that the average layer depth of CMOW is significantly deeper than that of CBOW. Consequently, the model trained using CMOW/400 is more language-informed than the average layer depth of H-CBOW/400. H-CBOW and H-CMOW can roughly match the performance of CBOW/784, but the models trained using CMOW/400 are substantially more linguistically informed than H-CBOW/400. This suggests that when the level of the embedding layer is sufficiently deep, the models of both CBOW and CMOW are more language-informed than the ones of H-CBOW and CMOW.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "the Pearson correlation coefficients in Table VI present the above-mentioned results with the cosine similarity scores used to compare two word embeddings.",
        "evidence": " pearson correlation coefficients. The correlation scores for different word embeddings are reported in Table VI. We observe that, let alone a reduction in performance, the obtained results indicate an almost uniform improvement in the correlation values for the proposed word embeddings over the alternatives. Although Word2Sense performed slightly better on some of the datasets, it should be noted that it is trained on a significantly larger corpus. Categories from Roget's thesaurus are groupings of words that are similar in some sense which the original embedding algorithm may fail to capture. These word embeddings are vectorized and the Pearson correlation coefficients are obtained by measuring the cosine similarities between the embeddings. The results show that the proposed word embeddings are not only better than the other alternatives, they are also much better than the random baseline.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Sentiment polarity shifters have a low impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" does not significantly hurt the Opinion Representation phase, and thereby does not lead to incorrect computation of opinion distance.",
        "evidence": "distribuThe results of different polarity shifters and their comparison are shown in Table. [CONTINUE] First, we observe that simply using OD significantly outperforms OD-parse on datasets of \"Video Games\" and \"Pornography\", and thus leads to an incorrect computation of opinion distance. We also observe that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video Games\" and \"Pornography\", does not significantly hurt the prediction phase. [CONTINUE] Using the sentiment polarity shifters has a low impact on the clustering performance of opinion distance. We analyze that the \"Video Games\" and \"Pornography\" datasets contain relatively few sentences with positive sentiment. This is likely to explain why not utilizing the sentiment polarity shifters harms the prediction, as these datasets contain a limited number of sentences with positive sentiment.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Using only one attention head, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme.",
        "evidence": "This claim can be used to conclude that applying a single attention head to the attention mechanism, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme. We see that the effect is greater as we move into the range of 8-head attention heads, and decreases further as we move into the range of 16-head attention heads.",
        "table": "+----+--------------+------------+-------------+----------+\n|    |   # of Heads | Accuracy   |   Val. Loss | Effect   |\n+====+==============+============+=============+==========+\n|  0 |            1 | 89.44%     |      0.2811 | -6.84%   |\n+----+--------------+------------+-------------+----------+\n|  1 |            2 | 91.20%     |      0.2692 | -5.08%   |\n+----+--------------+------------+-------------+----------+\n|  2 |            4 | 93.85%     |      0.2481 | -2.43%   |\n+----+--------------+------------+-------------+----------+\n|  3 |            8 | 96.02%     |      0.2257 | -0.26%   |\n+----+--------------+------------+-------------+----------+\n|  4 |           10 | 96.28%     |      0.2197 | [EMPTY]  |\n+----+--------------+------------+-------------+----------+\n|  5 |           16 | 96.32%     |      0.219  | +0.04    |\n+----+--------------+------------+-------------+----------+",
        "label": "supports"
    },
    {
        "claim": "One reason is that when the reference action sequence is long, the probability of all actions being correct decreases.",
        "evidence": "The results are shown in Table. [CONTINUE] After applying our multi-decoder network, both the action and slot diversity are improved compared to our single-decoder baseline, [CONTINUE] HDSA has the worse performance, which is consistent with the results of [CONTINUE] when the reference action sequence is short, the probability of all actions being correct increases. [CONTINUE] HDSA has the worse performance, which is also consistent with the results of [CONTINUE] When the reference action sequence is long, the probability of all actions being correct decreases.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "As for Success metric, some ambiguous start location can cause low score.",
        "evidence": "The system was evaluated on the “Full” and “Other” classes. The “Full” class is the system trained from scratch, and the “Other” class is the system trained with the typical transfer learning method. We can see that the translation performance is poor for the “Other” class. This can be explained by the fact that the English language is relatively simple, which makes it hard for the system to pick out the correct start location, even with a native speaker. On the other hand, the translation performance is good for the “Messaging” class, which can be attributed to the use of the dot product and the “next” token. This can be attributed to the fact that this location is usually in the middle of the sentence, making it difficult for the system to find the starting sentence.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) does not outperform GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings does not outperform sentence embeddings alone, and does not exceed the upper boundary of BoC feature, in which again demonstrating the lack of competitiveness of BoC feature.",
        "evidence": "We use Wikipedia-PubMed-PMC annotations to evaluate the combination feature of BoC and sentence embeddings alone. As we can see, the combination feature of BoC and sentence embeddings does not outperform sentence embeddings alone, and does not exceed the upper boundary of BOC feature.. demonstrating the lack of competitiveness of BoC feature. We also observe that GloVe embeddings and ANN feature do not outperform BoC feature.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora.",
        "evidence": "We performed an ablation study to investigate the relationship between the number of terms in the corpus and the performance of DocSub. In this case, the number of terms in the corpus was set to 10,000. However, the performance of DocSub is only slightly better when the number of terms in the corpus was greater than 10,000. This may be expected as, the length of documents in the TED talks corpus is significantly longer than that of the Europarl corpus. However, this is only a qualitative observation and it is hard to quantify this definitively. It is important to emphasize, however, that the performance of DocSub is not significantly worse when the number of terms in the corpus is greater than 10,000.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "humans do poorly on hard instances, which requires deeper inference rather than surface cues, and neural language models largely overcome this difficulty.",
        "evidence": "This indicates that humans do not necessarily make the leap to performing well on hard instances. It is possible, for example, that the increase in the training data on Chinese is due to the model being exposed to a larger vocabulary, rather than improved word discrimination.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "SegMatch works slightly better than Audio2vec according to both criteria.",
        "evidence": "We used the estimation script provided by the conll16 task organizers to do a Wilcoxon signed-rank test over the test set. The results show that SegMatch outperforms all the baselines in terms of recall@10 and average rank.",
        "table": "+----+-------------+-----------------+---------------+------------+\n|    | [EMPTY]     | Recall@10 (%)   | Median rank   | RSAimage   |\n+====+=============+=================+===============+============+\n|  0 | VGS         | 27              | 6             | 0.4        |\n+----+-------------+-----------------+---------------+------------+\n|  1 | SegMatch    | [BOLD] 10       | [BOLD] 37     | [BOLD] 0.5 |\n+----+-------------+-----------------+---------------+------------+\n|  2 | Audio2vec-U | 5               | 105           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  3 | Audio2vec-C | 2               | 647           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  4 | Mean MFCC   | 1               | 1,414         | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  5 | Chance      | 0               | 3,955         | 0.0        |\n+----+-------------+-----------------+---------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] However, re-scoring our second submission after replacing these 10 files with the ones from our first submission resulted in a lower score of 67.07%.",
        "evidence": "We submitted two submissions for official evaluation. Our first submission contained all 10 files from our original submission and was scored 60.9% (micro-F1) af [ Continue]. However, re-scoring our second submission after replacing these 10 files with our first submission resulted in a lower score of 67.07%. Since we did not perform any post-processing, the values of these metrics are not available for our second submission.",
        "table": "+----+----------+------------------+-------------------------+\n|    | Run ID   | Official score   | Score with correction   |\n+====+==========+==================+=========================+\n|  0 | ep_1     | 60.29            | 66.76                   |\n+----+----------+------------------+-------------------------+\n|  1 | ep_2     | [BOLD] 60.90     | [BOLD] 67.35            |\n+----+----------+------------------+-------------------------+\n|  2 | ep_3     | 60.61            | 67.07                   |\n+----+----------+------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "All G2S models have [CONTINUE] higher entailment compared to S2S.",
        "evidence": "From the table, we see that the G2S models tend to have, on average, higher entailment than S2S in both the forward and backward direction. We also observe that the results are similar in the reverse direction. In particular, G2S-GAT has the highest gain in entailment compared to S2S in the forward direction. This shows that G2S models can produce more coherent and interesting responses that have higher self-confidence than S2S.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Our model does not outperform PG-MMR when trained and tested on the Multi-News dataset.",
        "evidence": "These numbers show that our model performs better than PG-MMR when trained and tested on the Multi-News dataset. The results also show that PG-MMR has better performance than CopyTransformer and Hi-MAP.",
        "table": "+----+------------------------------------------+--------------+--------------+---------------+\n|    | [BOLD] Method                            | [BOLD] R-1   | [BOLD] R-2   | [BOLD] R-SU   |\n+====+==========================================+==============+==============+===============+\n|  0 | First-1                                  | 26.83        | 7.25         | 6.46          |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  1 | First-2                                  | 35.99        | 10.17        | 12.06         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  2 | First-3                                  | 39.41        | 11.77        | 14.51         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  3 | LexRank Erkan and Radev ( 2004 )         | 38.27        | 12.70        | 13.20         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  4 | TextRank Mihalcea and Tarau ( 2004 )     | 38.44        | 13.10        | 13.50         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  5 | MMR Carbonell and Goldstein ( 1998 )     | 38.77        | 11.98        | 12.91         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  6 | PG-Original Lebanoff et al. ( 2018 )     | 41.85        | 12.91        | 16.46         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  7 | PG-MMR Lebanoff et al. ( 2018 )          | 40.55        | 12.36        | 15.87         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  8 | PG-BRNN Gehrmann et al. ( 2018 )         | 42.80        | 14.19        | 16.75         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  9 | CopyTransformer Gehrmann et al. ( 2018 ) | [BOLD] 43.57 | 14.03        | 17.37         |\n+----+------------------------------------------+--------------+--------------+---------------+\n| 10 | Hi-MAP (Our Model)                       | 43.47        | [BOLD] 14.89 | [BOLD] 17.41  |\n+----+------------------------------------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Compared to CMOW, the hybrid model shows significant differences.",
        "evidence": "We compared our model against the hybrid method proposed by \\citeauthormullenbach2018cross \\shortcitemullenbach2018cross.cmow model which is the state-of-the-art in this setting. The CBOW model is a standard word level combiner with character embedding of 400 dimensions, and the H-MKO is a version of the same word level combiner proposed by \\citeauthoryang2019cmow \\shortciteyang2019cmow. We see that the hybrid model outperforms the CBOW and CMOW methods on all the qualities. On average the CBOW/400 model obtains a improvement of +1.9% in Bshift accuracy, +3.9% in SubjNum accuracy, +1.5% in CoordInv accuracy, +13.3% in Topconst accuracy, +2.1% in somo accuracy and +7.5% in Wc accuracy. The large gains in accuracy over the CBOW baseline mean that the model can produce more precise and robust answers than the CBOW baseline. Moreover, the hybrid model obtains further improvements. As the dimension of the embedding space increases, the improvements of the hybrid model become more obvious. This clearly demonstrates that the hybrid model can generate better answers than the CBOW baseline.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "BI+IS decoding with single-domain trained models does not achieve gains over both the naive uniform approach and over oracle single-domain models.",
        "evidence": "This claim does not support the claim that decoding with single-domain trained models provides significant gains over oracle single-domain models. The differences are particularly large for the EWC-adapted models, which are larger than the UniM-adapted models. This suggests that the EWC-adapted models, which are provided with very large amount of in-domain training data, are dependent on the training data to improve the performance of the translation task.",
        "table": "+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|    | [BOLD] Language pair   | [BOLD] Model type   |   [BOLD] Oracle model |   [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS   |\n+====+========================+=====================+=======================+================================================+================================================+\n|  0 | es-en                  | Unadapted           |                  36.4 |                                           34.7 | 36.6                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  1 | es-en                  | No-reg              |                  36.6 |                                           34.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  2 | es-en                  | EWC                 |                  37   |                                           36.3 | [BOLD] 37.2                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  3 | en-de                  | Unadapted           |                  36.4 |                                           26.8 | 38.8                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  4 | en-de                  | No-reg              |                  41.7 |                                           31.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  5 | en-de                  | EWC                 |                  42.1 |                                           38.6 | [BOLD] 42.0                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The models in the upper portion (1-6) use only dialogue history and turn-level user goals, which are assumed to be error-free.",
        "evidence": "Results and analysis. First, we can see that using our domain-adaptive delexcalization and domain-aware belief span modeling can substantially improve the task performance for seq2seq and MD-sequicity, as compared to the copying mechanism. The combined score is 5.5% higher than the best non-agglutinating model. Second, our domain-adaptive delexcalization and multi-action data augmentation can further improve the performance of seq2seq and MD-sequicity, as compared to the copying mechanism. The results are 4.5% and 5.3% higher than the models based on dialogue history and turn-level user goals, respectively. This shows that the generated sequences can contain more useful information than the dialogue history and turn-level user goals. Third, the multi-action data augmentation achieves the best results for both seq2seq and MD-sequicity. It provides an additional 3.5% improvement in combined score, which suggests that the generated sequences can contain more useful information than the dialogue history and turn-level user goals.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The first subset contains results by our system, second subset contains results by Refresh, and third subset contains results by ExtAbsRL.",
        "evidence": "Our system performs very well on most metrics, while the Refresh model performs poorly. It is unclear why the refresh model seems to be better than our system, however, we hypothesize that it might be subject to the bias in the data. Despite the large difference between the human rating and the results by Refresh, our system still receives a high average rating. Next, we conduct an experiment that combines our system and ExtAbsRL and gets results that are close to the results by Refresh. We notice that the average rating of Refresh is significantly higher than our system, which suggests that the refresh model may be able to mask some of the errors.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Support Vector Machines (SVM) were used as baseline and the results of other proposed methods have been compared with them.",
        "evidence": "We saw that the proposed CNN-LSTMOur-neg-Ant outperformed the baseline results in both precision and recall for positive sentiment. For negative sentiment, the proposed CNN-LSTM-our-neg-Ant made the best results.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 5 shows that uniform ensembling outperforms all oracle models except es-en Bio, especially on general domains.",
        "evidence": "Uniform ensembling improves over the oracle models except es-en Bio, especially on general domains. [CONTINUE] This indicates that the uniform ensembling heuristic is a good middle ground between single and ensemble scenarios.",
        "table": "+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|    | [BOLD] Decoder configuration   | [BOLD] es-en  [BOLD] Health   | [BOLD] es-en  [BOLD] Bio   | [BOLD] en-de  [BOLD] News   | [BOLD] en-de  [BOLD] TED   | [BOLD] en-de  [BOLD] IT   |\n+====+================================+===============================+============================+=============================+============================+===========================+\n|  0 | Oracle model                   | 35.9                          | 36.1                       | 37.8                        | 24.1                       | 39.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  1 | Uniform                        | 33.1                          | 36.4                       | 21.9                        | 18.4                       | 38.9                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  2 | Identity-BI                    | 35.0                          | 36.6                       | 32.7                        | 25.3                       | 42.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  3 | BI                             | 35.9                          | 36.5                       | 38.0                        | 26.1                       | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  4 | IS                             | [BOLD] 36.0                   | 36.8                       | 37.5                        | 25.6                       | 43.3                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  5 | BI + IS                        | [BOLD] 36.0                   | [BOLD] 36.9                | [BOLD] 38.4                 | [BOLD] 26.4                | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results show that it is better to add knowledge as features when the knowledge quality is high than compile them into constraints.",
        "evidence": "The results show that it is better to add knowledge as features when the knowledge quality is high than to compile them into constraints. On the Winograd dataset, adding features significantly improves the precision compared with removing them, with the results of KnowComb being slightly worse than those of Algorithm. On the WinoCoref dataset, adding features slightly improves precision compared to removing them, with the results of KnowComb being slightly better than those of Algorithm.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.",
        "evidence": "We ran an experiment to investigate whether the parameter budget is the same across models. In particular, we vary the parameter budget D in DCGCN(1) and DCGCN(4), fixing the number of GCN layers to either 2 or 3. Results show that, generally, the shallower models are always better than the shallower ones. Especially, DCGCN(3) performs better than DCGCN(2) when the parameter budget is the same. Although the best results for DCGCN(4) are obtained when the parameter budget is the same, DCGCN(4) outperforms DCGCN(1) when the parameter budget is different, suggesting that shallower models may be exploiting model biases to achieve better results.",
        "table": "+----+----------------+-----+-------+-------------+-------------+\n|    | [BOLD] Model   |   D | #P    | B           | C           |\n+====+================+=====+=======+=============+=============+\n|  0 | DCGCN(1)       | 300 | 10.9M | 20.9        | 52.0        |\n+----+----------------+-----+-------+-------------+-------------+\n|  1 | DCGCN(2)       | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3 |\n+----+----------------+-----+-------+-------------+-------------+\n|  2 | DCGCN(2)       | 240 | 11.3M | 22.8        | 52.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  3 | DCGCN(4)       | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  4 | DCGCN(1)       | 420 | 12.6M | 22.2        | 52.4        |\n+----+----------------+-----+-------+-------------+-------------+\n|  5 | DCGCN(2)       | 300 | 12.5M | 23.8        | 53.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  6 | DCGCN(3)       | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1 |\n+----+----------------+-----+-------+-------------+-------------+\n|  7 | DCGCN(2)       | 360 | 14.0M | 24.2        | [BOLD] 54.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  8 | DCGCN(3)       | 300 | 14.0M | [BOLD] 24.4 | 54.2        |\n+----+----------------+-----+-------+-------------+-------------+\n|  9 | DCGCN(2)       | 420 | 15.6M | 24.1        | 53.7        |\n+----+----------------+-----+-------+-------------+-------------+\n| 10 | DCGCN(4)       | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8 |\n+----+----------------+-----+-------+-------------+-------------+\n| 11 | DCGCN(3)       | 420 | 18.6M | 24.5        | 54.6        |\n+----+----------------+-----+-------+-------------+-------------+\n| 12 | DCGCN(4)       | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------+-----+-------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation.",
        "evidence": "We now proceed to analyze the impact of the language modeling loss in our model. Generally, we find that adding the language modeling loss improves accuracy, at the cost of semantic preservation. For example, M2 has similar accuracy as M1: M0: Shen-1 after adding the word modeling loss, but accuracy increases from 23.6 to 49.9 after adding CSC and language modeling loss. However, we only observe a small increase in accuracy after adding the adversarial loss, compared to the 12.8 accuracy of M2. This suggests that the language modeling loss does not only improve syntactic generalization, but also semantic preservation.",
        "table": "+----+--------------------------------+-------+--------------+-------------+-------------+\n|    | [EMPTY]                        |   Acc | Sim          | PP          | GM          |\n+====+================================+=======+==============+=============+=============+\n|  0 | M0: shen-1                     | 0.694 | 0.728        | [BOLD] 22.3 | 8.81        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  1 | M1: M0 [ITALIC] +para          | 0.702 | 0.747        | 23.6        | 11.7        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  2 | M2: M0 [ITALIC] +cyc           | 0.692 | 0.781        | 49.9        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  3 | M3: M0 [ITALIC] +cyc+lang      | 0.698 | 0.754        | 39.2        | 12.0        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  4 | M4: M0 [ITALIC] +cyc+para      | 0.702 | 0.757        | 33.9        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  5 | M5: M0 [ITALIC] +cyc+para+lang | 0.688 | 0.753        | 28.6        | 11.8        |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  6 | M6: M0 [ITALIC] +cyc+2d        | 0.704 | [BOLD] 0.794 | 63.2        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+\n|  7 | M7: M6+ [ITALIC] para+lang     | 0.706 | 0.768        | 49.0        | [BOLD] 12.8 |\n+----+--------------------------------+-------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "BI and IS both individually outperform the oracle for all but IS-News, [CONTINUE] With adaptive decoding, we do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain.",
        "evidence": "The claim here is that each decoder combination can individually outperform the oracle for all but IS-news. We do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain. We simply assume that the best individual decoder combination is suited for the given domain.",
        "table": "+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|    | [BOLD] Decoder configuration   | [BOLD] es-en  [BOLD] Health   | [BOLD] es-en  [BOLD] Bio   | [BOLD] en-de  [BOLD] News   | [BOLD] en-de  [BOLD] TED   | [BOLD] en-de  [BOLD] IT   |\n+====+================================+===============================+============================+=============================+============================+===========================+\n|  0 | Oracle model                   | 35.9                          | 36.1                       | 37.8                        | 24.1                       | 39.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  1 | Uniform                        | 33.1                          | 36.4                       | 21.9                        | 18.4                       | 38.9                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  2 | Identity-BI                    | 35.0                          | 36.6                       | 32.7                        | 25.3                       | 42.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  3 | BI                             | 35.9                          | 36.5                       | 38.0                        | 26.1                       | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  4 | IS                             | [BOLD] 36.0                   | 36.8                       | 37.5                        | 25.6                       | 43.3                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  5 | BI + IS                        | [BOLD] 36.0                   | [BOLD] 36.9                | [BOLD] 38.4                 | [BOLD] 26.4                | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "Despite the models having fewer examples of bigger graphs to learn from, this does not lead to worse performance when handling graphs with higher diameters.",
        "evidence": "First, the evidence shows that our models GGNN and G2S-GAT are able to cope with graphs with varying sizes. For example, when the diameter is 0-7, the models do not see as much improvement in performance when compared to the 0-3 δ case because the source graph is smaller. But as the dimension increases, the models see less examples of smaller graphs to learn from and thus perform better. Another evidence shows that the max-pooling operation in G2S-GAT is able to close the gap between the performance of S2S and G2S-GGNN when the size of the source graph is greater than 0. Again, this result shows that our models GGNN and G2S-GAT are able to cope with increasing the size of graphs in a reasonable way.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "the main challenge of the sentiment classification task is to extract the information from the context.",
        "evidence": "Extractive of the context. The main challenge of the sentiment classification task is to extract the information from the context. It can be seen that sparse corpora like GloVe and word2vec perform poorly as compared to the original embeddings. The main reason is that the original embeddings are based on syntactic information, which contains most of the semantic information for the sentiment task. OIWE-IPG performs the worst as it is inadequate of capturing the semantic information. Spine and Word2Sense are able to improve the result since they are able to extract the semantic information from the context.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Analyzing Table 3, we can observe that all values of precision using the English corpora have higher scores when compared with the Portuguese corpora.",
        "evidence": "First, using the English corpora, we can see that the Portuguese corpora always have a higher F-score than the English corpora. This is very apparent for two reasons. First, the Portuguese corpora are more similar to English in every sense. Second, Portuguese is a more suitable language for cross-lingual learning.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "The contribution of the cue is clear: in particular, the relatively low precision of using the parser introduces more out of scope relations than in-scope.",
        "evidence": "The contribution of the cue is clear. In particular, the relatively low precision of using the parser introduces more out of scope relations than in-scope ones. This is consistent with our intuition.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The results furthermore show that the sdps based on the Stanford Basic (SB) representation do not provide the best performance, followed by the CoNLL08 representation.",
        "evidence": "Results show that the proposed SDPS network significantly outperforms the baseline SDPS network based on the Stanford basic representation. Specifically, we obtained an improvement of the F1-score of 2.27% when using the Sigmoid instead of the softplus features. The experimental results demonstrate that the Sigmoid features provide the best overall performance. We also tried to tune the number of feature maps (number of nodes in the feature map) for the Sigmoid features, but found that the results were not very good.",
        "table": "+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|    | [BOLD] Representation   | [BOLD] Hyper parameters Filter size   |   [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func.   |   [BOLD] Hyper parameters L2 Reg. |   [BOLD] Hyper parameters Learning rate |   [BOLD] Hyper parameters Dropout Prob. |   [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values   |\n+====+=========================+=======================================+=============================================+============================================+===================================+=========================================+=========================================+==================================================+==================================================+\n|  0 | CoNLL08                 | 4-5                                   |                                        1000 | Softplus                                   |                           11.5    |                                0.00113  |                                    1    |                                            73.34 | 74.49                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  1 | SB                      | 4-5                                   |                                         806 | Sigmoid                                    |                            0.0813 |                                0.00179  |                                    0.87 |                                            72.83 | [BOLD] 75.05                                     |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  2 | UD v1.3                 | 5                                     |                                         716 | Softplus                                   |                            1.66   |                                0.000963 |                                    1    |                                            68.93 | 69.57                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] When removing sweat smile and confused accuracy increased,",
        "evidence": "These results show that the accuracy of identifying the different emoji is significantly lower than the accuracy of identifying all 12 emoji categories. The greatest reduction in accuracy occurred for the “mask” emoji, where the accuracy dropped 12.27%. It is reasonable because the mask emoji tends to act as a stand-alone emoji, which is very similar to the frown emoji. The next largest reduction in accuracy occurred for the “Heart” emoji, with a loss of 4.91%. This emoji is very similar to the cry emoji, but without the cry emoji. Finally, the decreases for different emoji categories are similar, with the exception of “Sad” which decreased by 13.04%.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The comparison shows the powerful advantage of LSTM embeddings over the standard word embeddings in capturing word semantics, that is, semantic similarity.",
        "evidence": "The results show that, let alone a reduction, LSTM word embeddings consistently outperform the standard word embeddings in semantic similarity measure. We observe three main reasons: (i) the quality of embeddings is much better than that of word2vec; (ii) LSTM embeddings better capture semantic similarity than standard word embeddings, (iii) we have LSTM to represent semantic information of sentences; and (iv) the overall quality of embeddings is better than that of standard word embeddings in capturing semantics.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "These results demonstrate that NeuralTDabt indeed learns to generate non-extractive summaries and performs better than a regular extractive baseline, which randomly select sentences from the given document.",
        "evidence": "These results show that NeuralTDabt indeed learns to generate non-extractive summaries and performs better than a regular extractive baseline, which randomly select sentences from the given document..",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "by comparing it to extractive baseline NeuralTD, our proposed abstractive model NeuralTDabt exhibits better performance than extractive baseline, improving 0.9 ROUGE-1 points and 0.3 ROUGE-L points.",
        "evidence": "To evaluate the effectiveness of NeuralTD, we compare it to extractive baseline methods in Zhang et al. Zhang et al. Our model achieves 0.9 ROUGE-1 point improvement and 0.3 ROUGE-L point improvement, which indicates that it is capable of generating an informative summary. Similarly, compared with Kedzie et al.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "For Waseem (2016) we see that there is a significant difference in the estimated rates at which tweets are classified as racist across groups, with higher rates for the white group.",
        "evidence": "We see that there is a significant difference in the estimated rates at which tweets are classified as racist across groups. For instance, for the Waseem (2016) dataset, racism and sexism were estimated to occur at significantly higher rates than those in the Piblack and PiWhite groups.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "This can be attributed to the fact that the proposed approach relies on more than one concept words, while GloVe only uses the representation of the top concept word to classify the image.",
        "evidence": "The results show that for all questions, our approach performs better than GloVe. This can be attributed to the fact that our approach relies on the information of more than one concept word. In contrast, GloVe only uses the representation of the top concept word.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "DCGCN model is not able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a higher score of 33.6 by using 2M data and Seq2SeqK achieves an even higher score of 33.8 by using 20M data.",
        "evidence": "We claim that our DCGCN model achieves a BLEU score of 33.2, which is 0.4 points higher than the previous state-of-the-art without external data. Moreover, DCGCN(ensemble) model achieves an even higher score of 35.3, which is significantly higher than the previous state-of-the-art without external data. It should be noted that the other models, including GraphLSTM, require at least more external data to achieve their higher scores. Due to the space limitation, we only show the results of our single sentence-based model in the table.",
        "table": "+----+------------------------------------+-------------------+-------------+\n|    | [BOLD] Model                       | [BOLD] External   | B           |\n+====+====================================+===================+=============+\n|  0 | Seq2SeqK (Konstas et al.,  2017 )  | -                 | 22.0        |\n+----+------------------------------------+-------------------+-------------+\n|  1 | GraphLSTM (Song et al.,  2018 )    | -                 | 23.3        |\n+----+------------------------------------+-------------------+-------------+\n|  2 | GCNSEQ (Damonte and Cohen,  2019 ) | -                 | 24.4        |\n+----+------------------------------------+-------------------+-------------+\n|  3 | DCGCN(single)                      | -                 | 25.9        |\n+----+------------------------------------+-------------------+-------------+\n|  4 | DCGCN(ensemble)                    | -                 | [BOLD] 28.2 |\n+----+------------------------------------+-------------------+-------------+\n|  5 | TSP (Song et al.,  2016 )          | ALL               | 22.4        |\n+----+------------------------------------+-------------------+-------------+\n|  6 | PBMT (Pourdamghani et al.,  2016 ) | ALL               | 26.9        |\n+----+------------------------------------+-------------------+-------------+\n|  7 | Tree2Str (Flanigan et al.,  2016 ) | ALL               | 23.0        |\n+----+------------------------------------+-------------------+-------------+\n|  8 | SNRG (Song et al.,  2017 )         | ALL               | 25.6        |\n+----+------------------------------------+-------------------+-------------+\n|  9 | Seq2SeqK (Konstas et al.,  2017 )  | 0.2M              | 27.4        |\n+----+------------------------------------+-------------------+-------------+\n| 10 | GraphLSTM (Song et al.,  2018 )    | 0.2M              | 28.2        |\n+----+------------------------------------+-------------------+-------------+\n| 11 | DCGCN(single)                      | 0.1M              | 29.0        |\n+----+------------------------------------+-------------------+-------------+\n| 12 | DCGCN(single)                      | 0.2M              | [BOLD] 31.6 |\n+----+------------------------------------+-------------------+-------------+\n| 13 | Seq2SeqK (Konstas et al.,  2017 )  | 2M                | 32.3        |\n+----+------------------------------------+-------------------+-------------+\n| 14 | GraphLSTM (Song et al.,  2018 )    | 2M                | 33.6        |\n+----+------------------------------------+-------------------+-------------+\n| 15 | Seq2SeqK (Konstas et al.,  2017 )  | 20M               | 33.8        |\n+----+------------------------------------+-------------------+-------------+\n| 16 | DCGCN(single)                      | 0.3M              | 33.2        |\n+----+------------------------------------+-------------------+-------------+\n| 17 | DCGCN(ensemble)                    | 0.3M              | [BOLD] 35.3 |\n+----+------------------------------------+-------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "GGP-MBCM performs best in model 1, but is significantly worse than the other policies in models 2 and 3.",
        "evidence": "We used the multi-task learning setup with GP-MBM to run our experiments for 10 epochs. We also compared the performance of GP-MBM against the three policies from the claim section. From the table, we can see that GP-MBM performs best compared to the other policies in Model 1, but is significantly worse than the other policies in Models 2 and 3.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances.",
        "evidence": "In this section, we review the main results of our experiments with CopyA. We can see that the relative frequency of tokens in the alternative predictions is consistent with the results of automatic evaluation. For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances. It is also the token with the highest coverage in 19.7% of COPA training instances.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "supports"
    },
    {
        "claim": "However, the KL divergence between human dialog policy and RL agents policy is quite large, which means the training has more space to improve.",
        "evidence": "This means that the KL divergence between the human dialog policy and RL agents policy is quite large. Therefore, the training has more space to improve.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "we use the sum of the similarities between the question and the two sentences in the passage as a memory initialization",
        "evidence": "Table 4 shows the results for the systems evaluated. [CONTINUE] The results consistently show that our coverage mechanism further improves the performance over the MQAN baseline by a large margin. The resulting ELMo-based system is the best for both in-domain and out-of-domain evaluations.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "For example, a chatbot that generates a confusing or inappropriate response should be assigned a low efficiency score.",
        "evidence": "The main conclusion in light of these results is the need for further analysis on the inter-annotator agreement. For example, a system that generates a confusing or inappropriate response should be assigned a low efficiency score similar to ACER’s. However, the average accuracy of ACER is 55%, which means that any incorrect or inappropriate response contributes less than 1% to the overall score. This is actually not the case for PPO and ALDM, since they both generate highly efficient responses.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "the performance of GloVe and Word2vec remain unchanged if concept words are unseen in the training corpus.",
        "evidence": "It shows that the performance of GloVe and Word2vec almost stay unchanged if the words ‘a’ and ‘b’ are removed from the training corpus. This indicates that the semantic similarity embeddings are effective at associating questions with concepts.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our model does not obtain the best performance on three out of the four datasets.",
        "evidence": "This is because our model does not perform well on three out of the four datasets. The reason is that the hybrid model (i.e., GAN and CatSeq-RL) gives the best performance on two datasets, i.e., KP20k and INSURCE. This is mainly because there is not enough negative samples in the hybrid model. Furthermore, the negative samples in the hybrid model also have the largest impact on the performance. This is because the items in the hybrid model are not enough to influence the prediction results.",
        "table": "+----+-----------+--------------+--------------+--------------+-------------+\n|    | Model     | Inspec       | Krapivin     | NUS          | KP20k       |\n+====+===========+==============+==============+==============+=============+\n|  0 | Catseq    | 0.87803      | 0.781        | 0.82118      | 0.804       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  1 | Catseq-RL | 0.8602       | [BOLD] 0.786 | 0.83         | 0.809       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  2 | GAN       | [BOLD] 0.891 | 0.771        | [BOLD] 0.853 | [BOLD] 0.85 |\n+----+-----------+--------------+--------------+--------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "All other agents outperform our DKRN agent with a large margin.",
        "evidence": "By measuring the success rate, we can see that our DKRN agent has the highest success rate amongst all the agents. The results demonstrate the outstanding performance of our DKRN agent by making use of the large amount of training data we use.",
        "table": "+----+----------------+-------------+--------------+\n|    | System         | Succ. (%)   | Smoothness   |\n+====+================+=============+==============+\n|  0 | Retrieval-Stgy | 54.0        | 2.48         |\n+----+----------------+-------------+--------------+\n|  1 | PMI            | 46.0        | 2.56         |\n+----+----------------+-------------+--------------+\n|  2 | Neural         | 36.0        | 2.50         |\n+----+----------------+-------------+--------------+\n|  3 | Kernel         | 58.0        | 2.48         |\n+----+----------------+-------------+--------------+\n|  4 | DKRN (ours)    | [BOLD] 88.0 | [BOLD] 3.22  |\n+----+----------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "However, this alone cannot improve system-level ROUGE to the level that of the ROUGE-based decoder.",
        "evidence": "This simply shows that adding BERT embeddings to the MLP representation of the question is not enough to make the system-level ROUGE scores better than the state-of-the-art results. Specifically, the table shows that: 1) BERT+MLP with pretrained word embeddings achieves the best performance on both precision and recall. Specifically, the differences between the two ROUGE scores (reg. loss vs. p.reg. loss) are significantly smaller than that of the other two baselines (p.s. −0.03 vs. 0.05 for MLP, and p.s. −0.005 vs. .04 for SimRED). This shows that BERT is superior to the other embeddings. However, a BERT-based decoder still performs better than the MLP-based encoder, which is not surprising all by itself. It has significantly better precision than the two baselines, which are significantly better than the SimRED and MLP baselines. However, this alone cannot improve system-level ROUGE scores to the level that of the rouge-based decoder. For example, while BERT+MLP has significantly better precision than the two baselines (p.s. −0.05, p.s. −0.1 for SimRED), its recall is only slightly better than the SimRED-based decoder (p.s. −0.005) and it has significantly worse quality than the MLP-based encoder (p.s. −0.001, p.s. .206).",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with the ones from our first submission.",
        "evidence": "The system's official score was 60.9% (micro-F1) af [ Continue] Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with our first submission. As mentioned above, our second submission scored 60.9% (micro-F1) af [ Continue] Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with our first submission..",
        "table": "+----+----------+------------------+-------------------------+\n|    | Run ID   | Official score   | Score with correction   |\n+====+==========+==================+=========================+\n|  0 | ep_1     | 60.29            | 66.76                   |\n+----+----------+------------------+-------------------------+\n|  1 | ep_2     | [BOLD] 60.90     | [BOLD] 67.35            |\n+----+----------+------------------+-------------------------+\n|  2 | ep_3     | 60.61            | 67.07                   |\n+----+----------+------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "For German descriptions, The results are 11.05% better on average compared to (Gella et al., 2017) in symmetric mode.",
        "evidence": "For German descriptions, The results are 11.05% better on average compared to (Gella et al., 2017) in symmetric mode. AME also outperforms FME model significantly more often than the other models.",
        "table": "+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|    | [EMPTY]           | Image to Text R@1   | Image to Text R@5   | Image to Text R@10   | Image to Text Mr   | Text to Image R@1   | Text to Image R@5   | Text to Image R@10   | Text to Image Mr   | Alignment   |\n+====+===================+=====================+=====================+======================+====================+=====================+=====================+======================+====================+=============+\n|  0 | [BOLD] symmetric  | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  1 | Parallel gella:17 | 28.2                | 57.7                | 71.3                 | 4                  | 20.9                | 46.9                | 59.3                 | 6                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  2 | Mono              | 34.2                | 67.5                | 79.6                 | 3                  | 26.5                | 54.7                | 66.2                 | 4                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  3 | FME               | 36.8                | 69.4                | 80.8                 | 2                  | 26.6                | 56.2                | 68.5                 | 4                  | 76.81%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  4 | AME               | [BOLD] 39.6         | [BOLD] 72.7         | [BOLD] 82.7          | [BOLD] 2           | [BOLD] 28.9         | [BOLD] 58.0         | [BOLD] 68.7          | [BOLD] 4           | 66.91%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  5 | [BOLD] asymmetric | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  6 | Pivot gella:17    | 28.2                | 61.9                | 73.4                 | 3                  | 22.5                | 49.3                | 61.7                 | 6                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  7 | Parallel gella:17 | 30.2                | 60.4                | 72.8                 | 3                  | 21.8                | 50.5                | 62.3                 | 5                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  8 | Mono              | [BOLD] 42.0         | 72.5                | 83.0                 | 2                  | 29.6                | 58.4                | 69.6                 | 4                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  9 | FME               | 40.5                | 73.3                | 83.4                 | 2                  | 29.6                | 59.2                | [BOLD] 72.1          | 3                  | 76.81%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n| 10 | AME               | 40.5                | [BOLD] 74.3         | [BOLD] 83.4          | [BOLD] 2           | [BOLD] 31.0         | [BOLD] 60.5         | 70.6                 | [BOLD] 3           | 73.10%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The proposed CNN-LSTMOur-neg-Ant does not improve upon the simple CNNLSTM-w/o neg.",
        "evidence": "This shows that the proposed CNN-LSTMour-neg-Ant does not improve upon the simple CNNLSTM-w/o neg. .",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "When using more natural language text as an additional training resource, the models’ performance is improved dramatically, outperforming the previous state-of-the-art by 10 absolute points.",
        "evidence": "Word frequency (WF) is the percentage of words in the training corpus that are covered by the COCO dataset. As shown in the table, when using more natural language text as an additional training resource, the BERT-Large-FT model experiences an absolute improvement of 10 absolute points over the previous state-of-the-art.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] It can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise.",
        "evidence": "We start by analyzing the dialogs data. We can see that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise. We can also see that the reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise. The inform and match scores are very close to each other, indicating that the learned reward function has a robust reward function.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the least distinctive features for a tweet not being labeled as a complaint.",
        "evidence": "Top unigrams and part-of-speech features [CONTINUE] Using unigrams, we can see that one of the most distinctive features of a complaint is the presence of Not, which is a highly distinctive pronoun. Another is that the presence of Good or Great words or emotions are among the least distinctive features. For example, two of the top features are “good, great”, “win”, and “assent”. These features are remarkably lacking in our data, compared to other features like POS tags or unigrams.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "For all these systems, a three-sentence summarisation is required; so we set T=3 in our experiment.",
        "evidence": "To the best of our knowledge, NeuralTD is the first system that can achieve the ROUGE scores without requiring human-written summaries. It outperforms all the existing systems, including the neural network-based system Narayan et al. et al.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 4 shows that LRN has the highest EM/F1 score.",
        "evidence": "From this table, we can see that the proposed LCN-RNN has the best performance over all the other models. Compared with the other RNN-based models, i.e., RNNGram, SRU and ATR, LCN-RNN achieves the best performance in both EM and F1 score. Particularly, we can see that LCN-RNN achieves better performance than GRU and ATR in terms of the F1 score. The results demonstrate that the computational efficiency of LCN-RNN is much higher than other RNN-based models. In particular, we can see that LCN-RNN has great improvement over the other RNN-based models.",
        "table": "+----+---------+-----------+---------------------+----------------------------+\n|    | Model   | #Params   | Base                | +Elmo                      |\n+====+=========+===========+=====================+============================+\n|  0 | rnet*   | -         | 71.1/79.5           | -/-                        |\n+----+---------+-----------+---------------------+----------------------------+\n|  1 | LSTM    | 2.67M     | [BOLD] 70.46/78.98  | 75.17/82.79                |\n+----+---------+-----------+---------------------+----------------------------+\n|  2 | GRU     | 2.31M     | 70.41/ [BOLD] 79.15 | 75.81/83.12                |\n+----+---------+-----------+---------------------+----------------------------+\n|  3 | ATR     | 1.59M     | 69.73/78.70         | 75.06/82.76                |\n+----+---------+-----------+---------------------+----------------------------+\n|  4 | SRU     | 2.44M     | 69.27/78.41         | 74.56/82.50                |\n+----+---------+-----------+---------------------+----------------------------+\n|  5 | LRN     | 2.14M     | 70.11/78.83         | [BOLD] 76.14/ [BOLD] 83.83 |\n+----+---------+-----------+---------------------+----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Note that GloVe is the pre-trained word vectors in the very basic representation.",
        "evidence": "We present the results in Table. It can be seen that the alternative approaches that aim to improve interpretability, have poor performance on the word embeddings. However, our proposed method has comparable performance with the original GloVe embeddings. Our proposed method outperforms GloVe in semantic word analogy task and in overall results, while GloVe performs slightly better in syntactic word analogy task.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models.",
        "evidence": "As can be seen, our single model DCGCN achieves 19.0 and 12.1 BLEU points on the En-De and Encs tasks, respectively, significantly outperforming all the single models.. The results demonstrate the effectiveness of the proposed model ensembles, which are critical for the current competitive results. In particular, DCGCN attains 19.0 and 45.8 BLEU points on English-German and English-Czech, respectively, which are 3.6 and 2.1 BLEU points above the current state of the art.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "The model performs significantly worse when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function.",
        "evidence": "The claim makes use of existing work, which shows that hinge loss can be viewed as a better alternative to cross entropy loss. In a large model, cross entropy loss is better than hinge loss according to AUC@0.05. In a smaller model, the importance of cross entropy loss is more clear. This is indicating that the information injected into the loss function is crucial for the model to work properly.",
        "table": "+----+----------------------------+---------------------+------------------------------+------------------------+\n|    | [BOLD] Model               | [BOLD] Parameters   | [BOLD] Validation AUC@0.05   | [BOLD] Test AUC@0.05   |\n+====+============================+=====================+==============================+========================+\n|  0 | Base                       | 8.0M                | [BOLD] 0.871                 | 0.816                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  1 | 4L SRU → 2L LSTM           | 7.3M                | 0.864                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  2 | 4L SRU → 2L SRU            | 7.8M                | 0.856                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  3 | Flat → hierarchical        | 12.4M               | 0.825                        | 0.559                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  4 | Cross entropy → hinge loss | 8.0M                | 0.765                        | 0.693                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  5 | 6.6M → 1M examples         | 8.0M                | 0.835                        | 0.694                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  6 | 6.6M → 100K examples       | 8.0M                | 0.565                        | 0.417                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  7 | 200 → 100 negatives        | 8.0M                | 0.864                        | 0.647                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  8 | 200 → 10 negatives         | 8.0M                | 0.720                        | 0.412                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+",
        "label": "supports"
    },
    {
        "claim": "the results show that InferSent yields the highest correlation between METEOR and human evaluation, in both ρ and r. However, we see that InferSent has the lowest precision on the “good” summaries and the highest precision on the “bad” summaries",
        "evidence": "This claim can also be used to support or refute our claim that BERT performs better than InferSent. In order to perform this analysis, we test all the sentence pairs in the reference summaries using the correlation measure ρ and ROUGE scores. We then run this measure on all the test summaries, including the good and bad ones. We can see that InferSent yields the highest correlation measure with both METEOR and ROUGE in both ρ and r. However, BERT has the lowest precision on the “good” summaries, suggesting that it might filter some good sentences out of the summaries.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) outperforms ORACLE in all aspects.",
        "evidence": "Table. We observe that: (i) Our model (RA-Trans) directly uses rationale as augmented supervision outperforms all existing methods on all aspects. (ii) Compared to the rationale-agnostic baseline model, our model (RA-SVM) achieves much better performance by explicitly modeling the aspects. This shows the effectiveness of explicitly modeling the aspects. (iii) Our model achieves state-of-the-art performance in all aspects.",
        "table": "+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source            | Target      |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+===================+=============+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer aroma+palate | Beer look   | 74.41 |     74.83 |     74.94 |    72.75 |        76.41 | [BOLD] 79.53 |     80.29 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look+palate  | Beer aroma  | 68.57 |     69.23 |     67.55 |    69.92 |        76.45 | [BOLD] 77.94 |     78.11 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look+aroma   | Beer palate | 63.88 |     67.82 |     65.72 |    74.66 |        73.4  | [BOLD] 75.24 |     75.5  |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "refutes"
    },
    {
        "claim": "When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM.",
        "evidence": "We then evaluated the proposed DCGCN model in another different and more challenging setting. For this, we used exactly the same amount of training data, but increased the size of the unlabeled data to six times as many. As can be seen from the table, DCGCN achieves results that are both significantly better than current state-of-the-art sequence tagging models and are competitive with graphLSTM when trained with the same amount of data. In particular, we observe that DCGCN(ensemble) outperforms GCNSeq by 7.1 BLEU points when using 0.2M sentences, which is an improvement of 1.8 BLEU points over the best performing baseline. For DCGCN(single), the results are 1.6 BLEU points higher than the best performing baseline, but still worse than GCNSeq and GraphLSTM. This is not surprising since our dataset already has almost two times as many unlabeled data as other baselines. Moreover, DCGCN(ensemble) performs better than DCGCN(single) when trained with only 0.1M sentences, which is again about 2 BLEU points less than the best performing baseline.",
        "table": "+----+------------------------------------+-------------------+-------------+\n|    | [BOLD] Model                       | [BOLD] External   | B           |\n+====+====================================+===================+=============+\n|  0 | Seq2SeqK (Konstas et al.,  2017 )  | -                 | 22.0        |\n+----+------------------------------------+-------------------+-------------+\n|  1 | GraphLSTM (Song et al.,  2018 )    | -                 | 23.3        |\n+----+------------------------------------+-------------------+-------------+\n|  2 | GCNSEQ (Damonte and Cohen,  2019 ) | -                 | 24.4        |\n+----+------------------------------------+-------------------+-------------+\n|  3 | DCGCN(single)                      | -                 | 25.9        |\n+----+------------------------------------+-------------------+-------------+\n|  4 | DCGCN(ensemble)                    | -                 | [BOLD] 28.2 |\n+----+------------------------------------+-------------------+-------------+\n|  5 | TSP (Song et al.,  2016 )          | ALL               | 22.4        |\n+----+------------------------------------+-------------------+-------------+\n|  6 | PBMT (Pourdamghani et al.,  2016 ) | ALL               | 26.9        |\n+----+------------------------------------+-------------------+-------------+\n|  7 | Tree2Str (Flanigan et al.,  2016 ) | ALL               | 23.0        |\n+----+------------------------------------+-------------------+-------------+\n|  8 | SNRG (Song et al.,  2017 )         | ALL               | 25.6        |\n+----+------------------------------------+-------------------+-------------+\n|  9 | Seq2SeqK (Konstas et al.,  2017 )  | 0.2M              | 27.4        |\n+----+------------------------------------+-------------------+-------------+\n| 10 | GraphLSTM (Song et al.,  2018 )    | 0.2M              | 28.2        |\n+----+------------------------------------+-------------------+-------------+\n| 11 | DCGCN(single)                      | 0.1M              | 29.0        |\n+----+------------------------------------+-------------------+-------------+\n| 12 | DCGCN(single)                      | 0.2M              | [BOLD] 31.6 |\n+----+------------------------------------+-------------------+-------------+\n| 13 | Seq2SeqK (Konstas et al.,  2017 )  | 2M                | 32.3        |\n+----+------------------------------------+-------------------+-------------+\n| 14 | GraphLSTM (Song et al.,  2018 )    | 2M                | 33.6        |\n+----+------------------------------------+-------------------+-------------+\n| 15 | Seq2SeqK (Konstas et al.,  2017 )  | 20M               | 33.8        |\n+----+------------------------------------+-------------------+-------------+\n| 16 | DCGCN(single)                      | 0.3M              | 33.2        |\n+----+------------------------------------+-------------------+-------------+\n| 17 | DCGCN(ensemble)                    | 0.3M              | [BOLD] 35.3 |\n+----+------------------------------------+-------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "That is, the agent is informative and successful but forgets to ask what type of food users want to order occasionally.",
        "evidence": "The agent is informative and successful but forgets to ask what type of food users want to order occasionally.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide [CONTINUE] margin by our initialization strategy.",
        "evidence": "Firstly, we compare against our initialization strategy, named “Glorot”, which is consistent with the results of \\newcitewieting-16-full. We also compare with the more recent collaborative effort of \\newcitewieting-16-full, called n(0,0.1), which assumes the ground truth is always top. While glorot achieves slightly better results on Bshift and Topconst, cmow's ability to memorize word content is improved by a wide [continue] margin by our initialization strategy.",
        "table": "+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Initialization   | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+==================+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | N(0,0.1)         | 29.7        | 71.5        | 82.0        | 78.5        | 60.1        | 80.5        | 76.3        | 74.7        | [BOLD] 51.3 | 52.5        |\n+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | Glorot           | 31.3        | [BOLD] 72.3 | 81.8        | 78.7        | 59.4        | 81.3        | 76.6        | [BOLD] 74.6 | 50.4        | 57.0        |\n+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Our paper        | [BOLD] 35.1 | 70.8        | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | [BOLD] 82.8 | [BOLD] 79.7 | 74.2        | 50.7        | [BOLD] 72.9 |\n+----+------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model.",
        "evidence": "Table. The results of CLUSTER+KCP and CLUSTER+LEmma indicate that pre-clustering documents to topics is not beneficial. Our joint model performs substantially better than our joint model..",
        "table": "+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|    | [BOLD] Model                     | R       | MUC P   | [ITALIC] F1   | B3 P    | CEAF- [ITALIC] e P   | CoNLL  [ITALIC] F1   |\n+====+==================================+=========+=========+===============+=========+======================+======================+\n|  0 | [BOLD] Baselines                 | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  1 | Cluster+Lemma                    | 75.5    | 79.9    | 73.6          | 85      | 71.7                 | 76.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  2 | CV Cybulska and Vossen ( 2015a ) | -       | 75      | 64            | 78      | -                    | 73                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  3 | KCP Kenyon-Dean et al. ( 2018 )  | 71      | 71      | 69            | 67      | 67                   | 69                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  4 | Cluster+KCP                      | 77.4    | 79.3    | 71.5          | 87.2    | 66.4                 | 73.6                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  5 | [BOLD] Model Variants            | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  6 | Disjoint                         | 80.3    | 83.6    | 75.9          | 86      | 71.9                 | 78.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  7 | Joint                            | 81      | 84.5    | 77.3          | 85.1    | 73.8                 | [BOLD] 79.5          |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "These experiments show that the number of factors giving the best performance may vary depending on the underlying data distribution.",
        "evidence": "These experiments show that the number of factors giving the best performance may vary depending on the underlying data distribution. In practice, we can observe that the best performance may vary between 1 and 5, as the number of factors varies in the data distribution.",
        "table": "+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|    |   [ITALIC] m |   NYT10 Prec. |   NYT10 Rec. | NYT10 F1     |   NYT11 Prec. |   NYT11 Rec. | NYT11 F1     |\n+====+==============+===============+==============+==============+===============+==============+==============+\n|  0 |            1 |         0.541 |        0.595 | [BOLD] 0.566 |         0.495 |        0.621 | 0.551        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  1 |            2 |         0.521 |        0.597 | 0.556        |         0.482 |        0.656 | 0.555        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  2 |            3 |         0.49  |        0.617 | 0.547        |         0.509 |        0.633 | 0.564        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  3 |            4 |         0.449 |        0.623 | 0.522        |         0.507 |        0.652 | [BOLD] 0.571 |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  4 |            5 |         0.467 |        0.609 | 0.529        |         0.488 |        0.677 | 0.567        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The results in Table 3 show that translation quality of LRN is significantly worse than that of GRU (-0.57 BLEU).",
        "evidence": "From the table, we can see that GRU and LRN have similar performance on the training set, while the gap is much larger on the decoding set. Without seeing the source language, we can conclude that the translation quality of RNN is significantly worse than that of GRU. In particular, the BLEU score of RNN is 26.26, which is 19.1 BLEU points worse than that of GRU.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 4 and does not improve at lower layers, with some drops at layers 1 and 2.",
        "evidence": "We see that the prediction task of POS tagging improves at the begin of the network’s training, while the prediction task of semantic tagging remains stable. We hypothesize that this is due to the implicit self-attention mechanism in the self-attention layers that makes the model less likely to make mistakes while making predictions.",
        "table": "+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|    | [ITALIC] k           | Ar                   | Es                   | Fr                   | Ru                   | Zh                   | En                   |\n+====+======================+======================+======================+======================+======================+======================+======================+\n|  0 | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  1 | 0                    | 88.0                 | 87.9                 | 87.9                 | 87.8                 | 87.7                 | 87.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  2 | 1                    | 92.4                 | 91.9                 | 92.1                 | 92.1                 | 91.5                 | 89.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  3 | 2                    | 91.9                 | 91.8                 | 91.8                 | 91.8                 | 91.3                 | 88.3                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  4 | 3                    | 92.0                 | 92.3                 | 92.1                 | 91.6                 | 91.2                 | 87.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  5 | 4                    | 92.1                 | 92.4                 | 92.5                 | 92.0                 | 90.5                 | 86.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  6 | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  7 | 0                    | 81.9                 | 81.9                 | 81.8                 | 81.8                 | 81.8                 | 81.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  8 | 1                    | 87.9                 | 87.7                 | 87.8                 | 87.9                 | 87.7                 | 84.5                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  9 | 2                    | 87.4                 | 87.5                 | 87.4                 | 87.3                 | 87.2                 | 83.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 10 | 3                    | 87.8                 | 87.9                 | 87.9                 | 87.3                 | 87.3                 | 82.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 11 | 4                    | 88.3                 | 88.6                 | 88.4                 | 88.1                 | 87.7                 | 82.1                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 12 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 13 | [EMPTY]              | 32.7                 | 49.1                 | 38.5                 | 34.2                 | 32.1                 | 96.6                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "As shown in Table 8, G2S approaches outperform the S2S baseline.",
        "evidence": "As shown in Table 8, the proposed models significantly outperform the S2S baseline. Moreover, the proposed G2S approaches perform closer to the state-of-the-art S2S approach, the gold standard annotation is not available.",
        "table": "+----+----------------------+----------------------+---------------------+\n|    | <bold>Model</bold>   | <bold>ADDED</bold>   | <bold>MISS</bold>   |\n+====+======================+======================+=====================+\n|  0 | S2S                  | 47.34                | 37.14               |\n+----+----------------------+----------------------+---------------------+\n|  1 | G2S-GIN              | 48.67                | 33.64               |\n+----+----------------------+----------------------+---------------------+\n|  2 | G2S-GAT              | 48.24                | 33.73               |\n+----+----------------------+----------------------+---------------------+\n|  3 | G2S-GGNN             | 48.66                | 34.06               |\n+----+----------------------+----------------------+---------------------+\n|  4 | GOLD                 | 50.77                | 28.35               |\n+----+----------------------+----------------------+---------------------+\n|  5 | [EMPTY]              | [EMPTY]              | [EMPTY]             |\n+----+----------------------+----------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "In analogy 2, all relations are different.",
        "evidence": "We make several observations from the results. First, in absolute terms, the sparse methods perform worse than the original GloVe embeddings. We attribute this to the fact that in analogy 2, the relations are more complete than semantic relations. The results are consistent with the results reported by \\newciteDBLP:journals/corr/abs-1910-10683 for the same data. Second, the sparse methods that aim at integrating syntactic knowledge, i.e., GloVe and word2vec, do not achieve much improvement in analogy 2. We attribute this to the fact that the syntactic relations are not well integrated into the word embeddings. For example, in analogy 2, the absolute accuracy of GloVe is 78.94% and accuracies of word2vec and OIWE-IPG are 23.44% and 21.84%, respectively. In contrast, our proposed method achieves 79.96% and 63.52% in accuracy. It demonstrates that our proposed method can integrate syntactic knowledge into word embeddings.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Selective attention mechanisms like sparsemax and especially TVMAX reduce repetition, as measured by the REP metric reported in Table 1.",
        "evidence": "As can be seen, sparsemax and especially TVMAX reduce repetition significantly, as measured by the REP metric reported in Table 1. [CONTINUE] Moreover, the results of sparsemax are slightly better than softmax in terms of BLEU4. [CONTINUE] Selective attention mechanisms like sparsemax and especially TVMAX reduce repetition significantly.",
        "table": "+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|    | [EMPTY]   | MSCOCO spice   | MSCOCO cider   | MSCOCO rouge [ITALIC] L   | MSCOCO bleu4   | MSCOCO meteor   | MSCOCO rep↓   | Flickr30k spice   | Flickr30k cider   | Flickr30k rouge [ITALIC] L   | Flickr30k bleu4   | Flickr30k meteor   | Flickr30k rep↓   |\n+====+===========+================+================+===========================+================+=================+===============+===================+===================+==============================+===================+====================+==================+\n|  0 | softmax   | 18.4           | 0.967          | 52.9                      | 29.9           | 24.9            | 3.76          | 13.5              | 0.443             | 44.2                         | 19.9              | 19.1               | 6.09             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  1 | sparsemax | [BOLD] 18.9    | [BOLD] 0.990   | [BOLD] 53.5               | [BOLD] 31.5    | [BOLD] 25.3     | 3.69          | [BOLD] 13.7       | [BOLD] 0.444      | [BOLD] 44.3                  | [BOLD] 20.7       | [BOLD] 19.3        | 5.84             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  2 | TVmax     | 18.5           | 0.974          | 53.1                      | 29.9           | 25.1            | [BOLD] 3.17   | 13.3              | 0.438             | 44.2                         | 20.5              | 19.0               | [BOLD] 3.97      |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB.",
        "evidence": "As shown in the table, the proposed DCGCN model achieves much better performance than the other models, including the ensemble model of Seq2SeqB. In particular, on AMR17, we improve the best single model of Seq2Seq by 1.9 BLEU points. Due to the large number of possible corrections, the ensemble model only performs well on B.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "On the same dataset, we have competitive results to Damonte and Cohen (2019).",
        "evidence": "We see that the proposed G2S-GAT has better performance than S2S and G2S-GIN over different datasets and experimental conditions. For example, on LDC2015E86, G2S-GGNN beats S2S by about 0.53 and G2S-GAT by about 0.35. Likewise, on LDC2017T10, G2S-GGNN beats S2S by about 0.18 and G2S-GAT by about 0.33. These results indicate that our approach is effective for the generation of fake news.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "The word analogy test was first introduced in [32] to assess the quality of word vectors.",
        "evidence": "We present the results of the comparative analysis of word vectors in Table. It can be seen that the alternative approaches that aim to improve interpretability, have poor performance on the word analogy tests. However, our proposed method has comparable performance with the original GloVe embeddings. Our proposed method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Adding either the global node or the linear combination does not improve the baseline models with only dense connections.",
        "evidence": "Baseline Models We train the proposed model variants on the training set of the DCGCN4 proposal. At first glance, the contribution of the global node seems less than the linear combination. However, we argue that this is not the case. The linear combination is better than simply adding the linear node on top of the graph attention, but this is not the case. The best choice of global node is indeed B, which is indeed higher than the linear combination. This suggests that the global node and the linear combination are complementary to each other. We also tried to include the coverage mechanism in the decoder modules, but it did not help.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] In addition, other words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are also distinctive for tweets that are not complaints.",
        "evidence": "Top unigrams and part-of-speech features for complaints are shown in Table [CONTINUE] However, other words and clusters expressing positive states such as gratitude (thank, great, love) are also distinctive for tweets that are not complaints. [CONTINUE] For example, the presence of terms that show gratitude, great, win and laughter are two of the most distinctive features for tweets that are not complaints. [CONTINUE] Several unigrams and part-of-speech features for complaints are also shown in Table [CONTINUE] However, there are several unigrams and part-of-speech features for which these features are not distinctive. [CONTINUE] Complaints use more possessive pronouns, indicating that the user is describing personal experiences. [CONTINUE] A distinctive part-of-speech pattern common in complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items or services possessed by the complainer (e.g., my account, my order). [CONTINUE] Question marks are distinctive of complaints, as many complaints are formulated as questions to the responsible party (e.g., why is this not working?, when will [CONTINUE] get my response?). [CONTINUE] Mentions of time are specific of complaints (been, still, on, days, Temporal References cluster). [CONTINUE] In addition, the presence of verbs in past participle (VBN) is the most distinctive part-of-speech pattern of tweets not being complaints. These are used to describe actions completed in the past (e.g., i've bought, have come) in order to provide context for the complaint. [CONTINUE] Several part-of-speech patterns distinctive of complaints involve present verbs in third person singular (VBZ). [CONTINUE] Verbs in gerund or present participle are used as a complaint strategy",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The first one models the agenda state space as discrete and predefined, while the other agent encodes a stochastic latent space for agenda representation.",
        "evidence": "We compare GDPL with its variants, including GDPL-sess, GDPL-discr, and GDPL, on the task of representing an agenda state into a discrete and predefined latent space. GDPL achieves the best performance in Inform F1 and Match rate, and its task success is the second highest. GDPL also outperforms GDPL-discr and GDPL-sess on the task of success rate. All three variants of GDPL outperform human, though GDPL does not outperform human in most metrics. This suggests that GDPL is able to model the latent space more effective than the other methods.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that predictive performance is relatively consistent across all domains with two exceptions ('Food & Beverage' consistently shows lower performance, while 'Other' achieves higher performance) when using all the data available from the other domains.",
        "evidence": "We assume that our training data is relatively robust to some domain variations. For example, there is very little difference in the accuracy of the apparel and food & Beverage domains, with the exception of the Software domain. This is partially explained by the fact that these two domains are the most similar to each other (e.g. each domain has their own best performing feature) whereas the other domains are very general in how they perform.",
        "table": "+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|    | [BOLD] Test   | F&B     | A       | R       | Ca      | Se      | So      | T       | E       | O       |\n+====+===============+=========+=========+=========+=========+=========+=========+=========+=========+=========+\n|  0 | [BOLD] Train  | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  1 | Food & Bev.   | –       | 58.1    | 52.5    | 66.4    | 59.7    | 58.9    | 54.1    | 61.4    | 53.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  2 | Apparel       | 63.9    | –       | 74.4    | 65.1    | 70.8    | 71.2    | 68.5    | 76.9    | 85.6    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  3 | Retail        | 58.8    | 74.4    | –       | 70.1    | 72.6    | 69.9    | 68.7    | 69.6    | 82.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  4 | Cars          | 68.7    | 61.1    | 65.1    | –       | 58.8    | 67.     | 59.3    | 62.9    | 68.2    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  5 | Services      | 65.     | 74.2    | 75.8    | 74.     | –       | 68.8    | 74.2    | 77.9    | 77.9    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  6 | Software      | 62.     | 74.2    | 68.     | 67.9    | 72.8    | –       | 72.8    | 72.1    | 80.6    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  7 | Transport     | 59.3    | 71.7    | 72.4    | 67.     | 74.6    | 75.     | –       | 72.6    | 81.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  8 | Electronics   | 61.6    | 75.2    | 71.     | 68.     | 75.     | 69.9    | 68.2    | –       | 78.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  9 | Other         | 56.1    | 71.3    | 72.4    | 70.2    | 73.5    | 67.2    | 68.5    | 71.     | –       |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n| 10 | All           | 70.3    | 77.7    | 79.5    | 82.0    | 79.6    | 80.1    | 76.8    | 81.7    | 88.2    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "Such case is the most difficult task for this model.",
        "evidence": "Such case is the most difficult task for this model as the difference between the in-scope and out-ofscope labels is thus much greater than that between the in-scope and out-ofscope labels.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that predictive performance is not consistent across all domains, with 'Food & Beverage' consistently showing lower performance and 'Other' achieving higher performance when using all the data available from the other domains.",
        "evidence": "We compared our model’s performance to several baselines using training data from each domain separately. First, we observed that the performance of our model was very similar across all domains, with the exception of food & Beverage showing lower performance on average. We believe that this can be explained by the fact that the products in the food & Beverage domain are high-margin and the training data is relatively small, making it difficult for the model to predict these features from the other domains. Second, we observed that predictive performance is not consistent across all domains, with 'Food & Beverage' consistently showing higher performance when using all the data available from the other domains.. This shows that our model could potentially be applied to other domains, although we leave this analysis to future work.",
        "table": "+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|    | [BOLD] Test   | F&B     | A       | R       | Ca      | Se      | So      | T       | E       | O       |\n+====+===============+=========+=========+=========+=========+=========+=========+=========+=========+=========+\n|  0 | [BOLD] Train  | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  1 | Food & Bev.   | –       | 58.1    | 52.5    | 66.4    | 59.7    | 58.9    | 54.1    | 61.4    | 53.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  2 | Apparel       | 63.9    | –       | 74.4    | 65.1    | 70.8    | 71.2    | 68.5    | 76.9    | 85.6    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  3 | Retail        | 58.8    | 74.4    | –       | 70.1    | 72.6    | 69.9    | 68.7    | 69.6    | 82.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  4 | Cars          | 68.7    | 61.1    | 65.1    | –       | 58.8    | 67.     | 59.3    | 62.9    | 68.2    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  5 | Services      | 65.     | 74.2    | 75.8    | 74.     | –       | 68.8    | 74.2    | 77.9    | 77.9    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  6 | Software      | 62.     | 74.2    | 68.     | 67.9    | 72.8    | –       | 72.8    | 72.1    | 80.6    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  7 | Transport     | 59.3    | 71.7    | 72.4    | 67.     | 74.6    | 75.     | –       | 72.6    | 81.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  8 | Electronics   | 61.6    | 75.2    | 71.     | 68.     | 75.     | 69.9    | 68.2    | –       | 78.7    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|  9 | Other         | 56.1    | 71.3    | 72.4    | 70.2    | 73.5    | 67.2    | 68.5    | 71.     | –       |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n| 10 | All           | 70.3    | 77.7    | 79.5    | 82.0    | 79.6    | 80.1    | 76.8    | 81.7    | 88.2    |\n+----+---------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "On the WinoCoref dataset, it improves by 15%.",
        "evidence": "Performance on Winograd is worse than on WinoCoref. A reason for this is that Winograd has very coarse-grained entity annotations. Adding more fine-grained entity types only improves the precision but degrades the recall. This is the main reason why we did not get better performance on KnowComb and KnowFeat compared to Rahman and Ng \\shortciteRahman2012resolving. First, they use unlabeled data, which is not appropriate for scientific domain adaptation. Second, they rely on entity types that are part of pre-defined knowledge chains, which are not general enough for end-to-end learning.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "These results confirm that simultaneously learning the tasks enhances the performance of a DPP model.",
        "evidence": "The results presented in Table 3 show that simultaneously learning the tasks enhances the performance of a DPP model. Our joint model is significantly better than the [CONTINUE] Table 3 shows that simultaneously learning the tasks enhances the performance of a DPP model.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 50-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al.",
        "evidence": "We submitted two runs of our experiments. In the first, we used a slightly larger BiLSTM and increased the number of hidden units in the network by one. In the second, we used 50-dimensional part-ofspeech embeddings and also tried various optimization strategies, including SGD with different learning rates and a stepwise learning rate schedule as proposed by Conneau et al. We observed that using more hidden units did not lead to better results, and that the best results were achieved using 50-dimensional POS embeddings. Regarding optimization strategies, we also tried using a stepwise learning rate schedule as described by Conneau et al. However, we did not observe further improvements with this approach.",
        "table": "+----+--------------------+-----------------------+-------------+\n|    | [BOLD] Variation   | [BOLD] Accuracy (%)   | [BOLD] Δ%   |\n+====+====================+=======================+=============+\n|  0 | Submitted          | [BOLD] 69.23          | -           |\n+----+--------------------+-----------------------+-------------+\n|  1 | No emoji           | 68.36                 | - 0.87      |\n+----+--------------------+-----------------------+-------------+\n|  2 | No ELMo            | 65.52                 | - 3.71      |\n+----+--------------------+-----------------------+-------------+\n|  3 | Concat Pooling     | 68.47                 | - 0.76      |\n+----+--------------------+-----------------------+-------------+\n|  4 | LSTM hidden=4096   | 69.10                 | - 0.13      |\n+----+--------------------+-----------------------+-------------+\n|  5 | LSTM hidden=1024   | 68.93                 | - 0.30      |\n+----+--------------------+-----------------------+-------------+\n|  6 | LSTM hidden=512    | 68.43                 | - 0.80      |\n+----+--------------------+-----------------------+-------------+\n|  7 | POS emb dim=100    | 68.99                 | - 0.24      |\n+----+--------------------+-----------------------+-------------+\n|  8 | POS emb dim=75     | 68.61                 | - 0.62      |\n+----+--------------------+-----------------------+-------------+\n|  9 | POS emb dim=50     | 69.33                 | + 0.10      |\n+----+--------------------+-----------------------+-------------+\n| 10 | POS emb dim=25     | 69.21                 | - 0.02      |\n+----+--------------------+-----------------------+-------------+\n| 11 | SGD optim lr=1     | 64.33                 | - 4.90      |\n+----+--------------------+-----------------------+-------------+\n| 12 | SGD optim lr=0.1   | 66.11                 | - 3.12      |\n+----+--------------------+-----------------------+-------------+\n| 13 | SGD optim lr=0.01  | 60.72                 | - 8.51      |\n+----+--------------------+-----------------------+-------------+\n| 14 | SGD optim lr=0.001 | 30.49                 | - 38.74     |\n+----+--------------------+-----------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Each extractive summaries of a subset is rated by three annotators who are asked to rank the summaries based on the following criteria: structure, meaning preservation, and relevance, on a 1-5 Likert scale",
        "evidence": "Each extractive summary is assessed by three experienced annotators. We asked the annotators to rank the summaries based on the following criteria: structure, meaning preservation, and relevance, on a 1-5 Likert scale. While the results for Refresh are not comparable to the results on the full dataset, we find that the extractive summaries generated by ExtAbsRL generally contain more salient information, and are more fluent and concise. The reference summaries all fall short on meaning preservation, reflecting that the meaning of the original claim does not necessarily match the ground truth.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Its productivity of 57.5% expresses that it appears in incorrect alternatives 7.5% more often than expected by random chance.",
        "evidence": "The productivity of the incorrect alternative is 57.5%. This corresponds to that it appears in incorrect alternatives 7.5% more often than expected by random chance.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "refutes"
    },
    {
        "claim": "In both cases, the original embeddings perform better than the new ones.",
        "evidence": "We tested the new embeddings in the Italian and German datasets for semantic similarity against the original embeddings. In both cases, the original embeddings were found to be significantly better than the new ones. In particular, for German, they turned out to be better than the original embeddings. Interestingly, the English results for the German dataset are similar to the results for the Italian one. This suggests that the new embeddings do not solely improve the readability of the original embeddings, but also to have the same effect as the original embeddings in case of similarity.",
        "table": "+----+-----------+----------------+------------------+---------------+-----------------+\n|    | [EMPTY]   |   Italian Orig | Italian Debias   |   German Orig | German Debias   |\n+====+===========+================+==================+===============+=================+\n|  0 | SimLex    |          0.28  | [BOLD] 0.288     |         0.343 | [BOLD] 0.356    |\n+----+-----------+----------------+------------------+---------------+-----------------+\n|  1 | WordSim   |          0.548 | [BOLD] 0.577     |         0.547 | [BOLD] 0.553    |\n+----+-----------+----------------+------------------+---------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "We can also observe that the combination of learned reward and coverage penalty in our system further boosts the performance of NeuralTD with learned rewards, relative to using normal ROUGE or the learned reward only",
        "evidence": "Table. Additionally, we can also find that the combination of the learned reward and coverage penalty in our system further boosts the performance of NeuralTD with normal ROUGE and the learned reward only.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence.",
        "evidence": "Since our model is only designed for MTR, its performance with respect to BLEU-4 is not affected by the number of words retrieved from the system. Nevertheless, it appears that our model generates longer sentences whose are comparable with human sentences, both with about 22 words per sentence. Moreover, retrieval results of our models are better than those of retrieval-based models. Comparing our models with human models, we can see that retrieval-based models (e.g., Seq2Seq and Seq2Seqaug) generate longer sentences than corresponding MTR-based models, even though the latter has a slightly better performance with respect to BLEU-4. Since our model is built solely based on MTR, it is more robust to the number of words retrieved from the system.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Some of our bidirectional models obtain 92-93% accuracy.",
        "evidence": "From this table, we can see that the best tagger for each model is a 3-gram model with an accuracy of 91.9% on Uni and 92.8% on Bi, and a 1-gram model with an accuracy of 92.3% on Res.",
        "table": "+----+-------+-------+----------+----------+----------+----------+----------+\n|    | Uni   | POS   |   0 87.9 |   1 92.0 |   2 91.7 |   3 91.8 |   4 91.9 |\n+====+=======+=======+==========+==========+==========+==========+==========+\n|  0 | Uni   | SEM   |     81.8 |     87.8 |     87.4 |     87.6 |     88.2 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  1 | Bi    | POS   |     87.9 |     93.3 |     92.9 |     93.2 |     92.8 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  2 | Bi    | SEM   |     81.9 |     91.3 |     90.8 |     91.9 |     91.9 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  3 | Res   | POS   |     87.9 |     92.5 |     91.9 |     92   |     92.4 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  4 | Res   | SEM   |     81.9 |     88.2 |     87.5 |     87.6 |     88.5 |\n+----+-------+-------+----------+----------+----------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "our model achieved the best results in terms of appropriateness and diversity.",
        "evidence": "From the table, we can see that our model outperforms HDSA in terms of appropriateness. Our model is also better in terms of diversity, indicating that our model can generate more diverse responses. Compared with HDSA, our model gets better results on both appropriateness and diversity.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "Table II shows that Nepal and Macedonia are roughly balanced, while Kerala is imbalanced.",
        "evidence": "The same table shows the size of each of the datasets. It is clear that Nepal has the largest amount of messages with 6,063 messages, followed by Macedonia with 205 messages. Both of these datasets are balanced in terms of number of messages. However, Kerala has the highest number of messages, with 2,046 messages. This shows that Kerala is imbalanced with messages from urgent and non-urgent messages. The length of messages in Macedonia and Nepal is similar, as well. Since we are looking at the messages that contain urgent and non-urgent messages, we can’t use Macedonia and Nepal for our comparison.",
        "table": "+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+\n|    | Dataset   | Unlabeled / Labeled Messages   | Urgent / Non-urgent Messages   | Unique Tokens   |   Avg. Tokens / Message | Time Range            |\n+====+===========+================================+================================+=================+=========================+=======================+\n|  0 | Nepal     | 6,063/400                      | 201/199                        | 1,641           |                      14 | 04/05/2015-05/06/2015 |\n+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+\n|  1 | Macedonia | 0/205                          | 92/113                         | 129             |                      18 | 09/18/2018-09/21/2018 |\n+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+\n|  2 | Kerala    | 92,046/400                     | 125/275                        | 19,393          |                      15 | 08/17/2018-08/22/2018 |\n+----+-----------+--------------------------------+--------------------------------+-----------------+-------------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied.",
        "evidence": "According to this table, although GRU and LRSN outperform the LSTM by 0.3∼0.9 in terms of accuracy, they sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, it does not improve significantly over \"ranking\".",
        "evidence": "We can see that (1) with the human evaluation, the performances of \"ranking\" and \"top-pairs\" are similar to each other, while the performances of \"linguistic\" are slightly better than \"ranking\". (2) It can be seen that with the help oflinguistic, the performance of \"ranking\" is improved significantly. However, the improvement over \"ranking\" is not very large, which means that the \"ranking\" data is already very good.",
        "table": "+----+-------------+-------+-----------------------+--------------------------+---------+-------+\n|    | [EMPTY]     |   MUC |   <italic>B</italic>3 |   CEAF<italic>e</italic> |   CoNLL |   LEA |\n+====+=============+=======+=======================+==========================+=========+=======+\n|  0 | ranking     | 74.31 |                 64.23 |                    59.73 |   66.09 | 60.47 |\n+----+-------------+-------+-----------------------+--------------------------+---------+-------+\n|  1 | +linguistic | 74.35 |                 63.96 |                    60.19 |   66.17 | 60.2  |\n+----+-------------+-------+-----------------------+--------------------------+---------+-------+\n|  2 | top-pairs   | 73.95 |                 63.98 |                    59.52 |   65.82 | 60.07 |\n+----+-------------+-------+-----------------------+--------------------------+---------+-------+\n|  3 | +linguistic | 74.32 |                 64.45 |                    60.19 |   66.32 | 60.62 |\n+----+-------------+-------+-----------------------+--------------------------+---------+-------+",
        "label": "supports"
    },
    {
        "claim": "RANDOM is the best performing baseline here, and other baselines are far from gender-parity.",
        "evidence": "We can see that Random is the best baseline here, and other baselines are far from gender-parity. The best performing baseline here is Parallelism, which outperforms other baselines by a wide margin. This is coherent with the results from the linguistic evaluation.",
        "table": "+----+--------------------+-------------+-------------+-----------------------+-------------+\n|    | [EMPTY]            | M           | F           | B                     | O           |\n+====+====================+=============+=============+=======================+=============+\n|  0 | Random             | 47.5        | 50.5        | [ITALIC] 1.06         | 49.0        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  1 | Token Distance     | 50.6        | 47.5        | [ITALIC] 0.94         | 49.1        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  2 | Topical Entity     | 50.2        | 47.3        | [ITALIC] 0.94         | 48.8        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  3 | Syntactic Distance | 66.7        | 66.7        | [ITALIC]  [BOLD] 1.00 | 66.7        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  4 | Parallelism        | [BOLD] 69.3 | [BOLD] 69.2 | [ITALIC]  [BOLD] 1.00 | [BOLD] 69.2 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  5 | Parallelism+URL    | [BOLD] 74.2 | [BOLD] 71.6 | [ITALIC]  [BOLD] 0.96 | [BOLD] 72.9 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  6 | Transformer-Single | 59.6        | 56.6        | [ITALIC] 0.95         | 58.1        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  7 | Transformer-Multi  | 62.9        | 61.7        | [ITALIC] 0.98         | 62.3        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus, but the difference is not statistically significant.",
        "evidence": "The claim makes use of data from two different annotator corpora. In the black-aligned corpus, tweets are classified as containing sexism almost twice as frequently and 1.1 times as frequently as containing racism and sexism compared to those in the white-aligned corpus. The difference is not statistically significant. In the white-aligned corpus, tweets are classified as containing sexism and racism less than twice as frequently and 1.1 times as frequently as those in the black-aligned corpus.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model.",
        "evidence": "We can see that pre-clustering to topics was significantly more effective than removing documents from the clusters than with no pre-clustering: Clinics+KCP had an absolute improvement of 6.6 points in CoNLL F1 over our joint model. Remarkably, this was achieved without using any additional resource, tools or annotated data. We believe that this impressive performance should be taken without increasing the number of annotated documents. There is still a large gap between the current performance of the KCP and our joint model.",
        "table": "+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|    | [BOLD] Model                     | R       | MUC P   | [ITALIC] F1   | B3 P    | CEAF- [ITALIC] e P   | CoNLL  [ITALIC] F1   |\n+====+==================================+=========+=========+===============+=========+======================+======================+\n|  0 | [BOLD] Baselines                 | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  1 | Cluster+Lemma                    | 75.5    | 79.9    | 73.6          | 85      | 71.7                 | 76.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  2 | CV Cybulska and Vossen ( 2015a ) | -       | 75      | 64            | 78      | -                    | 73                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  3 | KCP Kenyon-Dean et al. ( 2018 )  | 71      | 71      | 69            | 67      | 67                   | 69                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  4 | Cluster+KCP                      | 77.4    | 79.3    | 71.5          | 87.2    | 66.4                 | 73.6                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  5 | [BOLD] Model Variants            | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  6 | Disjoint                         | 80.3    | 83.6    | 75.9          | 86      | 71.9                 | 78.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  7 | Joint                            | 81      | 84.5    | 77.3          | 85.1    | 73.8                 | [BOLD] 79.5          |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "This means that the cleaned dataset is less complex overall, with more references per MR and fewer diverse MRs.",
        "evidence": "The dataset has been carefully cleaned before the use of the GPT-2 model in order to improve the results. That is, the data from the training set has been split into train and development sets, and the references for the development set have been removed. The main reason for this is that the dataset was less complex overall, with more references per MR and fewer diverse mrs.. Since the GPT-2 model can only score single-sentence paraphrases, the results are not ideal.",
        "table": "+----+---------------------+---------------+--------------+---------------+-----------------+\n|    | [BOLD] Dataset      | [BOLD] Part   | [BOLD] MRs   | [BOLD] Refs   | [BOLD] SER(%)   |\n+====+=====================+===============+==============+===============+=================+\n|  0 | Original            | Train         | 4,862        | 42,061        | 17.69           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  1 | Original            | Dev           | 547          | 4,672         | 11.42           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  2 | Original            | Test          | 630          | 4,693         | 11.49           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  3 | [0.5pt/2pt] Cleaned | Train         | 8,362        | 33,525        | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  4 | [0.5pt/2pt] Cleaned | Dev           | 1,132        | 4,299         | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  5 | [0.5pt/2pt] Cleaned | Test          | 1,358        | 4,693         | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "For both datasets, our approach does not substantially outperform the baselines.",
        "evidence": "For both datasets, our approach slightly outperforms the baselines. Guo et al. As shown in the table, G2S-GAT is always better than S2S and G2S-GIN. Moreover, G2S-GGNN is slightly better than G2S-GGNN, but the difference is not statistically significant. Overall, our G2S-GAT is on par with the best performing baselines.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much lower.",
        "evidence": "As expected, the results for Italian are better than those for German. In both languages, the difference between the average of the debiased embeddings is much lower than the average of the original embeddings. In Italian, the difference is 91.67%. In German, the difference is only 91.67%.",
        "table": "+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|    | [EMPTY]          |   Italian Original |   Italian Debiased |   Italian English | Italian Reduction   |   German Original |   German Debiased |   German English | German Reduction   |\n+====+==================+====================+====================+===================+=====================+===================+===================+==================+====================+\n|  0 | Same Gender      |              0.442 |              0.434 |             0.424 | –                   |             0.491 |             0.478 |            0.446 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  1 | Different Gender |              0.385 |              0.421 |             0.415 | –                   |             0.415 |             0.435 |            0.403 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  2 | difference       |              0.057 |              0.013 |             0.009 | [BOLD] 91.67%       |             0.076 |             0.043 |            0.043 | [BOLD] 100%        |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "As can be seen in the results presented in Table 3 the models using TVMAX in the output attention layer outperform the models using softmax and sparsemax.",
        "evidence": "As can be seen in the results presented in the table, the models using TVMAX in the output attention layer outperform the models using softmax and sparsemax.. The results are interesting especially in the case of yes/no questions where the sparsemax model outperforms the softmax model. This is because selecting bounding boxes of yes/ no questions is easier than selecting bounding boxes of other questions, and thus the linear interpolation of the bounding box’s output and the output of the output of the attention layer is beneficial. Moreover, the results are slightly superior in the case of number questions where the sparsemax model is on par with the softmax model.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "“Coverage” represents how much text a system extracts for a document (higher is better); “Overlap” represents the percentage of words that are in the extractive summarization (higher is better) “Avg.",
        "evidence": "Table 1 shows the human evaluation results. Our system outperforms Refresh significantly (p<0.01) in terms of all the evaluation metrics. In terms of extractive summarization, our system also outperforms the extractive summarization baseline. However, the extractive summarization based on Refresh is slightly better than our system. This is because the extractive summarization is generated by copying words from the input document. In contrast, our system extracts words from the input document that are relevant to the content of the input document.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "For Marian amun, the effect of adding domain labels is significant as we can see in Table 3.",
        "evidence": "As can be seen, our approach is effective for MARIAN AMUN and even better than the strong baseline methods. In particular, we observe that even without domain labels, our approach has significant improvements over the strong baseline methods in all cases.",
        "table": "+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|    | [EMPTY]   | en-fr                                | flickr16    | flickr17    | mscoco17    |\n+====+===========+======================================+=============+=============+=============+\n|  0 | A         | subs1M [ITALIC]  [ITALIC] H+MS-COCO  | 66.3        | 60.5        | 52.1        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  1 | A         | +domain-tuned                        | 66.8        | 60.6        | 52.0        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  2 | A         | +labels                              | [BOLD] 67.2 | 60.4        | 51.7        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  3 | T         | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | 66.9        | 60.3        | [BOLD] 52.8 |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  4 | T         | +labels                              | [BOLD] 67.2 | [BOLD] 60.9 | 52.7        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  5 | [EMPTY]   | en-de                                | flickr16    | flickr17    | mscoco17    |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  6 | A         | subs1M [ITALIC]  [ITALIC] H+MS-COCO  | 43.1        | 39.0        | 35.1        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  7 | A         | +domain-tuned                        | 43.9        | 39.4        | 35.8        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  8 | A         | +labels                              | 43.2        | 39.3        | 34.3        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  9 | T         | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | [BOLD] 44.4 | 39.4        | 35.0        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n| 10 | T         | +labels                              | 44.1        | [BOLD] 39.8 | [BOLD] 36.5 |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Tweets containing emoji seem to be harder for the model to classify than those without.",
        "evidence": "We then repeated the experiment with hashtags and emojis. We can see that tweets containing emoji seem to be easier for the model to classify than those without. This is consistent with the results of the classification experiment with sentiment analysis Wang et al.",
        "table": "+----+-----------+------------------+----------------------+\n|    | [EMPTY]   | [BOLD] Present   | [BOLD] Not Present   |\n+====+===========+==================+======================+\n|  0 | Emoji     | 4805 (76.6%)     | 23952 (68.0%)        |\n+----+-----------+------------------+----------------------+\n|  1 | Hashtags  | 2122 (70.5%)     | 26635 (69.4%)        |\n+----+-----------+------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "We observe that the results for the UD representation are quite a bit lower than the two others.",
        "evidence": "Results of our experiments. We observe that the ud representation performs slightly worse than the other two. We suspect that this is due to the fact that the training data is relatively small, containing only 31k utterances. In terms of the feature maps, however, we achieve the best results for the ud v1.3.",
        "table": "+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|    | [BOLD] Representation   | [BOLD] Hyper parameters Filter size   |   [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func.   |   [BOLD] Hyper parameters L2 Reg. |   [BOLD] Hyper parameters Learning rate |   [BOLD] Hyper parameters Dropout Prob. |   [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values   |\n+====+=========================+=======================================+=============================================+============================================+===================================+=========================================+=========================================+==================================================+==================================================+\n|  0 | CoNLL08                 | 4-5                                   |                                        1000 | Softplus                                   |                           11.5    |                                0.00113  |                                    1    |                                            73.34 | 74.49                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  1 | SB                      | 4-5                                   |                                         806 | Sigmoid                                    |                            0.0813 |                                0.00179  |                                    0.87 |                                            72.83 | [BOLD] 75.05                                     |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  2 | UD v1.3                 | 5                                     |                                         716 | Softplus                                   |                            1.66   |                                0.000963 |                                    1    |                                            68.93 | 69.57                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] EWC models perform well over multiple domains, so the improvement over uniform ensembling is less striking than for unadapted models.",
        "evidence": "Our uniform ensembling heuristics are less striking than for unadapted models. This is expected, as models trained on the same language are trained to have a similar output ratio as the oracle model.",
        "table": "+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|    | [BOLD] Decoder configuration   | [BOLD] es-en  [BOLD] Health   | [BOLD] es-en  [BOLD] Bio   | [BOLD] en-de  [BOLD] News   | [BOLD] en-de  [BOLD] TED   | [BOLD] en-de  [BOLD] IT   |\n+====+================================+===============================+============================+=============================+============================+===========================+\n|  0 | Oracle model                   | 35.9                          | 37.8                       | 37.8                        | 27.0                       | 57.0                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  1 | Uniform                        | 36.0                          | 36.4                       | [BOLD] 38.9                 | 26.0                       | 43.5                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  2 | BI + IS                        | [BOLD] 36.2                   | [BOLD] 38.0                | 38.7                        | [BOLD] 26.1                | [BOLD] 56.4               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "In future work, we are also looking into a systematic way of identifying the markers as well as introducing negation for the LSTM which may be able to capture the negation aspects better.",
        "evidence": "As shown in the results, the proposed CNN-LSTMOur-neg-Ant significantly outperforms all baselines for both positive and negative sentiment. Other variations of the CNN-LSTM show that negation does not seem to be adversely affecting the performance of the model.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "So, the score of analogy 2 will be 0.",
        "evidence": "This happens because the code of our method is publicly available. Word2Sense performed slightly better than OIWE-IPG in semantic analogies, while it did not perform as well in syntactic analogies. Spine performs better in semantic analogies, but it performs worse in syntactic analogies. The results show that the interpretability of our method is consistent with the results reported in other works.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "WN-N shows high coverage containing many high-frequency members.",
        "evidence": "The results from our experiments suggest that wN-N contains a higher coverage than other corpora for two reasons. First, this dataset does not contain many high-frequency members. Second, the training set of wN-N is rather small in comparison to other corpora. We believe that this shows that, even though wN-N is not very strict between type and word distribution, the coverage still is high.",
        "table": "+----+----------+------+--------+--------+\n|    | target   |   VN |   WN-V |   WN-N |\n+====+==========+======+========+========+\n|  0 | type     |   81 |     66 |     47 |\n+----+----------+------+--------+--------+\n|  1 | x+POS    |   54 |     39 |     43 |\n+----+----------+------+--------+--------+\n|  2 | lemma    |   88 |     76 |     53 |\n+----+----------+------+--------+--------+\n|  3 | x+POS    |   79 |     63 |     50 |\n+----+----------+------+--------+--------+\n|  4 | shared   |   54 |     39 |     41 |\n+----+----------+------+--------+--------+",
        "label": "refutes"
    },
    {
        "claim": "In both cases the classifiers trained upon their data are still more likely to flag white-aligned tweets as sexism.",
        "evidence": "The results show that for tweets labeled as sexism by both groups, the classifier trained on Waseem and Hovy (2016) is still more likely to flag white-aligned tweets as sexism than those trained on the Davidson et al. (2017) and Golbeck et al. (2018) datasets. In both cases, the classifiers are more likely to flag black-aligned tweets as sexism.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.01  |                0.01  |       -0.632 | [EMPTY]      |                                  0.978 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.963 |                0.944 |       20.064 | ***          |                                  1.02  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.011 |                0.011 |       -1.254 | [EMPTY]      |                                  0.955 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.349 |                0.29  |       28.803 | ***          |                                  1.203 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.012 |                0.012 |       -0.162 | [EMPTY]      |                                  0.995 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.017 |                0.015 |        4.698 | ***          |                                  1.152 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.988 |                0.991 |       -6.289 | ***          |                                  0.997 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.099 |                0.091 |        6.273 | ***          |                                  1.091 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.074 |                0.027 |       46.054 | ***          |                                  2.728 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.925 |                0.968 |      -41.396 | ***          |                                  0.956 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.01  |                0.01  |        0     | [EMPTY]      |                                  1     |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "However, the model using TVMAX in the final attention layer does not necessarily achieve the highest accuracy, showing that features obtained using the TVMAX transformation are not necessarily a better complement to bounding box features.",
        "evidence": "However, the model using TVMAX in the final attention layer does not always achieve the highest accuracy. The results show that features obtained using the TVMAX transformation are not necessarily a better complement to bounding box features. This is consistent with our hypothesis.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The key advantage of this method is that one does not need a large human-annotated corpus for RL training but can use a simulated corpus for supervised and RL training",
        "evidence": "We evaluated GDPL on the task of RL. This method does not use any large human-annotated corpus for training and can be readily applied in practice. We used the simulated task dataset as the ground-truth agenda for evaluation. In the table, we report three evaluation metrics: inform inform, match rate, and agenda success rate. As the table shows, GDPL achieves the best performance in all these metrics. Although GDPL is not the best in task success, its result on the inform rate and match rate is comparable to that of human annotators. In the RL task, GDPL is comparable to human performance in terms of RL accuracy. This is because although GDPL does not need a large human-annotated corpus for training, the size of the training corpus is sufficient for the RL task.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "It closely matches the performance of ORACLE with only 0.40% absolute difference.",
        "evidence": "Our model closely matches the performance of the oracle. It improves 0.4% absolute on average over the best baseline. Intuitively, our model is capable of exploiting the nuances between aspects of the aspects. We therefore provide details for future works.",
        "table": "+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source            | Target      |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+===================+=============+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer aroma+palate | Beer look   | 74.41 |     74.83 |     74.94 |    72.75 |        76.41 | [BOLD] 79.53 |     80.29 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look+palate  | Beer aroma  | 68.57 |     69.23 |     67.55 |    69.92 |        76.45 | [BOLD] 77.94 |     78.11 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look+aroma   | Beer palate | 63.88 |     67.82 |     65.72 |    74.66 |        73.4  | [BOLD] 75.24 |     75.5  |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "supports"
    },
    {
        "claim": "In Table 2, we can see a noticeable margin brought by our capsule-based approach over the strong baselines on EUR-Lex, and competitive results on RCV1.",
        "evidence": "In Table 2, we can see a noticeable margin brought by our capsule-based approach over the strong baselines on EUR-Lex. Our approach outperforms the best baseline on EUR-Lex by 6.01%, and achieves competitive results on RCV1 by only 0.20%.",
        "table": "+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|    | <bold>Datasets</bold>   | <bold>Metrics</bold>   |   <bold>FastXML</bold> |   <bold>PD-Sparse</bold> |   <bold>FastText</bold> | <bold>Bow-CNN</bold>   |   <bold>CNN-Kim</bold> |   <bold>XML-CNN</bold> | <bold>Cap-Zhao</bold>   | <bold>NLP-Cap</bold>   | <bold>Impv</bold>   |\n+====+=========================+========================+========================+==========================+=========================+========================+========================+========================+=========================+========================+=====================+\n|  0 | RCV1                    | PREC@1                 |                  94.62 |                    95.16 |                   95.4  | 96.40                  |                  93.54 |                  96.86 | 96.63                   | <bold>97.05</bold>     | +0.20%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  1 | RCV1                    | PREC@3                 |                  78.4  |                    79.46 |                   79.96 | 81.17                  |                  76.15 |                  81.11 | 81.02                   | <bold>81.27</bold>     | +0.20%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  2 | RCV1                    | PREC@5                 |                  54.82 |                    55.61 |                   55.64 | <bold>56.74</bold>     |                  52.94 |                  56.07 | 56.12                   | 56.33                  | -0.72%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  3 | [EMPTY]                 | NDCG@1                 |                  94.62 |                    95.16 |                   95.4  | 96.40                  |                  93.54 |                  96.88 | 96.63                   | <bold>97.05</bold>     | +0.20%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  4 | [EMPTY]                 | NDCG@3                 |                  89.21 |                    90.29 |                   90.95 | 92.04                  |                  87.26 |                  92.22 | 92.31                   | <bold>92.47</bold>     | +0.17%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  5 | [EMPTY]                 | NDCG@5                 |                  90.27 |                    91.29 |                   91.68 | 92.89                  |                  88.2  |                  92.63 | 92.75                   | <bold>93.11</bold>     | +0.52%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  6 | EUR-Lex                 | PREC@1                 |                  68.12 |                    72.1  |                   71.51 | 64.99                  |                  68.35 |                  75.65 | -                       | <bold>80.20</bold>     | +6.01%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  7 | EUR-Lex                 | PREC@3                 |                  57.93 |                    57.74 |                   60.37 | 51.68                  |                  54.45 |                  61.81 | -                       | <bold>65.48</bold>     | +5.93%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  8 | EUR-Lex                 | PREC@5                 |                  48.97 |                    47.48 |                   50.41 | 42.32                  |                  44.07 |                  50.9  | -                       | <bold>52.83</bold>     | +3.79%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  9 | [EMPTY]                 | NDCG@1                 |                  68.12 |                    72.1  |                   71.51 | 64.99                  |                  68.35 |                  75.65 | -                       | <bold>80.20</bold>     | +6.01%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n| 10 | [EMPTY]                 | NDCG@3                 |                  60.66 |                    61.33 |                   63.32 | 55.03                  |                  59.81 |                  66.71 | -                       | <bold>71.11</bold>     | +6.59%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n| 11 | [EMPTY]                 | NDCG@5                 |                  56.42 |                    55.93 |                   58.56 | 49.92                  |                  57.99 |                  64.45 | -                       | <bold>68.80</bold>     | +6.75%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain low degree nodes.",
        "evidence": "From the table, we see that S2S, G2S-GGNN and G2S-GAT have similar performance but the performance of G2S-GAT is slightly better than G2S-GGNN when handling low-dimensional graphs. [CONTINUE] The large gap between S2S and G2S-GAT shows the importance of taking into account the information of the entire graph into account.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%).",
        "evidence": "From the table, we can see that all previous methods are ∼1 BLEU point worse than the RNN-based ATR baseline. However, our proposed method, OLRN, achieves 1.2 BLEU point improvement over the RNN-based ATR and 1.5 BLEU point improvement over the SRU-based model. It is clear that the proposed methods can translate sentences faster than previous sentence-level baseline methods. Even compared with the conventional GRU model, OLRN is still 12%∼19% faster than SRU in training, and 13%∼15% faster in decoding.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Compared to CMOW, the hybrid model shows rather small differences.",
        "evidence": "The first group shows that, compared to CMOW, the hybrid model shows rather small improvements. Our proposed model (H-COW) can outperform MCOW in noun, verb, subjNum, CoordInv, TopConst, and Scythe. It can also outperform CBOW/400, which is a word based model, which shows that character-level models can sometimes have difficulty in tracking the whole word history. However, character-level models still achieve much better performance than word-level models, which indicates that subjNum, CoordInv, and TopConst are usually hard to learn. The second group shows that, compared to CMOW, the hybrid model can outperform MCOW/784, which is a very large model. However, the improvement is not significant compared to our proposed model. This may be because the computational complexity of our method is rather reasonable.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "MLP with BERT as encoder does not have the best overall performance.",
        "evidence": "From this table, we see that the proposed MLP with BERT as encoder does not perform the best overall. Specifically, compared to the other two baselines (simRED and MLP), BERT-based model does not achieve the best performance on both precision and recall. For example, among the baseline models, BERT-based model obtains only slightly better precision on recall than the vanilla CNN-RNN, which is significantly better than the hybrid model (MLP), i.e., without BERT as encoder. As for the effect of BERT optimization, we believe that fine-tuning BERT for MLP can potentially mitigate the overfitting issue.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "MIL-ND does not significantly outperform MIL: the 95% confidence intervals for them overlap.",
        "evidence": "We can see that our τMIL-ND outperforms MIL on two of the three measures. The MIL (Model 1) difference is small, which seems to be because of the small number of overlap domains. On the other hand, our τMIL-ND outperforms MIL (Model 2) by about 2.5% on in-domain F1. This implies that our approach is able to match or exceed the quality of the target domain responses using little human-annotated training data.",
        "table": "+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|    | System                     | All P        | All R        | All F1             | In  [ITALIC] E+ P   | In  [ITALIC] E+ R   | In  [ITALIC] E+ F1   |\n+====+============================+==============+==============+====================+=====================+=====================+======================+\n|  0 | Name matching              | 15.03        | 15.03        | 15.03              | 29.13               | 29.13               | 29.13                |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  1 | MIL (model 1)              | 35.87        | 35.87        | 35.87 ±0.72        | 69.38               | 69.38               | 69.38 ±1.29          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  2 | MIL-ND (model 2)           | 37.42        | [BOLD] 37.42 | 37.42 ±0.35        | 72.50               | [BOLD] 72.50        | [BOLD] 72.50 ±0.68   |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  3 | [ITALIC] τMIL-ND (model 2) | [BOLD] 38.91 | 36.73        | [BOLD] 37.78 ±0.26 | [BOLD] 73.19        | 71.15               | 72.16 ±0.48          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  4 | Supervised learning        | 42.90        | 42.90        | 42.90 ±0.59        | 83.12               | 83.12               | 83.12 ±1.15          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "in general, 5.2% of tokens are negation cues, 26.1% of tokens are negated, and 11.2% of tokens are negated but are not cues.",
        "evidence": "The claim text contains a large number of negation cues. In order to build a more robust and robust model, we analyzed the training data. We excluded false negation cues from our training data, and only considered true negation cues when they were present in the data. We also removed tweets that contained one of the negation cues. Based on the analysis, we conclude that the length of the negation completion is relatively low. The average scope length of tweets is 2.9, the average sentence length is 13.6, and the average tweet length is 22.3.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "However, when gold PP attachment are used, we note only a small improvement of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which suggests that adding PP predictions as features is not an effective approach.",
        "evidence": "We used the oracle PP attachment as a set of features. This indicates that the oracle prediction is not an effective approach by itself. In this table, we also report the results when using the gold PP attachment for our models. We see that using the gold PP attachment as features only leads to a small improvement of 10.46 points in accuracy for the models. This is extremely lower than the improvements achieved by the oracle system, which demonstrates that the oracle prediction is not an effective approach by itself.",
        "table": "+----+-------------------+-------------------+-------------------+\n|    | [BOLD] System     |   [BOLD] Full UAS |   [BOLD] PPA Acc. |\n+====+===================+===================+===================+\n|  0 | RBG               |             94.17 |             88.51 |\n+----+-------------------+-------------------+-------------------+\n|  1 | RBG + HPCD (full) |             94.19 |             89.59 |\n+----+-------------------+-------------------+-------------------+\n|  2 | RBG + LSTM-PP     |             94.14 |             86.35 |\n+----+-------------------+-------------------+-------------------+\n|  3 | RBG + OntoLSTM-PP |             94.3  |             90.11 |\n+----+-------------------+-------------------+-------------------+\n|  4 | RBG + Oracle PP   |             94.6  |             98.97 |\n+----+-------------------+-------------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "BERT achieved a final accuracy of 87.47%, lower than ULMFiT's full performance.",
        "evidence": "BERT achieved an accuracy of 87.47%, lower than ULMFiT's full performance. The pretrained ULMFiT is trained for over 12 hours, with an average of 2.5 epochs per epoch. GPT-2, on the other hand, finetuned for 8 hours, with an average of 4.5 epochs per epoch. Further, ULMFiT (no LM finetuning) outperforms BERT (BERT + multitasking) by 3.9%, while GPT-2 (BERT + multitasking) performs slightly better. This suggests that the performance of ULMFiT is not solely determined by the number of epochs.",
        "table": "+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|    | Model                     | Val. Accuracy   |   Loss |   Val. Loss | Pretraining Time   | Finetuning Time   |\n+====+===========================+=================+========+=============+====================+===================+\n|  0 | Siamese Networks          | 77.42%          | 0.5601 |      0.5329 | [EMPTY]            | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  1 | BERT                      | 87.47%          | 0.4655 |      0.4419 | 66 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  2 | GPT-2                     | 90.99%          | 0.2172 |      0.1826 | 78 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  3 | ULMFiT                    | 91.59%          | 0.375  |      0.1972 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  4 | ULMFiT (no LM Finetuning) | 78.11%          | 0.5512 |      0.5409 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  5 | BERT + Multitasking       | 91.20%          | 0.3155 |      0.3023 | 66 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  6 | GPT-2 + Multitasking      | 96.28%          | 0.2609 |      0.2197 | 78 hours           | 5m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results reported in Table 7 show that precision on BDI indeed increases as a result of the reduced effect of grammatical gender on the embeddings for German and Italian, i.e.",
        "evidence": "As expected, the results reported in Table 7 show that precision on BDI indeed increases as a result of the reduced effect of grammatical gender on the embeddings for German and Italian.",
        "table": "+----+-----------+----------------+----------------+---------------+---------------+\n|    | [EMPTY]   | Italian → En   | Italian En →   | German → En   | German En →   |\n+====+===========+================+================+===============+===============+\n|  0 | Orig      | 58.73          | 59.68          | 47.58         | 50.48         |\n+----+-----------+----------------+----------------+---------------+---------------+\n|  1 | Debias    | [BOLD] 60.03   | [BOLD] 60.96   | [BOLD] 47.89  | [BOLD] 51.76  |\n+----+-----------+----------------+----------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "All metrics have good correlations and become more informative when BERT embeddings are used",
        "evidence": "all the metrics have good correlations. Pearson’s r stays at .286 for all the metrics. Spearman’s rank correlation (Precision) is comparable for G-Pre and G-Rec, but weaker for ROUGE-SU4 and BLEU-1.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "Replacing the attention normalizing function with softmax operation also reduces the F1 score marginally (A3−A5).",
        "evidence": "We can see that, for BiLSTM-CNN, replacing the attention normalizing function with softmax operation also reduces the F1 score marginally (A3−A5). The results are consistent with our intuition.",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] A distinctive part-of-speech pattern common in complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items of services possessed by the complainer (e.g., my account, my order).",
        "evidence": "Top unigrams and part-of-speech patterns (subword unigrams and part-of-speech pattern) of complaints are analyzed. [CONTINUE] Negations tend to use possessive pronouns followed by nouns (PRP$ nn) which refer to items of services possessed by the complainer (e.g., my account, my order). This is one of the features people tend to mention in complaints. Another pattern is that verbs that are used to refer to good or bad services (good, great, win, fix) are frequently used in complaints. For example, the pronoun on “good” often refers to items of services possessed by the complainer (e.g., good, great, fix) whereas the noun on “good” refers to anything to the complainer (e.g., good, bad, issue, days).",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The HAN models do not outperform MEAD in terms of sentence prediction.",
        "evidence": "Here, we summarize the claim that HAN models do not outperform MEAD. With respect to the ROUGE scores, HAN models have 0.4±0.5 and 1.8% improvement over MEAD for the full and sentence-level datasets respectively. This indicates that HAN models can generate more natural and accurate summary than MEAD. In terms of the sentence accuracy, HAN models are still not as good as MEAD. This is expected, since MEAD is designed for sentence summarization, which is a more complex task than ROUGE. Still, HAN models still achieve some improvement over MEAD in terms of sentence accuracy. This suggests that HAN models can generate more natural and accurate summary than MEAD.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Interestingly, G2S-GIN has better performance among our models.",
        "evidence": "From the table, we see that G2S-GAT has better performance than S2S and G2S-GGNN in all datasets. For example, in LDC2015e86, G2S-GAT achieves 22.93% BLEU score, which is slightly better than S2S by 0.1. And in LDC2017T10, G2S-GAT achieves 27.87% BLEU score, which is about a 1.4% improvement over S2S. Overall, G2S-GAT has a better performance than all the baselines.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] For LOC, it turns out that candidate selection is not a bottleneck: when candidate selection was flawless, the models made only about 55% errors, down from about 96%.",
        "evidence": "As expected, the supervised learning baseline is stronger than it was for LOC, due to the candidate selection problem using strict n-gram matching. Interestingly, this strong supervised learning baseline also does not work well for other entity typing models. Specifically, we observe that the InferSent models do well on MISC, probably because there are many training examples for MISC, and fine-tuning only on MISC leads to overfitting – the InferSent models do well on MISC, but do poorly on the other entity typing tasks.",
        "table": "+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|    | System              | All LOC      | All ORG      | All PER      | All MISC     | In  [ITALIC] E+ LOC   | In  [ITALIC] E+ ORG   | In  [ITALIC] E+ PER   | In  [ITALIC] E+ MISC   |\n+====+=====================+==============+==============+==============+==============+=======================+=======================+=======================+========================+\n|  0 | Name matching       | 96.26        | 89.48        | 57.38        | 96.60        | 92.32                 | 76.87                 | 47.40                 | 76.29                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  1 | MIL                 | 57.09        | [BOLD] 76.30 | 41.35        | 93.35        | 11.90                 | [BOLD] 47.90          | 27.60                 | 53.61                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  2 | MIL-ND              | 57.15        | 77.15        | 35.95        | 92.47        | 12.02                 | 49.77                 | 20.94                 | 47.42                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  3 | [ITALIC] τMIL-ND    | [BOLD] 55.15 | 76.56        | [BOLD] 34.03 | [BOLD] 92.15 | [BOLD] 11.14          | 51.18                 | [BOLD] 20.59          | [BOLD] 40.00           |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+\n|  4 | Supervised learning | 55.58        | 61.32        | 24.98        | 89.96        | 8.80                  | 14.95                 | 7.40                  | 29.90                  |\n+----+---------------------+--------------+--------------+--------------+--------------+-----------------------+-----------------------+-----------------------+------------------------+",
        "label": "refutes"
    },
    {
        "claim": "TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure.",
        "evidence": "This claim holds in all corpora but the English version of TED talks, where TF has the best values of recall and F-measure.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Table 4 shows the BLEU scores of our Dual2seq model taking gold or automatic AMRs as inputs.",
        "evidence": "The result shown in Table 4 shows that the BLEU score of our dual-seq model taking gold or automatic AMRs as inputs is significantly higher than the BLEU score of the model taking automatic AMRs as inputs. [CONTINUE] This is an encouraging result, in particular, since the use of automatic AMRs has been considered so far to be reliable and accurate for machine translation.",
        "table": "+----+-------------+---------------+\n|    | AMR Anno.   | BLEU          |\n+====+=============+===============+\n|  0 | Automatic   | 16.8          |\n+----+-------------+---------------+\n|  1 | Gold        | [BOLD] *17.5* |\n+----+-------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Our model does not improve the precision scores on both datasets with good recall scores.",
        "evidence": "Our model does not significantly improve the recall scores on both datasets with good recall scores. However, our F1 score on NYT10 is slightly higher than the corresponding scores of the baselines. We reason that this is due to the fact that NYT10 and NYT11 are distributed in a hierarchical way, while NYT10 was randomly distributed. This can be seen from the fact that our model can well capture the temporal dependencies.",
        "table": "+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|    | Model                 | NYT10 Prec.   | NYT10 Rec.   | NYT10 F1      | NYT11 Prec.   | NYT11 Rec.   | NYT11 F1      |\n+====+=======================+===============+==============+===============+===============+==============+===============+\n|  0 | CNN zeng2014relation  | 0.413         | 0.591        | 0.486         | 0.444         | 0.625        | 0.519         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  1 | PCNN zeng2015distant  | 0.380         | [BOLD] 0.642 | 0.477         | 0.446         | 0.679        | 0.538†        |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  2 | EA huang2016attention | 0.443         | 0.638        | 0.523†        | 0.419         | 0.677        | 0.517         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  3 | BGWA jat2018attention | 0.364         | 0.632        | 0.462         | 0.417         | [BOLD] 0.692 | 0.521         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  4 | BiLSTM-CNN            | 0.490         | 0.507        | 0.498         | 0.473         | 0.606        | 0.531         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  5 | Our model             | [BOLD] 0.541  | 0.595        | [BOLD] 0.566* | [BOLD] 0.507  | 0.652        | [BOLD] 0.571* |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics perform in the same range as the part of speech tags.",
        "evidence": "This claim is supported by the results of the neural network models which performed the best in the task of sentiment prediction. Specifically, the best results were obtained by the LSTM model with LSTM layer, max-pooling layer, bidirectional LSTM layer, and max-pooling factor with kernel size 3. This model achieved the best results in terms of accuracy, F1, and area under the ROC curve (AUC). These results suggest that the use of topical features can improve the prediction accuracy as the distributions of syntactic and semantic information are in the same range as the part of speech tags. The Word2Vec topics perform the best in terms of AUC, and the best in terms of F1 and AUC than the LIWC dictionaries, which contain only syntactic and semantic information. This is coherent with the results of the sentiment analysis results, showing that syntactic information as well as semantic information can be improved with our proposed approaches.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, the highest accuracy was achieved by using Binary Cross Entropy, with a score of 55.20.",
        "evidence": "We can see that using beam search on the corpus-level results in the baseline methods achieved an accuracy of 48.54%. However, using deep learning models, the highest accuracy was achieved when using softmax. This result suggests that, in contrast to Harris et al.",
        "table": "+----+----------------------+-----------------------+\n|    | Model                | Accuracy on RefClef   |\n+====+======================+=======================+\n|  0 | BM + Softmax         | 48.54                 |\n+----+----------------------+-----------------------+\n|  1 | BM + BCE             | 55.20                 |\n+----+----------------------+-----------------------+\n|  2 | BM + FL              | 57.13                 |\n+----+----------------------+-----------------------+\n|  3 | BM + FL + Img-Resize | [BOLD] 61.75          |\n+----+----------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "despite their sensitivity to these semantic clues, BERT models trained with their own distributions alone make better decisions when we combine their outputs.",
        "evidence": "Our results showed that, as expected, models trained with their own distributions alone make better decisions. We suspect that this is because the distractor words are often syntactically meaningless, and the model has to learn to ignore them in order to make correct predictions. For example, although “woman” and “mother” have similar syntactic structures, the syntactic similarity of “woman” and “mother” is not nearly as clear as the similarity between “woman” and “went”, for example, there is a much larger gap in the similarity between “woman” and “went” after adding the syntactic information of “mother” and “sentence”.",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "for example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).",
        "evidence": "The results from each model are very different from each other. For example, the RoBERTa-large achieves a BLEU score of 90.6 compared to 76.5 for BERT-large, which is still 2.7 points lower than the result of GCN+RC+LA.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, it is not as robust as MQAN, suffering a dramatic decrease in performance on QA-SRL.",
        "evidence": "This claim is particularly strong as recent QA-SRL models, i.e., SQuAD and QA-SRL, have shown improved performance in the QA-SRL task. However, it is not as robust as MQAN, as we can see from the drop in performance on QA-SRL.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The use of annotated NLDs as supervision does not improve the generalization ability of question answering.",
        "evidence": "The claim is that the PRKGC models learned to reason over more than simple shortest paths. Especially, the PRKGC model outperforms the NLD-based model (the PRKGC+NS model) in all the four metrics. This shows that the NLDs used as supervision does not improve the ability of question answering.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "If the user simulator may select the same action only in a row, this allows the action space to be reduced to 6 possible action sequences.",
        "evidence": "The action space is reduced to 6 possible actions (the smaller, the better). We can see that GDPL performs the best in all cases.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "Next sentence prediction (NSP) has a positive impact.",
        "evidence": "The claim that next sentence prediction (NSP) has a positive impact can be investigated by training on combative datasets, i.e., B-COPA, or B-COPA+RoBERTa. Overall, training on B-COPA improves accuracy for BERT-large, RoBERTa-large and BERT-*-NSP. However, training on B-COPA improves accuracy for BERT-large and RoBERTa-large on the hard dataset. The results clearly show that the choice of the training dataset doesn’t affect model performance on the “hard” dataset.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Pretraining the HAN models yields significantly better results than those without.",
        "evidence": "We use ROUGE-1, ROUGE-2, and sentence-level F-score as our automatic evaluation metrics. Overall, HAN models outperform all the baselines by a large margin. HAN+pretraining, HAN+pretrainu and HAN+pretrainr all achieve significant improvements over HAN. They also obtain an improvement in sentence-level F-score over all the baselines, showing that HAN models are capable of producing more informative summaries. [CONTINUE] It is worth noting that continuing pretraining HAN even further (i.e., HAN+pretrainu and HAN+pretrainr) yields even better results. This suggests that the HAN models we pretrain can be further improved.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Despite performing slightly worse than sparsemax under automatic metrics, TVMAX does not outperform sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2.",
        "evidence": "Despite performing slightly worse than sparsemax under automatic metrics, TVMAX outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation.",
        "table": "+----+-----------+-------------+-----------------------+\n|    | [EMPTY]   | caption     | attention relevance   |\n+====+===========+=============+=======================+\n|  0 | softmax   | 3.50        | 3.38                  |\n+----+-----------+-------------+-----------------------+\n|  1 | sparsemax | 3.71        | 3.89                  |\n+----+-----------+-------------+-----------------------+\n|  2 | TVmax     | [BOLD] 3.87 | [BOLD] 4.10           |\n+----+-----------+-------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The ULMFiT model achieved the best results with a F1-score of 0.861 on the training dataset and a F1-score of 0.701 on the test dataset.",
        "evidence": "Baseline Models We evaluated our proposed models using the provided baseline models. In particular, we used the ULMFiT model which achieved the best results on the training dataset with a F1-score of 0.861 and a F1-score of 0.701 on the test dataset.",
        "table": "+----+---------------------------------------------------------+---------------------+--------------------+\n|    | [BOLD] Model                                            |   [BOLD] F1 (train) |   [BOLD] F1 (test) |\n+====+=========================================================+=====================+====================+\n|  0 | [BOLD] Multinomial Naive Bayes (using Count Vectorizer) |               0.641 |              0.517 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  1 | [BOLD] Logistic Regression (using Count Vectorizer)     |               0.679 |              0.572 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  2 | [BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer)     |               0.695 |              0.576 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  3 | [BOLD] LSTM (128 LSTM Units)                            |               0.731 |              0.591 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  4 | [BOLD] Provided Baseline                                |               0.72  |              0.267 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  5 | [BOLD] ULMFit*                                          |               0.861 |              0.701 |\n+----+---------------------------------------------------------+---------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "For example, the is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 17.0% of COPA training instances.",
        "evidence": "For example, the token with the highest coverage is the token The. It appears in either a correct alternative or wrong alternative in 17.0% of COPA training instances. Its coverage is also significantly higher than that of the other alternative tokens. For example, the token The is in the correct alternative in 55.3% of COPA training instances. In contrast, it appears in just 9.4% of the instances of the incorrect alternative.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "refutes"
    },
    {
        "claim": "The best performing system is not KnowComb.",
        "evidence": "The results show that the best performing system is KnowComb. It significantly outperforms all other approaches on both datasets.",
        "table": "+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|    | Dataset            | Metric    |   Illinois |   IlliCons | rahman2012resolving   |   KnowFeat |   KnowCons | KnowComb     |\n+====+====================+===========+============+============+=======================+============+============+==============+\n|  0 | [ITALIC] Winograd  | Precision |      51.48 |      53.26 | 73.05                 |      71.81 |      74.93 | [BOLD] 76.41 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+\n|  1 | [ITALIC] WinoCoref | AntePre   |      68.37 |      74.32 | —–                    |      88.48 |      88.95 | [BOLD] 89.32 |\n+----+--------------------+-----------+------------+------------+-----------------------+------------+------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "The results for testing on cleaned data (Table 3, top half) do not confirm the positive impact of cleaned training data and also show that the cleaned test data is not more challenging (cf.",
        "evidence": "The results for testing on the original test set and the cleaned data (Cleansed) are presented in Tab. The results of testing on the original test set after cleaning are presented in Tab. The models trained on the cleaned data and tested on the original data are very close to the results on the original test set. This indicates that the data that has been cleaned is not so challenging. Moreover, models trained on the cleaned data and tested on the original data have similar performance on METEOR and ROUGE-L, while the performance on CIDEr of the model trained on the cleaned data and tested on the original data is very close.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Table 6 shows that our system outperforms the best previous approaches across the five languages.",
        "evidence": "Our system outperforms the best previous approaches across the five languages.",
        "table": "+----+------------+-----------------------------+--------------+\n|    | Language   | System                      | F1           |\n+====+============+=============================+==============+\n|  0 | es         | GTI                         | 68.51        |\n+----+------------+-----------------------------+--------------+\n|  1 | es         | L +  [BOLD] CW600 + W2VW300 | [BOLD] 69.92 |\n+----+------------+-----------------------------+--------------+\n|  2 | es         | Baseline                    | 51.91        |\n+----+------------+-----------------------------+--------------+\n|  3 | fr         | IIT-T                       | 66.67        |\n+----+------------+-----------------------------+--------------+\n|  4 | fr         | L +  [BOLD] CW100           | [BOLD] 69.50 |\n+----+------------+-----------------------------+--------------+\n|  5 | fr         | Baseline                    | 45.45        |\n+----+------------+-----------------------------+--------------+\n|  6 | nl         | IIT-T                       | 56.99        |\n+----+------------+-----------------------------+--------------+\n|  7 | nl         | L +  [BOLD] W2VW400         | [BOLD] 66.39 |\n+----+------------+-----------------------------+--------------+\n|  8 | nl         | Baseline                    | 50.64        |\n+----+------------+-----------------------------+--------------+\n|  9 | ru         | Danii.                      | 33.47        |\n+----+------------+-----------------------------+--------------+\n| 10 | ru         | L +  [BOLD] CW500           | [BOLD] 65.53 |\n+----+------------+-----------------------------+--------------+\n| 11 | ru         | Baseline                    | 49.31        |\n+----+------------+-----------------------------+--------------+\n| 12 | tr         | L +  [BOLD] BW              | [BOLD] 60.22 |\n+----+------------+-----------------------------+--------------+\n| 13 | tr         | Baseline                    | 41.86        |\n+----+------------+-----------------------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "increasing the number of items in each set does not help, since the simple [ITALIC] nearest-neighbour method starts with a prohibitively high precision, which cannot be improved by introducing more instances",
        "evidence": "This does not help. Since the nearest neighbours are trained on a very small subset of the training set, the improvement of our nearest-neighbour method starts at a prohibitively high precision, and cannot be improved by introducing more instances.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "According to the table, the drop of precision demonstrates that the capsule net is more useful than the word-level attention.",
        "evidence": "This table shows that the word-level attention is significantly more useful than the capsule-level attention.. From the results, we can see that the drop of precision demonstrates that the capsule net is more useful than the word-level attention.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments [CONTINUE] From Table 5, all validations show weak correlations on the Yelp dataset and poor correlations on Literature.",
        "evidence": "We use the greedy method to compute sentence-level Spearman’s ρ and compare it with our two baselines, sim and pp. All experiments show weak correlations on the Yelp dataset and poor correlations on literature..",
        "table": "+----+----------+---------------------------------------------------------------------------+--------+--------+\n|    | Metric   | Method of validation                                                      |   Yelp |   Lit. |\n+====+==========+===========================================================================+========+========+\n|  0 | Acc      | % of machine and human judgments that match                               |  94    |  84    |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  1 | Sim      | Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation |   0.79 |   0.75 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+\n|  2 | PP       | Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency       |   0.81 |   0.67 |\n+----+----------+---------------------------------------------------------------------------+--------+--------+",
        "label": "refutes"
    },
    {
        "claim": "We gain further improvement by adding monolingual data and get an accuracy of 74.2%, which is only 0.3 points higher than the best language model.",
        "evidence": "Compared to the SOTA, the fine-tuned model gets an improvement of 0.3 points of accuracy. In terms of language, the English-only-LM is the best model, followed by the Spanish-only-LM and the Czech-only-LM. This confirms the superiority of SLC-last-LM over All:Shuffled-LM and All:CS-Only-LM. The poor results of the SLC-only-LM and the Cs-Only+vocab-LM are mainly due to the lack of enough monolingual data. The comparison between All:CS-Last-LM and Cs-Only-LM shows that without additional monolingual data, All:CS-Last-LM and Cs-Only-LM can achieve comparable results. However, there is still a huge gap between All:CS-Last-LM and Cs-Only-LM. The additional monolingual data is not enough to train a proper language model. We can find the reason behind this issue by comparing the Cs-Only-LM and the Cs-Only+vocab-LM. Without proper nouns, the Cs-Only+vocab-LM can achieve better results than the Cs-Only-LM, while with additional nouns the Cs-Only-LM can even achieve a better performance than the English-only-LM. Without more concrete nouns, the fine-tuned model is comparable to the Cs-Only-LM. However, the Cs-Only-LM can achieve a better accuracy than the English-only-LM, which demonstrates its superiority over the Cs-Only-LM.",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "POS-disambiguation, in turn, fragments the vocabulary and consistently reduces the coverage with the effect being less pronounced for lemmatized targets.",
        "evidence": "We first compare the coverage with the presence of lemmatized targets for the disambiguation task. The results are similar to the ones of POS tagging, in particular, the advantage of using the character-based disambiguation model is smaller when compared to the type and word-based disambiguation models. This is expected as the type identification task is performed on parts of the vocabulary while the lemmatization model only uses the words that appear in the training set. More importantly, the coverage is consistently lower for all targets while the effect of the disambiguation model is less pronounced for lemmatized targets.",
        "table": "+----+----------+------+--------+--------+\n|    | target   |   VN |   WN-V |   WN-N |\n+====+==========+======+========+========+\n|  0 | type     |   81 |     66 |     47 |\n+----+----------+------+--------+--------+\n|  1 | x+POS    |   54 |     39 |     43 |\n+----+----------+------+--------+--------+\n|  2 | lemma    |   88 |     76 |     53 |\n+----+----------+------+--------+--------+\n|  3 | x+POS    |   79 |     63 |     50 |\n+----+----------+------+--------+--------+\n|  4 | shared   |   54 |     39 |     41 |\n+----+----------+------+--------+--------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Under system setup, our model CANDELA does not statistically significantly outperform all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005).",
        "evidence": "Our results are shown in Table. Our model (candela) significantly outperforms all comparisons and the retrieval model in all metrics. Specifically, candela performs better than the retrieval model in terms of B-2, B-4, and R-2 without oracle retrieval (p < .0005). Only two systems are competitive w/ oracle retrieval: seq2seqaug and seq2seq. This is mainly because the seq2seq model makes use of document context to enrich the representation of the passage with image and description. Comparing our results with human results, we can see that retrieval is better than both seq2seq and seq2seqaug. Another observation is that our model is better than both baselines in terms of MTR and #sent. This is mainly because employing PSG in the retrieval step can significantly boost the performance.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "One work-around for this would be to leverage the sequential nature of the user simulator action selection.",
        "evidence": "While GDPL achieves the best overall performance, it is important to leverage the sequential nature of the user simulator action selection. In practice, we can tune the latter part of the model depending on the task. For example, in Eq. we can choose to optimize for PPO to reduce the number of user simulator turns, thereby increasing the chance of user simulator responses that are relevant to the query.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "Unlike [14], we do not use HypeNET because the code is not publicly available.",
        "evidence": "We use GloVe embeddings to perform the experiments and employ the inverse-path similarity between the embeddings and the question embeddings as the reasoning method. Unlike the previous work, we do not use CoNLL-2002 Our results are the average of three runs, i.e., in terms of F-score. In particular, our proposed method outperforms all the baselines in semantic and syntactic similarity tasks. However, it should be noted that the results of the semantic group are better than the syntactic groups for several methods, e.g., GloVe performs better than word2vec in semantic group and word2sense performs better than OIWE-IPG in syntactic group. This is due to the fact that the semantic group of GloVe is high precision while the syntactic group is low precision.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Opinion distance methods do not generally outperform the competition on both ARI and Silhouette coefficient.",
        "evidence": "The ARI and Silhouette coefficient results are shown in Table. [CONTINUE] Opinion distance methods do not generally outperform the competition on both ARI and Silhouette coefficient. [CONTINUE] Doc2vec and BERT are exceptions, [CONTINUE] For example, TF-IDF performs best on two topics, Atheism and Education voucher scheme, but poorly on the other.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "However, at similar levels of Acc, our models have higher BLEU scores than prior work.",
        "evidence": "The source code is provided by \\newciteyang2018unsupervised. However, at similar levels of accuracy, our models have higher BLEU scores than prior work.",
        "table": "+----+----------------------+----------------------+----------------------+\n|    | Model                | BLEU                 | Acc∗                 |\n+====+======================+======================+======================+\n|  0 | fu-1                 | [EMPTY]              | [EMPTY]              |\n+----+----------------------+----------------------+----------------------+\n|  1 | Multi-decoder        | 7.6                  | 0.792                |\n+----+----------------------+----------------------+----------------------+\n|  2 | Style embed.         | 15.4                 | 0.095                |\n+----+----------------------+----------------------+----------------------+\n|  3 | simple-transfer      | simple-transfer      | simple-transfer      |\n+----+----------------------+----------------------+----------------------+\n|  4 | Template             | 18.0                 | 0.867                |\n+----+----------------------+----------------------+----------------------+\n|  5 | Delete/Retrieve      | 12.6                 | 0.909                |\n+----+----------------------+----------------------+----------------------+\n|  6 | yang2018unsupervised | yang2018unsupervised | yang2018unsupervised |\n+----+----------------------+----------------------+----------------------+\n|  7 | LM                   | 13.4                 | 0.854                |\n+----+----------------------+----------------------+----------------------+\n|  8 | LM + classifier      | [BOLD] 22.3          | 0.900                |\n+----+----------------------+----------------------+----------------------+\n|  9 | Untransferred        | [BOLD] 31.4          | 0.024                |\n+----+----------------------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9).",
        "evidence": "The claim By applying our technique, the entire DCGCN model can be flexibly extended to DCGCN2, which can be viewed as an alternative to DCGCN1. For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9). Moreover, DCGCN(4) can obtain the best performance, which is significantly better than DCGCN(1) and DCGCN(2). Similar effects can be observed when varying the number of convolution layers. For example, when the number of convolution layers is reduced from 4 to 2, the performance of DCGCN(4) is still worse than DCGCN(1).",
        "table": "+----+----------------+-----+-------+-------------+-------------+\n|    | [BOLD] Model   |   D | #P    | B           | C           |\n+====+================+=====+=======+=============+=============+\n|  0 | DCGCN(1)       | 300 | 10.9M | 20.9        | 52.0        |\n+----+----------------+-----+-------+-------------+-------------+\n|  1 | DCGCN(2)       | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3 |\n+----+----------------+-----+-------+-------------+-------------+\n|  2 | DCGCN(2)       | 240 | 11.3M | 22.8        | 52.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  3 | DCGCN(4)       | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  4 | DCGCN(1)       | 420 | 12.6M | 22.2        | 52.4        |\n+----+----------------+-----+-------+-------------+-------------+\n|  5 | DCGCN(2)       | 300 | 12.5M | 23.8        | 53.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  6 | DCGCN(3)       | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1 |\n+----+----------------+-----+-------+-------------+-------------+\n|  7 | DCGCN(2)       | 360 | 14.0M | 24.2        | [BOLD] 54.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  8 | DCGCN(3)       | 300 | 14.0M | [BOLD] 24.4 | 54.2        |\n+----+----------------+-----+-------+-------------+-------------+\n|  9 | DCGCN(2)       | 420 | 15.6M | 24.1        | 53.7        |\n+----+----------------+-----+-------+-------------+-------------+\n| 10 | DCGCN(4)       | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8 |\n+----+----------------+-----+-------+-------------+-------------+\n| 11 | DCGCN(3)       | 420 | 18.6M | 24.5        | 54.6        |\n+----+----------------+-----+-------+-------------+-------------+\n| 12 | DCGCN(4)       | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------+-----+-------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "In conclusion, these results above can show the robustness and effectiveness of our DCGCN models.",
        "evidence": "The results of the GCN model always improve with the added features. Comparing the GCN model with the DCGCN model, we can see the improvement score of the DCGCN model is greater than GCN model. For example, when +RC (4) is used, the improvement score of DCGCN1 over GCN1 is 3.8% and 3.4% on BLEU and C respectively. And similarly, for +RC (9), the improvement score is 3.6% and 2.6% on BLEU and C respectively. In these cases, we can see the effectiveness of our DCGCN model.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "SegMatch works much better than Audio2vec according to both criteria.",
        "evidence": "We clearly can see that Support Vector Machines (SVG) is worse than the other algorithms. We looked at the recall@10 and mean MFCC values, and we can see that SGM is significantly better than Audio2Vec. We can also see the consensus score is also much higher than the other algorithms.",
        "table": "+----+-------------+-----------------+---------------+------------+\n|    | [EMPTY]     | Recall@10 (%)   | Median rank   | RSAimage   |\n+====+=============+=================+===============+============+\n|  0 | VGS         | 27              | 6             | 0.4        |\n+----+-------------+-----------------+---------------+------------+\n|  1 | SegMatch    | [BOLD] 10       | [BOLD] 37     | [BOLD] 0.5 |\n+----+-------------+-----------------+---------------+------------+\n|  2 | Audio2vec-U | 5               | 105           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  3 | Audio2vec-C | 2               | 647           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  4 | Mean MFCC   | 1               | 1,414         | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  5 | Chance      | 0               | 3,955         | 0.0        |\n+----+-------------+-----------------+---------------+------------+",
        "label": "supports"
    },
    {
        "claim": "In contrast, our DCGCN models cannot be trained using a large number of layers.",
        "evidence": "Note that the best results of the GCN models are obtained using 2 layers in our model. In contrast, the best results of the DCGCN models are obtained using 9 layers in our model. As it can be seen, the proposed DCGCN models can be trained much faster than GCNs, with a best-case accuracy of 55.4%, and significantly better performance than other models, including the GCN baseline. This is not true for smaller or more fine-grained models, e.g., the DCGCN3 and DCGCN2. This is because that the attention mechanism of the DCGCN model is not powerful enough, and requires a large number of layers to properly capture the locality differences between the language and image regions. In contrast, the GCN models can be trained in a single layer, i.e. 4 frames or less, as is the case in the GCN models.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Logistic Regression outperforms other classifiers in extracting most relations.",
        "evidence": "The claim-premise logistic regression outperforms other classifiers in extracting most relations.. The improvement is statistically significant using two-tailed t-test with p-value < 0.01. Further, embeddings derived from the Wiki-PubMed-PMC outperform sentence embeddings derived from GloVe. ANN outperforms SVM, LR, and SVM in extracting most relations.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Our joint model outperforms all the base [CONTINUE] The results reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016).",
        "evidence": "We reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016). Our joint model is 1.8% (absolute) better in CoNLL F1 score than all the base models. Moreover, it is also 1.2% (absolute) better in B3 F1 score and 0.3% better in CoNLL<F1 than the disjoint baseline.",
        "table": "+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>                                      | R       | MUC P   | <italic>F</italic>1   | B3 P    | CEAF-<italic>e</italic> P   | CoNLL <italic>F</italic>1   |\n+====+=========================================================+=========+=========+=======================+=========+=============================+=============================+\n|  0 | <bold>Baselines</bold>                                  | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  1 | Cluster+Lemma                                           | 75.5    | 79.9    | 73.6                  | 85      | 71.7                        | 76.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  2 | CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | -       | 75      | 64                    | 78      | -                           | 73                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  3 | KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 71      | 71      | 69                    | 67      | 67                          | 69                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  4 | Cluster+KCP                                             | 77.4    | 79.3    | 71.5                  | 87.2    | 66.4                        | 73.6                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  5 | <bold>Model Variants</bold>                             | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  6 | Disjoint                                                | 80.3    | 83.6    | 75.9                  | 86      | 71.9                        | 78.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  7 | Joint                                                   | 81      | 84.5    | 77.3                  | 85.1    | 73.8                        | <bold>79.5</bold>           |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Moreover, the model using TVMAX in the final attention layer achieves the highest accuracy, showing that features obtained using the TVMAX transformation are a better complement to bounding box features.",
        "evidence": "The results show that the features obtained using the TVMAX transformation are better than the features obtained using softmax or sparsemax in all the tasks. Moreover, the results are slightly superior when the sparsemax transformation is used in the final attention layer. This shows that features obtained using the TVMAX transformation are a better complement to bounding box features.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "Syntactic part-ofspeech features do not obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section do not hold high predictive accuracy for the task.",
        "evidence": "The syntactic part-ofspeech features obtained higher performance than any sentiment or complaint feature group. The best performing sentiment feature group was the Volkova & Bachrach (V&B) which obtained 60.0 F1. This was still lower than the best performing complaint feature group, which obtained 55.2 F1. The Volkova & Bachrach (VADER) features – which obtained 68.0 F1 – were also the best performing sentiment features for the task. On the other hand, the complaint specific features obtained slightly lower performance than the sentiment features, showing the previous section does not hold high predictive accuracy for the task.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly well compared to TF-IDF.",
        "evidence": "The opinion distance method OD performs particularly well compared to TF-IDF, SENT2Vec, Doc2Vec and BERT. This is especially pronounced for the \"Hydroelectric Dams\" dataset, where the opinion distance OD achieves an average improvement of 6% compared to TF-IDF, and 10% compared to SENT2Vec and Doc2Vec.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Using only one attention head, thereby attending to only one context position at once, does not degrade the performance to less than the performance of 10 heads using the standard finetuning scheme.",
        "evidence": "This claim can be used to conclude that applying a single attention head to the attention mechanism does not degrade the performance of the TiDCS model. We measured the performance of the TiDCS model with the standard finetuning scheme. In particular, we divided the attention weights of the TiDCS model into six parts, according to the number of heads used in the respective experiments. As can be seen, the performance is significantly degraded when reducing the number of attention heads. This is not surprising, since using only one attention head, one attention head at a time, is not able to degrade the performance to less than the performance of 10 heads using the standard finetuning scheme.",
        "table": "+----+--------------+------------+-------------+----------+\n|    |   # of Heads | Accuracy   |   Val. Loss | Effect   |\n+====+==============+============+=============+==========+\n|  0 |            1 | 89.44%     |      0.2811 | -6.84%   |\n+----+--------------+------------+-------------+----------+\n|  1 |            2 | 91.20%     |      0.2692 | -5.08%   |\n+----+--------------+------------+-------------+----------+\n|  2 |            4 | 93.85%     |      0.2481 | -2.43%   |\n+----+--------------+------------+-------------+----------+\n|  3 |            8 | 96.02%     |      0.2257 | -0.26%   |\n+----+--------------+------------+-------------+----------+\n|  4 |           10 | 96.28%     |      0.2197 | [EMPTY]  |\n+----+--------------+------------+-------------+----------+\n|  5 |           16 | 96.32%     |      0.219  | +0.04    |\n+----+--------------+------------+-------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "it outperforms the baseline on three out of the four test datasets, achieving the best results on Glockner.",
        "evidence": "Table 4 shows the performance of our system on the in-domain and out-of-domain evaluations. [CONTINUE] The first row shows the results for the in-domain evaluations, and the second row the results for the SNLI and Glockner datasets. Our model is clearly better than the baseline, with large improvements on both SNLI and Glockner. Among all the baseline models, ESIM (ELMo) is the smallest one, despite its size. [CONTINUE] Although MQAN is comparable to ESIM on Glockner and SNLI, it does not outperform the baseline on the SICK dataset. [CONTINUE] With the help of coverage information, our model is able to achieve much better results on the Glockner dataset than the baseline.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The first set of results in Table 3 shows that the completely right/left branching baselines dominate the hierarchical right/left branching ones.",
        "evidence": "We focus on the performance on the inter-domain and compare our results with state-of-the-art systems, including non-two-stage baselines, as well as single-stage baselines (one-stage and two-stage). The superiority of the two-stage approach has been validated through the experiments in Wang et al.",
        "table": "+----+--------------------------------+--------------------------------+--------------------------------+\n|    | Approach                       | RST-DTtest                     | Instr-DTtest                   |\n+====+================================+================================+================================+\n|  0 | Right Branching                | 54.64                          | 58.47                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  1 | Left Branching                 | 53.73                          | 48.15                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  2 | Hier. Right Branch.            | [BOLD] 70.82                   | [BOLD] 67.86                   |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  3 | Hier. Left Branch.             | 70.58                          | 63.49                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  4 | [BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  5 | HILDAHernault et al. ( 2010 )  | 83.00                          | —                              |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  6 | DPLPJi and Eisenstein ( 2014 ) | 82.08                          | —                              |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  7 | CODRAJoty et al. ( 2015 )      | 83.84                          | [BOLD] 82.88                   |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  8 | Two-StageWang et al. ( 2017 )  | [BOLD] 86.00                   | 77.28                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  9 | [BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 10 | Two-StageRST-DT                | ×                              | 73.65                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 11 | Two-StageInstr-DT              | 74.48                          | ×                              |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 12 | Two-StageOurs(avg)             | 76.42                          | [BOLD] 74.22                   |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 13 | Two-StageOurs(max)             | [BOLD] 77.24                   | 73.12                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 14 | Human Morey et al. ( 2017 )    | 88.30                          | —                              |\n+----+--------------------------------+--------------------------------+--------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "In Italian, we get a reduction of 91.67% of the gap with respect to English.",
        "evidence": "We see that the difference between German and Italian is not very large, close to 0.05 difference. In Italian, we get a reduction of 91.67% of the gap with respect to English. In German, we get a reduction of 91.67%.",
        "table": "+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|    | [EMPTY]          |   Italian Original |   Italian Debiased |   Italian English | Italian Reduction   |   German Original |   German Debiased |   German English | German Reduction   |\n+====+==================+====================+====================+===================+=====================+===================+===================+==================+====================+\n|  0 | Same Gender      |              0.442 |              0.434 |             0.424 | –                   |             0.491 |             0.478 |            0.446 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  1 | Different Gender |              0.385 |              0.421 |             0.415 | –                   |             0.415 |             0.435 |            0.403 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  2 | difference       |              0.057 |              0.013 |             0.009 | [BOLD] 91.67%       |             0.076 |             0.043 |            0.043 | [BOLD] 100%        |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "The results in Table 5 show that the three types of whitelists perform comparably to each other when the true response is added.",
        "evidence": "The experiments show that the three types of whitelists perform comparably to each other when the true response is added. [CONTINUE] The BLEU scores computed with the frequency and the clustering whitelists are slightly higher than those computed with random whitelists. [CONTINUE] The results in Table 5 show that the three types of whitelists perform comparably to each other when the true response is added. [CONTINUE] The BLEU scores computed with the frequency and the clustering whitelists are slightly higher than those computed with the random whitelists.",
        "table": "+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|    | [BOLD] Whitelist   |   [BOLD] R@1 |   [BOLD] R@3 |   [BOLD] R@5 |   [BOLD] R@10 |   [BOLD] BLEU |\n+====+====================+==============+==============+==============+===============+===============+\n|  0 | Random 10K+        |        0.252 |        0.4   |        0.472 |         0.56  |         37.71 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  1 | Frequency 10K+     |        0.257 |        0.389 |        0.455 |         0.544 |         41.34 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  2 | Clustering 10K+    |        0.23  |        0.376 |        0.447 |         0.541 |         37.59 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  3 | Random 1K+         |        0.496 |        0.663 |        0.728 |         0.805 |         59.28 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  4 | Frequency 1K+      |        0.513 |        0.666 |        0.726 |         0.794 |         67.05 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  5 | Clustering 1K+     |        0.481 |        0.667 |        0.745 |         0.835 |         61.88 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  6 | Frequency 10K      |        0.136 |        0.261 |        0.327 |         0.42  |         30.46 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  7 | Clustering 10K     |        0.164 |        0.292 |        0.36  |         0.457 |         31.47 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  8 | Frequency 1K       |        0.273 |        0.465 |        0.55  |         0.658 |         47.13 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  9 | Clustering 1K      |        0.331 |        0.542 |        0.65  |         0.782 |         49.26 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "This can be observed in both Balanced COPA and Textual Entailment experiments.",
        "evidence": "The claim that BERT and RoBERTa have better performance than BERT-Base-NSP is well supported by the results in both datasets.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "On the NYT11 dataset, m = 5 gives the best performance.",
        "evidence": "The claim-tracking module closes the gap between our approach and previous state-of-the-art approaches. In order to get more insight into how our approach works, we further trained our model on the NYT10 and NYT11 datasets and evaluate its performance on the test datasets. We take the models trained on NYT10 dataset as our baseline, and the ones trained on NYT11 dataset as our meta-learner. From the results, we can see that m=5 has the best performance on both datasets. It also shows that m=3 and m=4 have similar performance on the NYT10 dataset. These results clearly demonstrate that our meta-learner can work well on datasets with many entities. Furthermore, the performance of our approach on the NYT11 dataset is slightly lower than the one on NYT10 dataset. We believe that this is due to the fact that NYT10 has around 55.9% overlapping tuples between the training and test data while NYT11 has only 41.7% of overlapping tuples.",
        "table": "+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|    |   [ITALIC] m |   NYT10 Prec. |   NYT10 Rec. | NYT10 F1     |   NYT11 Prec. |   NYT11 Rec. | NYT11 F1     |\n+====+==============+===============+==============+==============+===============+==============+==============+\n|  0 |            1 |         0.541 |        0.595 | [BOLD] 0.566 |         0.495 |        0.621 | 0.551        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  1 |            2 |         0.521 |        0.597 | 0.556        |         0.482 |        0.656 | 0.555        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  2 |            3 |         0.49  |        0.617 | 0.547        |         0.509 |        0.633 | 0.564        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  3 |            4 |         0.449 |        0.623 | 0.522        |         0.507 |        0.652 | [BOLD] 0.571 |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  4 |            5 |         0.467 |        0.609 | 0.529        |         0.488 |        0.677 | 0.567        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "The high AUC indicates that our model can easily distinguish between the true response and negative responses.",
        "evidence": "The fine-tuning results in the table are obtained by McNemar’s test. We use the area under curve (AUC) as the evaluation metric. The high AUC indicates that our model can easily distinguish between the true response and negative responses. It is evident that the performance of our model surpasses all previous state-of-the-art metrics. Especially, the AUC@0.1 measure considers only the top-1 response, yet our model can achieve a very high AUC@0.977. This indicates that our model can easily distinguish between the true and negative responses.",
        "table": "+----+-----------------+---------------------+---------------+\n|    | [BOLD] Metric   |   [BOLD] Validation |   [BOLD] Test |\n+====+=================+=====================+===============+\n|  0 | AUC             |               0.991 |         0.977 |\n+----+-----------------+---------------------+---------------+\n|  1 | AUC@0.1         |               0.925 |         0.885 |\n+----+-----------------+---------------------+---------------+\n|  2 | AUC@0.05        |               0.871 |         0.816 |\n+----+-----------------+---------------------+---------------+\n|  3 | AUC@0.01        |               0.677 |         0.63  |\n+----+-----------------+---------------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "However, this reflects the high variability of the test set.",
        "evidence": "The results show that the system’s performance is significantly better than the two baselines. However, this reflects the high variability of the test set. The difference is substantial between the in-scope and out-ofscope predictions.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Also, our data augmentation technique (NO-TRANSLATIONS) seem to have far smaller impact on the final score then we expected.",
        "evidence": "The summarization dataset is relatively small (less than 100,000 sentence pairs) so the results are not directly comparable. But the results with our data augmentation technique seem to have a much smaller impact on the final score then we expected.",
        "table": "+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|    | ID LSTM-800        |   5-fold CV 70.56 | Δ 0.66   | Single model 67.54   | Δ 0.78   | Ensemble 67.65   | Δ 0.30   |\n+====+====================+===================+==========+======================+==========+==================+==========+\n|  0 | LSTM-400           |             70.5  | 0.60     | [BOLD] 67.59         | 0.83     | [BOLD] 68.00     | 0.65     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  1 | IN-TITLE           |             70.11 | 0.21     | [EMPTY]              | [EMPTY]  | 67.52            | 0.17     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  2 | [BOLD] SUBMISSION  |             69.9  | –        | 66.76                | –        | 67.35            | –        |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  3 | NO-HIGHWAY         |             69.72 | −0.18    | 66.42                | −0.34    | 66.64            | −0.71    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  4 | NO-OVERLAPS        |             69.46 | −0.44    | 65.07                | −1.69    | 66.47            | −0.88    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  5 | LSTM-400-DROPOUT   |             69.45 | −0.45    | 65.53                | −1.23    | 67.28            | −0.07    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  6 | NO-TRANSLATIONS    |             69.42 | −0.48    | 65.92                | −0.84    | 67.23            | −0.12    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  7 | NO-ELMO-FINETUNING |             67.71 | −2.19    | 65.16                | −1.60    | 65.42            | −1.93    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+",
        "label": "supports"
    },
    {
        "claim": "For all batch sizes, the training throughput on the linear dataset is the highest, while the throughput on the balanced dataset is the lowest.",
        "evidence": "The claim that the Balanced dataset is easier is supported by the fact that the amount of information carried by the dataset is significantly higher than the amount of information carried by the linear dataset. For example, when training a model on a batch size of 1, the estimated throughput is 27.3 processing an utterance every 7.6 seconds, while the estimated throughput is only 7.6 processing an utterance every 2.09 seconds. This leads to the conclusion that the information carried by the dataset is significant, and that it would be interesting to increase the batch size if the models are able to extract sufficient amount of information from the dataset.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The Patt model was able to generate relations for all terms in the Europarl and TED Talks corpora, as evidenced by the metrics in Table 6.",
        "evidence": "This was not true for the NumberRels and AverageTree relations, which were only available for the TED talks corpus. From this list, we see that DomSum was able to generate relations for 94% of the terms in the Europarl and TED talks corpora. Combining the two representations, we were able to increase the coverage by 25% to 100% for the Dsim and TF relations. The number of relations that can be generated increased drastically from 13% to 15% for the MaxDepth and DocSubtitles corpora, and from 3% to 6% for the TG-subtitles. Also, DomSum was able to generate relations for TED talks without distinguishing between depth and width, though the difference was not statistically significant.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 957    | 1,000  | 1,000  | 1,000 | 1,000 | 836      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 44     | 1      | 1      | 1     | 1     | 43       | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,588  | 1,025  | 1,028  | 1,185 | 1,103 | 1,184    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 21     | 921    | 901    | 788   | 835   | 8        | 15       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 921    | 901    | 788   | 835   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 11.82  | 921    | 901    | 788   | 835   | 3.05     | 8.46     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 1.78   | 1      | 1      | 1     | 1     | 2.62     | 1.77     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 20     | 2      | 3      | 4     | 3     | 88       | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.99   | 1.03   | 1.03   | 1.19  | 1.10  | 4.20     | 2.38     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 476    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 164    | 2      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 521    | 1,029  | 1,331  | 3,025 | 3,438 | 3,802    | 1,009    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 16     | 915    | 658    | 454   | 395   | 118      | 12       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 913    | 658    | 454   | 395   | 110      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 5.82   | 914    | 658    | 454   | 395   | 112.24   | 5.95     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.75   | 1      | 1      | 1     | 1     | 1.05     | 2.02     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 25     | 2      | 77     | 13    | 12    | 66       | 98       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.83   | 1.03   | 1.36   | 3.03  | 3.44  | 6.64     | 2.35     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "refutes"
    },
    {
        "claim": "We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets.7 As Table 4 shows, previous models perform similarly on both subsets, with the exception of Sasaki et al.",
        "evidence": "As can be seen, BERT and RoBERTa perform similarly on both subsets.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "Consequently, CMOW-R does not outperform CMOW-C on 10 out of 11 supervised downstream tasks. On average over all downstream tasks, the relative improvement is not 20.8%.",
        "evidence": "This claim does not hold water. Although our method is very close to COW-R, it does not outperform it on any of the 11 tasks.",
        "table": "+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method   | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+==========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CMOW-C   | 85.9        | 72.1        | 69.4        | 87.0        | [BOLD] 71.9 | 85.4        | 74.2        | 73.8        | 37.6        | 54.6        | 71.3        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW-R   | [BOLD] 87.5 | [BOLD] 73.4 | [BOLD] 70.6 | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | [BOLD] 77.2 | [BOLD] 74.7 | [BOLD] 37.9 | [BOLD] 56.5 | [BOLD] 76.2 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | CBOW-C   | [BOLD] 90.0 | [BOLD] 79.3 | [BOLD] 74.6 | [BOLD] 87.5 | [BOLD] 72.9 | 85.0        | [BOLD] 80.0 | 78.4        | 41.0        | 60.5        | [BOLD] 79.2 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | CBOW-R   | [BOLD] 90.0 | 79.2        | 74.0        | 87.1        | 71.6        | [BOLD] 85.6 | 78.9        | [BOLD] 78.5 | [BOLD] 42.1 | [BOLD] 61.0 | 78.1        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "WN-N shows low coverage containing many low-frequency members.",
        "evidence": "The results show that lemmatization decreases the coverage while vn and wn-n have almost no effect.",
        "table": "+----+----------+------+--------+--------+\n|    | target   |   VN |   WN-V |   WN-N |\n+====+==========+======+========+========+\n|  0 | type     |   81 |     66 |     47 |\n+----+----------+------+--------+--------+\n|  1 | x+POS    |   54 |     39 |     43 |\n+----+----------+------+--------+--------+\n|  2 | lemma    |   88 |     76 |     53 |\n+----+----------+------+--------+--------+\n|  3 | x+POS    |   79 |     63 |     50 |\n+----+----------+------+--------+--------+\n|  4 | shared   |   54 |     39 |     41 |\n+----+----------+------+--------+--------+",
        "label": "supports"
    },
    {
        "claim": "with the same model and decoding scheme, for the 5-action experiments, data augmentation improves the Action BLEU by 0.2 and the Slot F1 by 1.89 on average.",
        "evidence": "The results are shown in Table 1. [CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the worse performance and benefits less from data augmentation comparing to our proposed domain-aware multi-decoder network,",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "acoustic supervision (27.7%) and multi-task learning (26.1%) show lower WER than minimizing DCE (31.1%) and FSEGAN (29.1%)).",
        "evidence": "The claim makes the claim that multi-task learning with acoustic supervision and multi-task learning with FSEGAN show lower WER than minimizing DCE (31.1% of the network) and multi-task learning with FSEGAN (29.1% of the network). We also observed that acoustic supervision (27.7%) and multi-task learning (26.1%) show lower WER than minimizing DCE (31.1%) and FSEGAN (29.1%).",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 38.4        | 0.958        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 41.0        | 0.775        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 31.1        | [BOLD] 0.392 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 29.1        | 0.421        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 27.7        | 0.476        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 9.3         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%.",
        "evidence": "The parser’s performance is significantly worse than the stagblcc parser. The difference is particularly striking on the essay level where the parsers often fail to learn. In comparison, the LSTM-parser is very much better than the stagblcc parser. This is not surprising as the LSTM parser is trained end-to-end with no pseudo-parallel data generated during inference.",
        "table": "+----+-----------+------------+---------------+\n|    | [EMPTY]   | STagBLCC   | LSTM-Parser   |\n+====+===========+============+===============+\n|  0 | Essay     | 60.62±3.54 | 9.40±13.57    |\n+----+-----------+------------+---------------+\n|  1 | Paragraph | 64.74±1.97 | 56.24±2.87    |\n+----+-----------+------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "However, our proposed method does not outperform the original GloVe embeddings.",
        "evidence": "We present the results of our method in word embedding analysis. As it can be seen, our proposed method does not outperform the original GloVe embeddings. This shows that although our proposed method does not outperform the original GloVe embeddings, the semantic and syntactic information our proposed model does provide is slightly better than the original GloVe embeddings.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop [CONTINUE] erties of sentences than CBOW.",
        "evidence": "We first compare between different variants of CMOW and their variants H-MOW. We first note that, in terms of perplexity, the proposed CMOW embeddings either achieve comparable or better performance than CBOW. H-MOW is slightly better than H-CBOW on noun, verb, and prepositional tasks, while CBOW/784 performs slightly better on somo and wc. Overall, we observe that CMOW embeddings consistently outperform CBOW embeddings across all tasks, which is consistent with the results of \\newciteMoYu2014abstract.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "On the other hand, choosing the best hypernym did not work very well for DocSub which obtained the lowest precision for the Portuguese corpora.",
        "evidence": "For the English language, finding the best hypernym combination was not very good. In fact, we obtained the lowest precision for the Portuguese corpora using a weighted combination. We believe that this is due to the fact that the Portuguese language benefits from similarity measures which are weaker than TF and DFS.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS |     TF |     DF | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+========+========+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1038 | 0.017  | 0.049  | 0.0641 | 0.0641 | 0.0613        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.041  | 0.027  | 0.027  | 0.1154        |   0.0661 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.6185        | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.6308        | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 |   0.5295 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  4 | R         | EN     | Europarl  | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0011        | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 |   0.0018 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0012        | 0.0008 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0016 |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0003        | 0.0009 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0017 |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  8 | F         | EN     | Europarl  | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0022        | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 |   0.0036 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0024        | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 |   0.0023 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0005        | 0.0018 | 0.0018 | 0.002  | 0.0021 | [BOLD] 0.0034 |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
        "evidence": "The results show that for all of the datasets, the classifier trained upon their data is more likely to flag black-aligned tweets as sexism than those trained without. The exception is the Waseem and Hovy (2016) classifier, which, on the Waseem dataset, is less likely to flag black-aligned tweets as sexism.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.01  |                0.01  |       -0.632 | [EMPTY]      |                                  0.978 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.963 |                0.944 |       20.064 | ***          |                                  1.02  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.011 |                0.011 |       -1.254 | [EMPTY]      |                                  0.955 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.349 |                0.29  |       28.803 | ***          |                                  1.203 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.012 |                0.012 |       -0.162 | [EMPTY]      |                                  0.995 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.017 |                0.015 |        4.698 | ***          |                                  1.152 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.988 |                0.991 |       -6.289 | ***          |                                  0.997 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.099 |                0.091 |        6.273 | ***          |                                  1.091 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.074 |                0.027 |       46.054 | ***          |                                  2.728 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.925 |                0.968 |      -41.396 | ***          |                                  0.956 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.01  |                0.01  |        0     | [EMPTY]      |                                  1     |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "As we can observe, it seems that clustering semantically related terms does not necessarily increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected.",
        "evidence": "As it can be seen, clustering semantically related terms does not increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment).",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "G2S models also generate sentences that contradict the reference sentences less.",
        "evidence": "From the table, we see that G2S models also generate sentences that match the reference sentences less frequently than S2S models, which shows that G2S models are able to leverage the pretrained word embeddings to enable generating “giant” sentences that are similar to their source sentences. In addition, the results also show that the gap between Gen ⇒Ref is smaller than that in the reverse direction, which suggests that G2S models can more effectively make use of the pretrained word embeddings to enhance the syntactic information.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Table 8: The contribution of each unsupervised learning for detecting negation triggers.",
        "evidence": "The contribution of each unsupervised learning is investigated by using the Wilcoxon signed-rank test. From the table, we can see that the method proposed by seq2seq can achieve the best performance against the baselines, with the large gap between the in-scope and out-ofscope methods.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Dual2seq is consistently better than the other systems under all three metrics, [CONTINUE] Dual2seq is better than both OpenNMT-tf and Transformer-tf .",
        "evidence": "We see that Dual2seq is consistently better than the other systems under all three metrics. More importantly, the BLEU and TER scores of Dual2seq are better than both OpenNMT-tf and Transformer-tf. It is worth mentioning that the OpenNMT-tf and Transformer-tf have similar BLEU scores, as they are both trained on the same data. However, Dual2seq has a better BLEU score than these two systems. This is another proof of the effectiveness of the proposed model.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "We empirically found that self-attention was not the most efficient in the 3rd stage.",
        "evidence": "We showed that self-attention was not the most efficient in the 3rd stage. We hypothesize that this is because the positional information from the 3-layer self-attention layer was sufficient. This is also consistent with the observation made by Anderson et al.",
        "table": "+----+------------------------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34                         | [BOLD] Eval set %   | [BOLD] #param   |\n+====+==========================================+=====================+=================+\n|  0 | Baseline (No SA)Anderson et al. ( 2018 ) | 55.00               | 0M              |\n+----+------------------------------------------+---------------------+-----------------+\n|  1 | SA (S: 1,2,3 - B: 1)                     | 55.11               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  2 | SA (S: 1,2,3 - B: 2)                     | 55.17               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  3 | [BOLD] SA (S: 1,2,3 - B: 3)              | [BOLD] 55.27        | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "In contrast, DAN does not always mask out punctuation and determiners using words indicative of the class label, as evidenced by the example sentence in the table.",
        "evidence": "DAN is not always successful in masking out punctuation and determiners using words indicative of the class label. One example is shown in the table.. where the CNN model correctly predicted a turn for the screenplay heroine. However, the model predicted a turn for the screenplay heroine would be “turned on a on (the edge)”, which is inconsistent with the label “turn on a on” in the original DAN paper, and likely would bemasked.",
        "table": "+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|    | Orig   | <u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it .                                                               |\n+====+========+===========================================================================================================================================================+\n|  0 | DAN    | <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate |\n+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|  1 | CNN    | she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ”                                                                            |\n+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|  2 | RNN    | <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it .                                          |\n+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
        "evidence": "Our joint model still does not achieve the state-of-the-art performance on all the metrics. The exception is the CoNLL F1 score. This score is 3.8 points lower than the strong lemma baseline.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under Meteor (around 5 points) is greater than that under BLEU (around 3 points).",
        "evidence": "We also trained different systems on the NC-v11 subset. We can see that, without trained on the NC-v11 dataset, the gap between Seq2seq and Dual2seq under METEOR is greater than that under BLEU. For example, the gap under BLEU between Seq2seq and Dual2seq under 5 points is around 3 points, while the gap under TER is around 3 points for both systems. This shows that the architectures have similar performance on full datasets, but that Dual2seq performs better on small datasets like NC-v11.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "The proposed method achieves the competitive accuracies using single vector compared to several vector models.",
        "evidence": "The results show that our proposed method achieves comparable results compared to several deep learning models. Specifically, word2vec model obtains 77.34% and 78.26% accuracies, respectively, which is close to the best accuracies achieved by the OIWE-IPG model and the Spine model.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "The resulting cross-dataset improvements on the SNLI and Glockner datasets are not larger than those on the SICK dataset.",
        "evidence": "This claim is quite strong. In particular, the resulting cross-dataset improvements on the SNLI and Glockner datasets are at least 4-6% larger than those on the SICK dataset.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "refutes"
    },
    {
        "claim": "In the en-de News/TED task (Table 4), all fine-tuning schemes give similar improvements on TED.",
        "evidence": "This claim can be used to explain why successive fine-tuning schemes give similar improvements on TED. We found that fine-tuning first with the News model improves results over fine-tuning with no-reg and L2 for all tasks. This suggests that fine-tuning News first with the language model objective is good for fine-tuning on its own. However, fine-tuning then with the EWC loss gives similar improvements on TED. This shows that EWC loss helps first with the language model objective and then with the EWC loss. We also found that L2 and EWC are not helpful for News and TED.",
        "table": "+----+-----------+--------------------------+---------------+--------------+-------------+\n|    |   [EMPTY] | [BOLD] Training scheme   | [BOLD] News   | [BOLD] TED   | [BOLD] IT   |\n+====+===========+==========================+===============+==============+=============+\n|  0 |         1 | News                     | 37.8          | 25.3         | 35.3        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  1 |         2 | TED                      | 23.7          | 24.1         | 14.4        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  2 |         3 | IT                       | 1.6           | 1.8          | 39.6        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  3 |         4 | News and TED             | 38.2          | 25.5         | 35.4        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  4 |         5 | 1 then TED, No-reg       | 30.6          | [BOLD] 27.0  | 22.1        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  5 |         6 | 1 then TED, L2           | 37.9          | 26.7         | 31.8        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  6 |         7 | 1 then TED, EWC          | [BOLD] 38.3   | [BOLD] 27.0  | 33.1        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  7 |         8 | 5 then IT, No-reg        | 8.0           | 6.9          | 56.3        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  8 |         9 | 6 then IT, L2            | 32.3          | 22.6         | 56.9        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  9 |        10 | 7 then IT, EWC           | 35.8          | 24.6         | [BOLD] 57.0 |\n+----+-----------+--------------------------+---------------+--------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "G2S-GAT has a better performance in handling graphs with node out-degrees higher than 9.",
        "evidence": "From the table, we can see that: 1) G2S-GAT has a better performance than S2S in handling graphs with node out-degrees higher than 9.5. 2) G2S-GAT is 3% and 5.7% better than S2S in handling 0-20 δ and 20-50 δ respectively. 3) The performance gap between G2S-GAT and S2S is only 5.1% in handling 0-3 δ. 4) On the other hand, the performance gap between G2S-GGNN and G2S-GAT is 26.6% in handling 0-3 δ and 22.5% in handling 0-6 δ. 5) We can see the performance gap between G2S-GGNN and G2S-GAT is also larger. G2S-GGNN is 10.3% and G2S-GAT is 6.7% better than S2S in handling 0-3 δ.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "we can see that our classifier does not quite achieve the results of the BiLSTM+scope, but is more robust in extracting the expression.",
        "evidence": "Surprisingly, our method performs even worse than the BiLSTM+scope with respect to the accuracy in extracting the expression. We hypothesize that the poor performance is due to the lack of direct comparison between the training set and the evaluation data.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The Wiener filtering method shows lower DCE, but higher WER than no enhancement.",
        "evidence": "The Wiener filtering method shows lower DCE than no enhancement.",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 38.4        | 0.958        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 41.0        | 0.775        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 31.1        | [BOLD] 0.392 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 29.1        | 0.421        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 27.7        | 0.476        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 9.3         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "we see that superficial cues for COPA are also significant for SB-COPA, showing that SB-COPA mirrors our human intuitions at least for this phenomenon.",
        "evidence": "At least for this phenomenon, we see that the superficial cues for COPA are also significant.",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "This is evident from the insignificant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters.",
        "evidence": "This is evident from the insignificant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters.",
        "table": "+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|    | [EMPTY]                   | Difference Function   | Seanad Abolition   | Video Games   | Pornography   |\n+====+===========================+=======================+====================+===============+===============+\n|  0 | OD-parse                  | Absolute              | 0.01               | -0.01         | 0.07          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  1 | OD-parse                  | JS div.               | 0.01               | -0.01         | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  2 | OD-parse                  | EMD                   | 0.07               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  3 | OD                        | Absolute              | [BOLD] 0.54        | [BOLD] 0.56   | [BOLD] 0.41   |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  4 | OD                        | JS div.               | 0.07               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  5 | OD                        | EMD                   | 0.26               | -0.01         | 0.01          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  6 | OD (no polarity shifters) | Absolute              | 0.23               | 0.08          | 0.04          |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  7 | OD (no polarity shifters) | JS div.               | 0.09               | -0.01         | -0.02         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+\n|  8 | OD (no polarity shifters) | EMD                   | 0.10               | 0.01          | -0.01         |\n+----+---------------------------+-----------------------+--------------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012.",
        "evidence": "Compared to the LR-all-features baseline, the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5), and AUC by 0.012.",
        "table": "+----+---------------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model                    | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+=================================+==============+=============+==============+\n|  0 | Most Frequent Class             | 64.2         | 39.1        | 0.500        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  1 | LR-All Features – Original Data | 80.5         | 78.0        | 0.873        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  2 | Dist. Supervision + Pooling     | 77.2         | 75.7        | 0.853        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  3 | Dist. Supervision + EasyAdapt   | [BOLD] 81.2  | [BOLD] 79.0 | [BOLD] 0.885 |\n+----+---------------------------------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "In addition, the noise-aware model is more stable and therefore requires fewer iterations to converge.",
        "evidence": "The results show that the alignment-aware model is 1.3% better than the average RNN in all languages. In addition, the iteration number of the noise-aware model is slightly fewer than the standard RNN, which suggests that it takes fewer iterations to converge. This makes the model more stable and therefore enables faster prototyping, which is a critical factor in low-resource scenarios where parallel data is scarce.",
        "table": "+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|    | Method                | En→It best   | En→It avg    |   En→It iters | En→De best   | En→De avg    |   En→De iters | En→Fi best   | En→Fi avg    |   En→Fi iters | En→Es best   | En→Es avg    |   En→Es iters |\n+====+=======================+==============+==============+===============+==============+==============+===============+==============+==============+===============+==============+==============+===============+\n|  0 | Artetxe et al., 2018b | [BOLD] 48.53 | 48.13        |           573 | 48.47        | 48.19        |           773 | 33.50        | 32.63        |           988 | 37.60        | 37.33        |           808 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|  1 | Noise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 |           471 | [BOLD] 49.67 | [BOLD] 48.89 |           568 | [BOLD] 33.98 | [BOLD] 33.68 |           502 | [BOLD] 38.40 | [BOLD] 37.79 |           551 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "This shows that more attention heads, thereby attending to multiple different contexts at once, does not necessarily lead to state-of-the-art results.",
        "evidence": "Attention weights. The results show that attending to multiple contexts at once, regardless of the number of attention heads, does not lead to state-of-the-art results. For example, following the loss function, we attend to the top-1 context (i.e. layer 1) with the accuracy of 89.44% and decrease by 6.84% after only using 2 attention heads. We also attend to the top-8 and bottom-1 contexts with the accuracy of 96.02% and decrease by 2.43% and 0.26%, respectively. Although these results show that more attention heads may not necessarily lead to state-of-the-art results, they are encouraging nevertheless.",
        "table": "+----+--------------+------------+-------------+----------+\n|    |   # of Heads | Accuracy   |   Val. Loss | Effect   |\n+====+==============+============+=============+==========+\n|  0 |            1 | 89.44%     |      0.2811 | -6.84%   |\n+----+--------------+------------+-------------+----------+\n|  1 |            2 | 91.20%     |      0.2692 | -5.08%   |\n+----+--------------+------------+-------------+----------+\n|  2 |            4 | 93.85%     |      0.2481 | -2.43%   |\n+----+--------------+------------+-------------+----------+\n|  3 |            8 | 96.02%     |      0.2257 | -0.26%   |\n+----+--------------+------------+-------------+----------+\n|  4 |           10 | 96.28%     |      0.2197 | [EMPTY]  |\n+----+--------------+------------+-------------+----------+\n|  5 |           16 | 96.32%     |      0.219  | +0.04    |\n+----+--------------+------------+-------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "Our model (OURS) does not obtain substantial gains in accuracy over the baselines across all three target aspects.",
        "evidence": "Our model (OURS) does not obtain substantial gains in accuracy over the baselines across all three target aspects.",
        "table": "+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source            | Target      |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+===================+=============+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer aroma+palate | Beer look   | 74.41 |     74.83 |     74.94 |    72.75 |        76.41 | [BOLD] 79.53 |     80.29 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look+palate  | Beer aroma  | 68.57 |     69.23 |     67.55 |    69.92 |        76.45 | [BOLD] 77.94 |     78.11 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look+aroma   | Beer palate | 63.88 |     67.82 |     65.72 |    74.66 |        73.4  | [BOLD] 75.24 |     75.5  |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "refutes"
    },
    {
        "claim": "In comparison, GDPL is still comparable with ACER and PPO, obtains a better match rate, and even achieves higher task success.",
        "evidence": " GDPL is still comparable with ACER and PPO on task success, obtains a better match rate, and even achieves higher task success than ALDM.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is only 2.9 percent (71.9% vs. 69.0%).",
        "evidence": "The claim is that BERT-large is already very competitive compared to previous work. It improves accuracy on the Easy subset by 8.6%, but the improvement on the Hard subset is only 2.9%. This increase can be attributed to high accuracy on the Easy subset, which is almost entirely attributed to the high quality of the unlabeled dataset.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "This reflects the dialog efficiency of all methods but ACER decreases with the time extension, which is the opposite with human’s preference.",
        "evidence": "ALDM outperforms PPO in all aspects (sign test, p-value < 0.01) which is consistent with human’s preference.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that our model exhibits the best performances.",
        "evidence": "From the table, we observe that: (1) Our model exhibits the best performances. This is because our model exploits the relations between words and entities in our dataset; (2) The entity-centric model (Rank+ExAtt) performs better than the entity-centric model (PCNN+ATT); (3) For the PCNN+ATT (1) model, the entity vectors are obtained by concatenating PCNN embeddings and the relation vectors obtained by concatenating PCNN embeddings and the relation vectors obtained by training the ExAtt model. Since the PCNN+ATT (1) model only uses the entity vectors for information retrieval, it is not suitable to utilize the information from the entity-centric model.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "supports"
    },
    {
        "claim": "G2S models generate sentences that contradict the reference sentences more.",
        "evidence": "From the table, we see that G2S models generate sentences that contradict the reference sentences more than the S2S models in all the tasks. Moreover, the gap in generation quality between G2S and S2S is more substantial in the “Gen ⇒ Gen” setting, which shows that our G2S models have learned to generate content that agrees more with the reference sentences. In Fig. the results show that G2S-GAT and G2S-GGNN outperform S2S in sentence generation. Moreover, we can see that the gap between G2S-GAT and G2S-GGNN is more significant in the “Gen ⇒ Gen” setting than in the “Gen → En” setting. This shows that our proposed text encoder help the model generate content that agrees more with the reference sentences.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "the overall results suggest that the combination of our negative opinion words with external sentiment lexicon outperform other methods",
        "evidence": "show that the antonym based learned representations are more useful for sentiment task as compared to prefixing with NOT_. The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg. baseline with F1 scores improving from 0.72 to 0.78 for positive sentiment and from 0.83 to 0.87 for negative sentiment.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] For Marian amun, the effect is negligible as we can see in Table 3.",
        "evidence": "The first row in the table shows the results without labels on three datasets. As can be seen, adding our domain-tuned model to the baseline model (Subs1MH+) has little effect on the three datasets. The second row shows the results of our model being improved in almost every case. In particular, we observe that our model with domain-tuned labels on Flickr17 and MSCOCO17 has the best results. Although the effect of our model is almost negligible in the case of Marian AMUN, it is much stronger in the case of SQuAD2.0. We believe that this is because the SQuAD2.0 model is already very strong enough, with the only difference being the approximate size of the visual features.",
        "table": "+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|    | [EMPTY]   | en-fr                                | flickr16    | flickr17    | mscoco17    |\n+====+===========+======================================+=============+=============+=============+\n|  0 | A         | subs1M [ITALIC]  [ITALIC] H+MS-COCO  | 66.3        | 60.5        | 52.1        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  1 | A         | +domain-tuned                        | 66.8        | 60.6        | 52.0        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  2 | A         | +labels                              | [BOLD] 67.2 | 60.4        | 51.7        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  3 | T         | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | 66.9        | 60.3        | [BOLD] 52.8 |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  4 | T         | +labels                              | [BOLD] 67.2 | [BOLD] 60.9 | 52.7        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  5 | [EMPTY]   | en-de                                | flickr16    | flickr17    | mscoco17    |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  6 | A         | subs1M [ITALIC]  [ITALIC] H+MS-COCO  | 43.1        | 39.0        | 35.1        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  7 | A         | +domain-tuned                        | 43.9        | 39.4        | 35.8        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  8 | A         | +labels                              | 43.2        | 39.3        | 34.3        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n|  9 | T         | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | [BOLD] 44.4 | 39.4        | 35.0        |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+\n| 10 | T         | +labels                              | 44.1        | [BOLD] 39.8 | [BOLD] 36.5 |\n+----+-----------+--------------------------------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "However, our summary is often significantly longer than the actual reference summary.",
        "evidence": "Our summaries are often significantly longer than the actual reference summary. In order to avoid large-scale training errors, we use human evaluation. We randomly sampled 200 items and each item includes an input and four outputs that shuffled to anonymize model identities. Then we launch a crowd-sourcing online study, asking 10 evaluators to rate the readability of each summary. Each item is rated by five evaluators. We use the ground truth summary as the gold standard for evaluating our approach. The results show that our summaries are both significantly shorter than the reference summaries and the human-written reference summaries are often closer. However, our summaries are also more readable.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "For example, on AMR17, the ensemble model of Seq2SeqB is 1 BLEU point higher than the single DCGCN model.",
        "evidence": "Beck et al. In the AMR17 dataset, due to the extremely limited training data, the single GNN2SEQ model is the best. Our model improves over the GNN2SEQ in all metrics. For example, on the metric of BLEU-1, our model improves over the best single model by 1.9 points. Due to the very small size of the training data in AMR17, all models suffer from low recall and the ensemble model of DCGCN is able to achieve much better BLEU score.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: [CONTINUE] as in Eq.",
        "evidence": "As can be seen, the very deep model substantially outperforms the baseline and the results continue to improve as the level of parser increases. The best result is achieved using the C-F1 measure, though the difference for the R-F1 measure is not statistically significant.",
        "table": "+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]                                         | C-F1 100%          | C-F1 50%           | R-F1 100%          | R-F1 50%           | F1 100%            | F1 50%             |\n+====+=================================================+====================+====================+====================+====================+====================+====================+\n|  0 | Y-3                                             | 49.59              | 65.37              | 26.28              | 37.00              | 34.35              | 47.25              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  1 | Y-3:Y<italic>C</italic>-1                       | 54.71              | 66.84              | 28.44              | 37.35              | 37.40              | 47.92              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  2 | Y-3:Y<italic>R</italic>-1                       | 51.32              | 66.49              | 26.92              | 37.18              | 35.31              | 47.69              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  3 | Y-3:Y<italic>C</italic>-3                       | <bold>54.58</bold> | 67.66              | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold> |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  4 | Y-3:Y<italic>R</italic>-3                       | 53.31              | 66.71              | 26.65              | 35.86              | 35.53              | 46.64              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  5 | Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95              | <bold>67.84</bold> | 27.90              | 39.71              | 36.54              | 50.09              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  6 | Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55              | 67.60              | 28.30              | 38.26              | 37.26              | 48.86              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "Again, one possible explanation is that cleaning the missing slots provided more complex training examples.",
        "evidence": "It can be seen that the performance of TGen− is much worse than TGen and SC-LSTM only on original test sets. This is very clear from the conclusion by [1-1] that the reason for the degradation of performance is due to the lack of slots in the original model. Although the performance can be improved when the slots are missing, we think the reason for this degradation is lack of sufficient training examples. At least we have examples to train on for a long time.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The Transformer performs best in terms of R-1 while Hi-MAP does not outperform it on R-2 and R-SU.",
        "evidence": "Our model outperforms PG-MMR when trained with the same number of parameters as PG-original, and is comparable to CopyTransformer when trained with less data. In particular, Hi-MAP has a much better ROUGE-1 score than PG-MMR, and is also competitive with CopyTransformer.",
        "table": "+----+------------------------------------------+--------------+--------------+---------------+\n|    | [BOLD] Method                            | [BOLD] R-1   | [BOLD] R-2   | [BOLD] R-SU   |\n+====+==========================================+==============+==============+===============+\n|  0 | First-1                                  | 26.83        | 7.25         | 6.46          |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  1 | First-2                                  | 35.99        | 10.17        | 12.06         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  2 | First-3                                  | 39.41        | 11.77        | 14.51         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  3 | LexRank Erkan and Radev ( 2004 )         | 38.27        | 12.70        | 13.20         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  4 | TextRank Mihalcea and Tarau ( 2004 )     | 38.44        | 13.10        | 13.50         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  5 | MMR Carbonell and Goldstein ( 1998 )     | 38.77        | 11.98        | 12.91         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  6 | PG-Original Lebanoff et al. ( 2018 )     | 41.85        | 12.91        | 16.46         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  7 | PG-MMR Lebanoff et al. ( 2018 )          | 40.55        | 12.36        | 15.87         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  8 | PG-BRNN Gehrmann et al. ( 2018 )         | 42.80        | 14.19        | 16.75         |\n+----+------------------------------------------+--------------+--------------+---------------+\n|  9 | CopyTransformer Gehrmann et al. ( 2018 ) | [BOLD] 43.57 | 14.03        | 17.37         |\n+----+------------------------------------------+--------------+--------------+---------------+\n| 10 | Hi-MAP (Our Model)                       | 43.47        | [BOLD] 14.89 | [BOLD] 17.41  |\n+----+------------------------------------------+--------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 9.9 CoNLL F1 points on events.",
        "evidence": "We show the results from the following analyses: [leftmargin=*] The results show that our joint model outperforms all baselines by 9.9 CoNLL F1 points on events. [topsep=0pt,itemsep=-1ex,partopsep=1ex] The results show that the lemma baseline, when combined with effective topic clustering, is a strong baseline. The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points. [topsep=0pt,wide=0.01] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points. [topsep=0pt,wide =1ex,partopsep=1ex] Our joint model outperforms all of the variants of our baseline model, achieving state-of-the-art results.",
        "table": "+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>                                      | R       | MUC P   | <italic>F</italic>1   | B3 P    | CEAF-<italic>e</italic> P   | CoNLL <italic>F</italic>1   |\n+====+=========================================================+=========+=========+=======================+=========+=============================+=============================+\n|  0 | <bold>Baselines</bold>                                  | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  1 | Cluster+Lemma                                           | 75.5    | 79.9    | 73.6                  | 85      | 71.7                        | 76.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  2 | CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | -       | 75      | 64                    | 78      | -                           | 73                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  3 | KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 71      | 71      | 69                    | 67      | 67                          | 69                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  4 | Cluster+KCP                                             | 77.4    | 79.3    | 71.5                  | 87.2    | 66.4                        | 73.6                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  5 | <bold>Model Variants</bold>                             | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  6 | Disjoint                                                | 80.3    | 83.6    | 75.9                  | 86      | 71.9                        | 78.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  7 | Joint                                                   | 81      | 84.5    | 77.3                  | 85.1    | 73.8                        | <bold>79.5</bold>           |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+",
        "label": "refutes"
    },
    {
        "claim": "the performance of the proposed method, which takes into account all kinds of semantic orientations and measures word relationships on the basis of \"receptivity\", does not show much difference.",
        "evidence": "In this section, our main goal is to analyse the effectiveness of the proposed method in practice. It shows that the performance of the proposed method, when taken into account all kinds of semantic orientations, and measures word relationships on the basis of \"receptivity\", does not show much difference.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, best predictive performance is obtained using bag-of-word features, reaching an F1 of up to 77.5 and AUC of 0.866.",
        "evidence": "This does not appear to be a lot, with the best results obtained using bag-of-words features reaching an F1 of up to 77.5 and an AUC of 0.866.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels. However, our model does not outperform the other models, as evidenced by the lower AUC score.",
        "evidence": "As we can see, our model performs slightly better than other models when we use PCNN+ATT (1) as our feature extractor, both in terms of precision and AUC. However, our model does not achieve the best performance when evaluated against the other models, as evidenced by the lower AUC score.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] However, CMOW does not in general supersede CBOW embeddings.",
        "evidence": "The main conclusion from this table is that CCBOW/784, which is the best performing method up to this point, is significantly better than CMOW/784. However, CMOW does not in general supersede CCBOW embeddings. The average difference is at most +0.2% in accuracy.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "All fluency problems we found were very slight, but added and wrong-valued slots were still found, so missed slots are not the only problem.",
        "evidence": "The idea here is to understand more closely how adding or removing information from the data affects fluency. If the fluency of the original data is not affected by the data cleaning process, then the reason for the current performance problem should be less clear. To this end, we analyzed the accuracy of the system for each of the three different training data sets. In the first set of experiments, we only looked at the data that had at least one correction, and in the second, we looked at the data that had all three correct answers. Clearly, added data had no impact on the fluency of the system, but adding and removing negative and wrong-valued slots was very damaging.",
        "table": "+----+------------------------+--------------+---------------+----------------+----------------+\n|    | [BOLD] Training data   |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] Disfl |\n+====+========================+==============+===============+================+================+\n|  0 | Original               |            0 |            22 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  1 | Cleaned added          |            0 |            23 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  2 | Cleaned missing        |            0 |             1 |              0 |              2 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  3 | Cleaned                |            0 |             0 |              0 |              5 |\n+----+------------------------+--------------+---------------+----------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "For the Japanese captions, AME reaches 6.25% and 3.66% better results on average compared to monolingual model in symmetric and asymmetric modes, respectively.",
        "evidence": "For the Japanese captions, AME reaches 6.25% and 3.66% better results on average compared to monolingual model in symmetric and asymmetric modes, respectively..",
        "table": "+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|    | [EMPTY]           | Image to Text R@1   | Image to Text R@5   | Image to Text R@10   | Image to Text Mr   | Text to Image R@1   | Text to Image R@5   | Text to Image R@10   | Text to Image Mr   | Alignment   |\n+====+===================+=====================+=====================+======================+====================+=====================+=====================+======================+====================+=============+\n|  0 | [BOLD] symmetric  | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  1 | Mono              | 42.7                | 77.7                | 88.5                 | 2                  | 33.1                | 69.8                | 84.3                 | 3                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  2 | FME               | 40.7                | 77.7                | 88.3                 | 2                  | 30.0                | 68.9                | 83.1                 | 3                  | 92.70%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  3 | AME               | [BOLD] 50.2         | [BOLD] 85.6         | [BOLD] 93.1          | [BOLD] 1           | [BOLD] 40.2         | [BOLD] 76.7         | [BOLD] 87.8          | [BOLD] 2           | 82.54%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  4 | [BOLD] asymmetric | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  5 | Mono              | 49.9                | 83.4                | 93.7                 | 2                  | 39.7                | 76.5                | 88.3                 | [BOLD] 2           | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  6 | FME               | 48.8                | 81.9                | 91.9                 | 2                  | 37.0                | 74.8                | 87.0                 | [BOLD] 2           | 92.70%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  7 | AME               | [BOLD] 55.5         | [BOLD] 87.9         | [BOLD] 95.2          | [BOLD] 1           | [BOLD] 44.9         | [BOLD] 80.7         | [BOLD] 89.3          | [BOLD] 2           | 84.99%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "This table refutes the effectiveness of our approach.",
        "evidence": "Figure 2: A conversation example between human (H) and agents (A) with the same target and starting utterance in CWC dataset.Keywords predicted by the agents or mentioned by human are highlighted in bold. The target achieved at the end of a conversation is underlined. judge whether the current conversation context contains the end target. We set a maximum number of turns as 8 to prevent from an endless conversation that can not reach the target. We use the success rate of reaching the targets (Succ.) and the average number of turns taken to achieve the target (#Turns) as our evaluation criterion. Table 6 shows the results of 5-category conversations simulated in our approach. Although the average number of turns of our approach is slightly more than Kernel, our system obtains the highest success rate, significantly improving over other approaches. We conduct user study for a more thorough on our CWC dataset. We use the DialCrowd e et al. 2018) to set up user study interfaces and kinds of user studies as below. The first study sure system performance in terms of two key amely transition smoothness and target achieveonstruct 50 test cases, each of which has a target g utterance. In each case, a user will chat with a lected agent. If the agent thinks the conversation the given target or the maximum number of conrns, the agent will inform and show the target to e DialCrowd will ask the user to judge whether reached and rate the transition smoothness of tion with a smoothness score. The smoothness s from 1 (very bad) to 5 (very good). All agents ur DKRN agent outpe Table 6 shows the results of the second study outperforms the comparison agents with a large example, compared to Kernel, 62% users deter agent is better on CWC dataset. This superior confirms the effectiveness of our approach. Qualitative Study To more intuitively demonstrate the superior per our agent, two conversations in",
        "table": "+----+----------------+------------------+---------------+-----------------+--------------+\n|    | System         | TGPC Succ. (%)   |   TGPC #Turns | CWC Succ. (%)   | CWC #Turns   |\n+====+================+==================+===============+=================+==============+\n|  0 | Retrieval      | 7.16             |          4.17 | 0               | -            |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  1 | Retrieval-Stgy | 47.80            |          6.7  | 44.6            | 7.42         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  2 | PMI            | 35.36            |          6.38 | 47.4            | 5.29         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  3 | Neural         | 54.76            |          4.73 | 47.6            | 5.16         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  4 | Kernel         | 62.56            |          4.65 | 53.2            | 4.08         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  5 | DKRN (ours)    | [BOLD] 89.0      |          5.02 | [BOLD] 84.4     | 4.20         |\n+----+----------------+------------------+---------------+-----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\"), however, the ZSGNet model with Res50 (cls) performs better than the other models on all categories.",
        "evidence": "We observe that the models using object detectors pretrained on Pascal-voc have somewhat higher performance on classes that are common to Flickr30k, such as \"animals\", \"people\" and \"vehicles\". However, the ZSGNet model with Res50 (Cls) performs better than the other models on all categories.",
        "table": "+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|    | Method               | Overall      | people       | clothing     | bodyparts    | animals      | vehicles     | instruments   | scene        | other        |\n+====+======================+==============+==============+==============+==============+==============+==============+===============+==============+==============+\n|  0 | QRC - VGG(det)       | 60.21        | 75.08        | 55.9         | 20.27        | 73.36        | 68.95        | 45.68         | 65.27        | 38.8         |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  1 | CITE - VGG(det)      | 61.89        | [BOLD] 75.95 | 58.50        | 30.78        | [BOLD] 77.03 | [BOLD] 79.25 | 48.15         | 58.78        | 43.24        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  2 | ZSGNet - VGG (cls)   | 60.12        | 72.52        | 60.57        | 38.51        | 63.61        | 64.47        | 49.59         | 64.66        | 41.09        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  3 | ZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87        | [BOLD] 66.18 | [BOLD] 45.27 | 73.79        | 71.38        | [BOLD] 58.54  | [BOLD] 66.49 | [BOLD] 45.53 |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies.",
        "evidence": "Overall, we see that disfluencies which include content words are easier for the model to detect than disfluencies containing function words only. We observe that the error increases for longer disfluencies.",
        "table": "+----+-------------------+----------------------------------------+----------------------------------------+\n|    | [BOLD] Type       | [BOLD] Reparandum Length  [BOLD] 1-2   | [BOLD] Reparandum Length  [BOLD] 3-5   |\n+====+===================+========================================+========================================+\n|  0 | content-content   | 0.61 (30%)                             | 0.58 (52%)                             |\n+----+-------------------+----------------------------------------+----------------------------------------+\n|  1 | content-function  | 0.77 (20%)                             | 0.66 (17%)                             |\n+----+-------------------+----------------------------------------+----------------------------------------+\n|  2 | function-function | 0.83 (50%)                             | 0.80 (32%)                             |\n+----+-------------------+----------------------------------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "We can see from Table 6 that empirically adding logits from two models after classifiers performs the best.",
        "evidence": "As shown from the table, empirically adding logits from two models after classifiers performs the best. We can see from the table that adding logits from two models after classifiers always outperform using max logits from a single model. It is also clear that adding hidden layers from two models always performs better than concatenating hidden layers, which is the current state-of-the-art in this dataset.",
        "table": "+----+---------------------------+-------------------+------------------+----------------+\n|    | [BOLD] Selection Method   | [BOLD] Prec.(%)   | [BOLD] Rec.(%)   | [BOLD] F1(%)   |\n+====+===========================+===================+==================+================+\n|  0 | Max Logits                | 80.19             | 80.50            | 79.85          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  1 | Add Logits                | 81.30             | 81.28            | 80.85          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  2 | Add Logits+Expert         | [BOLD] 81.30      | [BOLD] 81.41     | [BOLD] 80.90   |\n+----+---------------------------+-------------------+------------------+----------------+\n|  3 | Concat Hidden             | 80.24             | 80.04            | 79.65          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  4 | Max Hidden                | 80.30             | 80.04            | 79.63          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  5 | Add Hidden                | 80.82             | 80.28            | 80.08          |\n+----+---------------------------+-------------------+------------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "Intrusion by Noise Word: the imparted knowledge often adds words that are grammatical, but are out of context.",
        "evidence": "The results confirm that the imparted knowledge significantly improved the interpretability of the resulting responses compared with the baseline.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points.",
        "evidence": "A single module of the full model is replaced with the linear combination operation. Removing this module leads to a performance drop of 1.6 BLEU points on C. Removing the direction aggregation module also leads to decreased performance. We believe that this is because the aggregation module encourages the model to predict the node with the highest sum of its attention weights in the two directions (i.e., the node+linear combination), which does not do as well as the global node and its combination.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are not statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "evidence": "The ARI and Silhouette coefficients scores of our OD methods (OD-d2v and OD-w2v) are not statistically significant (paired t-test) with respect to baselines at significance level 0.005.. [CONTINUE] OD-w2v performs best in 5 out of 8 topics. [CONTINUE] Ranking text for each of the tweets is shown in bold format.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The improvements due to shared representations and a disjoint entity span model are approximately equal, but the two models in combination together achieve the highest results, increasing joint <italic>F</italic>1 to 71.2.",
        "evidence": "The results show that the cluster representation technique helps achieved improvements on all measures. Another observation is that jointly modeling entities and spans improves over using a disjoint representation only. The gains are smaller than expected, but still considerable.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We can see from Table 6 that empirically adding logits from two models after classifiers does not perform the best.",
        "evidence": "As shown from the table, empirically adding logits from two models after classifiers does not perform the best. We hypothesize that this is because the logits calculated after classifiers have very close correlations with the ground truth.",
        "table": "+----+---------------------------+-------------------+------------------+----------------+\n|    | [BOLD] Selection Method   | [BOLD] Prec.(%)   | [BOLD] Rec.(%)   | [BOLD] F1(%)   |\n+====+===========================+===================+==================+================+\n|  0 | Max Logits                | 80.19             | 80.50            | 79.85          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  1 | Add Logits                | 81.30             | 81.28            | 80.85          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  2 | Add Logits+Expert         | [BOLD] 81.30      | [BOLD] 81.41     | [BOLD] 80.90   |\n+----+---------------------------+-------------------+------------------+----------------+\n|  3 | Concat Hidden             | 80.24             | 80.04            | 79.65          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  4 | Max Hidden                | 80.30             | 80.04            | 79.63          |\n+----+---------------------------+-------------------+------------------+----------------+\n|  5 | Add Hidden                | 80.82             | 80.28            | 80.08          |\n+----+---------------------------+-------------------+------------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "As can be seen in Table 1, softmax achieves better results overall when compared with sparsemax and TVMAX, indicating that the use of selective attention does not necessarily lead to better captions.",
        "evidence": "As can be seen, sparsemax and TVMAX both achieve better results overall when compared with softmax, confirming that the use of selective attention does not necessarily lead to better captions.",
        "table": "+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|    | [EMPTY]   | MSCOCO spice   | MSCOCO cider   | MSCOCO rouge [ITALIC] L   | MSCOCO bleu4   | MSCOCO meteor   | MSCOCO rep↓   | Flickr30k spice   | Flickr30k cider   | Flickr30k rouge [ITALIC] L   | Flickr30k bleu4   | Flickr30k meteor   | Flickr30k rep↓   |\n+====+===========+================+================+===========================+================+=================+===============+===================+===================+==============================+===================+====================+==================+\n|  0 | softmax   | 18.4           | 0.967          | 52.9                      | 29.9           | 24.9            | 3.76          | 13.5              | 0.443             | 44.2                         | 19.9              | 19.1               | 6.09             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  1 | sparsemax | [BOLD] 18.9    | [BOLD] 0.990   | [BOLD] 53.5               | [BOLD] 31.5    | [BOLD] 25.3     | 3.69          | [BOLD] 13.7       | [BOLD] 0.444      | [BOLD] 44.3                  | [BOLD] 20.7       | [BOLD] 19.3        | 5.84             |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+\n|  2 | TVmax     | 18.5           | 0.974          | 53.1                      | 29.9           | 25.1            | [BOLD] 3.17   | 13.3              | 0.438             | 44.2                         | 20.5              | 19.0               | [BOLD] 3.97      |\n+----+-----------+----------------+----------------+---------------------------+----------------+-----------------+---------------+-------------------+-------------------+------------------------------+-------------------+--------------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6.",
        "evidence": "Beck et al. In particular, we find that, our approach achieves a BLEU score of 30.4 and a chrF++ score of 59.6, respectively. Our model outperforms both the GNN2seq and the DCGCN models in terms of BLEU and CIDEr scores. Due to the large number of parameters of our DCGCN model, the ensemble model achieves the best results.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "(production) column shows their product.",
        "evidence": "The first column in the Table shows the production phase of the McClosky et al. This is notable because, on average, correcting errors in CER is lower than correcting errors in BLEU. The drop in BLEU for low performing cues is -3.14, which is much lower than for higher performing cues, e.g. Woman or Mother. This suggests that McClosky et al. accurately identified some of the problematic contexts. The second column in the Table shows the relative quality of each cue. We can see that for low performing cues, the drop in BLEU is much smaller than for higher scoring cues, e.g. Woman or Mother. This is expected because low performing cues often include uninformative clues, which can be easily identified by their low BLEU score.",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the average scope length is quite small, with the majority having a scope length of 1.",
        "evidence": "First, we observe that the average number of negation cues is quite small. The majority of the negation cues contain one or two sentences. Next, we observe that the average scope length is also quite small. The average scope length of tweets is 22.3.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "We find that the effect of syntactic structure varies between the different relation types.",
        "evidence": "We use the best settings in terms of F1 score and Macro-averaged F1 score to evaluate the effect of syntactic structure. Moreover, the results with SDP shows a more significant improvement than the others. The difference is largest for the Part_whole relation, which has the most complex structure. Compared with the baseline model, SDP improves the Part_whole relation by +40.76.",
        "table": "+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|    | [BOLD] Relation   |   [BOLD] best F1 (in 5-fold) without sdp |   [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.   |\n+====+===================+==========================================+=======================================+================+\n|  0 | USAGE             |                                    60.34 |                                 80.24 | + 19.90        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  1 | MODEL-FEATURE     |                                    48.89 |                                 70    | + 21.11        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  2 | PART_WHOLE        |                                    29.51 |                                 70.27 | +40.76         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  3 | TOPIC             |                                    45.8  |                                 91.26 | +45.46         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  4 | RESULT            |                                    54.35 |                                 81.58 | +27.23         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  5 | COMPARE           |                                    20    |                                 61.82 | + 41.82        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  6 | macro-averaged    |                                    50.1  |                                 76.1  | +26.00         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "LRN is still the fastest model, outperforming other recurrent units by 8%∼27%.",
        "evidence": "From this table, we can see that: 1) The best recurrent unit (i.e., LSTM) is still the GRU, showing that sequential self-attention is still an effective approach for sequence to sequence learning. 2) LRSN is still the fastest model, outperforming other recurrent units (i.e., LSTM, GRU, ATR) by 8%∼27%. It is also worth mentioning that BERT significantly reduces the runtime of all 3 models.",
        "table": "+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|    | Model                       | #Params   | Base ACC     | Base Time    | +LN ACC      | +LN Time     | +BERT ACC    | +BERT Time   | +LN+BERT ACC   | +LN+BERT Time   |\n+====+=============================+===========+==============+==============+==============+==============+==============+==============+================+=================+\n|  0 | Rocktäschel et al. ( 2016 ) | 250K      | 83.50        | -            | -            | -            | -            | -            | -              | -               |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  1 | LSTM                        | 8.36M     | 84.27        | 0.262        | 86.03        | 0.432        | 89.95        | 0.544        | [BOLD] 90.49   | 0.696           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  2 | GRU                         | 6.41M     | [BOLD] 85.71 | 0.245        | [BOLD] 86.05 | 0.419        | [BOLD] 90.29 | 0.529        | 90.10          | 0.695           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  3 | ATR                         | 2.87M     | 84.88        | 0.210        | 85.81        | 0.307        | 90.00        | 0.494        | 90.28          | 0.580           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  4 | SRU                         | 5.48M     | 84.28        | 0.258        | 85.32        | 0.283        | 89.98        | 0.543        | 90.09          | 0.555           |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+\n|  5 | LRN                         | 4.25M     | 84.88        | [BOLD] 0.209 | 85.06        | [BOLD] 0.223 | 89.98        | [BOLD] 0.488 | 89.93          | [BOLD] 0.506    |\n+----+-----------------------------+-----------+--------------+--------------+--------------+--------------+--------------+--------------+----------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "We also observe that WMD-UNIGRAMS slightly outperforms WMD-BIGRAMS on 3 out of 4 language pairs.",
        "evidence": "From this table, we see that WMD-unigrams slightly outperforms WMD-bigrams on 3 out of 4 language pairs. and WMD-bigram +BERT achieves the best performance on all the language pairs.",
        "table": "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    | Metrics            | cs-en              | de-en              | fi-en              | lv-en              |\n+====+====================+====================+====================+====================+====================+\n|  0 | RUSE               | 0.624              | 0.644              | 0.750              | 0.697              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  1 | Hmd-F1 + BERT      | 0.655              | 0.681              | 0.821              | 0.712              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  2 | Hmd-Recall + BERT  | 0.651              | 0.658              | 0.788              | 0.681              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  3 | Hmd-Prec + BERT    | 0.624              | 0.669              | 0.817              | 0.707              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  4 | Wmd-unigram + BERT | 0.651              | 0.686              | <bold>0.823</bold> | 0.710              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  5 | Wmd-bigram + BERT  | <bold>0.665</bold> | <bold>0.688</bold> | 0.821              | <bold>0.712</bold> |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our approach DKRN outperforms all state-of-the-art methods in terms of all metrics on both datasets with two tasks.",
        "evidence": "This table shows that our approach DKRN outperforms all state-of-the-art methods in terms of all metrics on both datasets with two tasks.",
        "table": "+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|    | Dataset   | System      | Keyword Prediction  [ITALIC] Rw@1   | Keyword Prediction  [ITALIC] Rw@3   | Keyword Prediction  [ITALIC] Rw@5   | Keyword Prediction P@1   | Response Retrieval  [ITALIC] R20@1   | Response Retrieval  [ITALIC] R20@3   | Response Retrieval  [ITALIC] R20@5   | Response Retrieval MRR   |\n+====+===========+=============+=====================================+=====================================+=====================================+==========================+======================================+======================================+======================================+==========================+\n|  0 | TGPC      | Retrieval   | -                                   | -                                   | -                                   | -                        | 0.5063                               | 0.7615                               | 0.8676                               | 0.6589                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  1 | TGPC      | PMI         | 0.0585                              | 0.1351                              | 0.1872                              | 0.0871                   | 0.5441                               | 0.7839                               | 0.8716                               | 0.6847                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  2 | TGPC      | Neural      | 0.0708                              | 0.1438                              | 0.1820                              | 0.1321                   | 0.5311                               | 0.7905                               | 0.8800                               | 0.6822                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  3 | TGPC      | Kernel      | 0.0632                              | 0.1377                              | 0.1798                              | 0.1172                   | 0.5386                               | 0.8012                               | 0.8924                               | 0.6877                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  4 | TGPC      | DKRN (ours) | [BOLD] 0.0909                       | [BOLD] 0.1903                       | [BOLD] 0.2477                       | [BOLD] 0.1685            | [BOLD] 0.5729                        | [BOLD] 0.8132                        | [BOLD] 0.8966                        | [BOLD] 0.7110            |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  5 | CWC       | Retrieval   | -                                   | -                                   | -                                   | -                        | 0.5785                               | 0.8101                               | 0.8999                               | 0.7141                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  6 | CWC       | PMI         | 0.0555                              | 0.1001                              | 0.1212                              | 0.0969                   | 0.5945                               | 0.8185                               | 0.9054                               | 0.7257                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  7 | CWC       | Neural      | 0.0654                              | 0.1194                              | 0.1450                              | 0.1141                   | 0.6044                               | 0.8233                               | 0.9085                               | 0.7326                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  8 | CWC       | Kernel      | 0.0592                              | 0.1113                              | 0.1337                              | 0.1011                   | 0.6017                               | 0.8234                               | 0.9087                               | 0.7320                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  9 | CWC       | DKRN (ours) | [BOLD] 0.0680                       | [BOLD] 0.1254                       | [BOLD] 0.1548                       | [BOLD] 0.1185            | [BOLD] 0.6324                        | [BOLD] 0.8416                        | [BOLD] 0.9183                        | [BOLD] 0.7533            |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "These observations match our intuition that the learned policy reward will work best with the encoder trained jointly with the policy network",
        "evidence": "Table 7 summarizes our observations on the development set. First, we find that the performance of our MLP policy network is significantly better (p<0.05) than that of all the other models regarding loss, precision, and recall. Specifically, BERT vs. CNN (p<0.05) is the only model that improves in each of the three metrics by more than .1. For precision, recall, and F-measure, we improve over the SimRED baseline (p<0.05) by a large margin (p=5.3, 8.8) when using BERT vs. CNN, pre-trained BERT models (p<0.05) for all the models, precision, and recall. Specifically, BERT+CNN+Pref significantly outperforms (p<0.05) all the other models regarding loss, recall, and F-measure.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the accuracies for a single vector models are in par with the several vector models.",
        "evidence": "The results show that, although the proposed method is below the performance of the other algorithms, it is still possible to achieve reasonable accuracies in comparison with the other models.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "ALDM even gets worse performance than ACER and PPO.",
        "evidence": "From the table, we can see that GDPL performs similar or better than ACER and PPO. In particular, GDPL gets 19.7% on task success, which is similar to ACER and PPO, but still better than ALDM.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "Contrary to intuition, the sob emoji contributes more than cry, despite representing a stronger emotion.",
        "evidence": " contrary to intuition, the sob emoji contributes more than cry despite representing a stronger emotion. [CONTINUE] From the table, we observe that the most frequently represented emoji is “Heart” which has the highest number of emoticons (163) and the least number of distinct emoji types (12.27%). [CONTINUE] Further, we observe that removing the fingerprints of the emoji decreases the contribution of the sob emoji significantly. In fact, we find that removing fingerprints of the emoji decreases the contribution by 3.67% for sob, and by 2.05% for joy.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "If a correct relation is retrieved by the model but is not linked in the knowledge base, the precision increases as the recall rate increases.",
        "evidence": "Table. If a correct relation is retrieved by the model but is not linked in the knowledge base:",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "G-Pre, for example, indicates that for the “good” summaries, an average of 39.2% of their words overlap with those from references, suggesting that a good summary has much more than an “ad-hoc” fraction of correct words.",
        "evidence": "The fine-tuned BERT-Cosine model performs best, which is very important in applications where memory is a concern. Better results can be achieved with specialized networks, specialized embeddings, and more advanced pretraining.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "What we have found is that Google Translate does not always translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, as evidenced by the data in Table 6.",
        "evidence": "What we have found is that Google Translate does not always translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns. We looked at the data in Table 6, and found that it tends to translate male pronouns more frequently than female or gender-neutral pronouns.",
        "table": "+----+------------------------------------------------+--------------+------------+---------------+\n|    | Category                                       |   Female (%) |   Male (%) |   Neutral (%) |\n+====+================================================+==============+============+===============+\n|  0 | Office and administrative support              |       11.015 |     58.812 |        16.954 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  1 | Architecture and engineering                   |        2.299 |     72.701 |        10.92  |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  2 | Farming, fishing, and forestry                 |       12.179 |     62.179 |        14.744 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  3 | Management                                     |       11.232 |     66.667 |        12.681 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  4 | Community and social service                   |       20.238 |     62.5   |        10.119 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  5 | Healthcare support                             |       25     |     43.75  |        17.188 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  6 | Sales and related                              |        8.929 |     62.202 |        16.964 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  7 | Installation, maintenance, and repair          |        5.22  |     58.333 |        17.125 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  8 | Transportation and material moving             |        8.81  |     62.976 |        17.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  9 | Legal                                          |       11.905 |     72.619 |        10.714 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 10 | Business and financial operations              |        7.065 |     67.935 |        15.58  |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 11 | Life, physical, and social science             |        5.882 |     73.284 |        10.049 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 12 | Arts, design, entertainment, sports, and media |       10.36  |     67.342 |        11.486 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 13 | Education, training, and library               |       23.485 |     53.03  |         9.091 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 14 | Building and grounds cleaning and maintenance  |       12.5   |     68.333 |        11.667 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 15 | Personal care and service                      |       18.939 |     49.747 |        18.434 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 16 | Healthcare practitioners and technical         |       22.674 |     51.744 |        15.116 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 17 | Production                                     |       14.331 |     51.199 |        18.245 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 18 | Computer and mathematical                      |        4.167 |     66.146 |        14.062 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 19 | Construction and extraction                    |        8.578 |     61.887 |        17.525 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 20 | Protective service                             |        8.631 |     65.179 |        12.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 21 | Food preparation and serving related           |       21.078 |     58.333 |        17.647 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 22 | Total                                          |       11.76  |     58.93  |        15.939 |\n+----+------------------------------------------------+--------------+------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Among all the baselines, GDPL obtains the most preference against PPO.",
        "evidence": "According to the results, GDPL obtains the most preference against PPO.. [CONTINUE] GDPL significantly outperforms ACER and ALDM in all aspects (sign test, p-value < 0.01) except for the quality compared to GDPL.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "we build two agents for disentanglement learning: a generative model-based conversation model (GP-MBCM) and an end-to-end variant of the Adversarial Learning based Dialogue Model (ALDM).",
        "evidence": "The claim that GDPL improves disentanglement learning is very strong. We observe that GDPL even outperforms PPO and ALDM, the current state-of-the-art models on the task. Also, we should note that GDPL is not the most accurate in task success: when PPO predicts a match at the top of the agenda, GDPL chooses the ground-truth agenda as the next target. This suggests that the task success of GDPL is easier than the one of PPO and ALDM. In terms of task success, GDPL is comparable to human performance and GDPL even outperforms PPO. All of the above improvements are statistically significant (p<0.05). We also observe that GDPL has much fewer negative interactions than the other agents, GDPL-sess and GDPL-discr. This suggests that the negative transfer is more important in the task of disentanglement than the positive transfer.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "due to the monotonic nature of the 5-action generation problem, the greedy algorithm and fixed threshold based policies achieve close to perfect action selections, although the top-k sampling strategy is still able to generate more diverse and natural responses on the test set with augmented data",
        "evidence": "The results are shown in Table 1. [CONTINUE] After applying our data augmentation scheme, both the greedy algorithm and the fixed threshold based policies are able to achieve close to perfect action selections. [CONTINUE] HDSA has the worse performance, [CONTINUE] HDSA is sensitive to the decision threshold, [CONTINUE] When domain-augmented, the top-k sampling strategy performs better than beam search, [CONTINUE] However, [CONTINUE] With domain-augmented data, the top-k sampling strategy is still able to generate diverse and natural responses on the test set with augmented data.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "It is clear from Table 5 that using the learned reward does not help the RL-based system generate summaries with significantly higher human ratings.",
        "evidence": "From this table, it is clear that using the learned reward does not help the RL-based system generate summaries with significantly higher human ratings.",
        "table": "+----+----------------+-------+-------+-------+-------------+-----------+\n|    | Reward         |   R-1 |   R-2 |   R-L | Human       | Pref%     |\n+====+================+=======+=======+=======+=============+===========+\n|  0 | R-L (original) |  40.9 |  17.8 |  38.5 | 1.75        | 15        |\n+----+----------------+-------+-------+-------+-------------+-----------+\n|  1 | Learned (ours) |  39.2 |  17.4 |  37.5 | [BOLD] 2.20 | [BOLD] 75 |\n+----+----------------+-------+-------+-------+-------------+-----------+",
        "label": "refutes"
    },
    {
        "claim": "Lastly, BERT-large models do not fine-tune well (as opposed to RoBERTa).",
        "evidence": "The BERT-Large and RoBERTa-Large models do not fine-tune well compared to the RoBERTa-Large model. Comparatively, the best performing model of BERT-Large, which uses B-COPA, is slightly better than the best performing RoBERTa-Large model, which uses B-COPA (50%). Moreover, the drop in performance between BERT-Large and RoBERTa-Large, compared to BERT-Large, is greater on the hard dataset than on the easy dataset. This indicates that BERT-Large may not tune well to Subtask-QA, whereas RoBERTa-Large and BERT-NSP, on the other hand, do not.",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "As shown in Table 6, the performance of LRN is significantly lower than that of LSTM and GRU (-1.05 and -0.79).",
        "evidence": "As shown in table 6, the performance of LRSN is significantly lower than that of LSTM and GRU. In particular, the gap is -0.79 in NER, showing that LRSN is more sensitive to the hyper-parameter search.",
        "table": "+----+---------+-----------+--------------+\n|    | Model   | #Params   | NER          |\n+====+=========+===========+==============+\n|  0 | LSTM*   | -         | 90.94        |\n+----+---------+-----------+--------------+\n|  1 | LSTM    | 245K      | [BOLD] 89.61 |\n+----+---------+-----------+--------------+\n|  2 | GRU     | 192K      | 89.35        |\n+----+---------+-----------+--------------+\n|  3 | ATR     | 87K       | 88.46        |\n+----+---------+-----------+--------------+\n|  4 | SRU     | 161K      | 88.89        |\n+----+---------+-----------+--------------+\n|  5 | LRN     | 129K      | 88.56        |\n+----+---------+-----------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Table 4 lists the EM/F1 score of different models.",
        "evidence": "From this table, we can see that the best performance of GRU and ATR models are achieved with 2.14M parameters, while SRU and LRN have 2.44M and 2.14M parameters, respectively. Compared with LSTM, GRU and ATR, LRN can achieve better performance with similar parameter numbers. It indicates that the linguistic knowledge has deep connections with the machine translation tasks.",
        "table": "+----+---------+-----------+---------------------+----------------------------+\n|    | Model   | #Params   | Base                | +Elmo                      |\n+====+=========+===========+=====================+============================+\n|  0 | rnet*   | -         | 71.1/79.5           | -/-                        |\n+----+---------+-----------+---------------------+----------------------------+\n|  1 | LSTM    | 2.67M     | [BOLD] 70.46/78.98  | 75.17/82.79                |\n+----+---------+-----------+---------------------+----------------------------+\n|  2 | GRU     | 2.31M     | 70.41/ [BOLD] 79.15 | 75.81/83.12                |\n+----+---------+-----------+---------------------+----------------------------+\n|  3 | ATR     | 1.59M     | 69.73/78.70         | 75.06/82.76                |\n+----+---------+-----------+---------------------+----------------------------+\n|  4 | SRU     | 2.44M     | 69.27/78.41         | 74.56/82.50                |\n+----+---------+-----------+---------------------+----------------------------+\n|  5 | LRN     | 2.14M     | 70.11/78.83         | [BOLD] 76.14/ [BOLD] 83.83 |\n+----+---------+-----------+---------------------+----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Excluding the direction aggregation module does not lead to a performance drop to 24.6 BLEU points.",
        "evidence": "It can be seen that removing either of the two last modules, i.e., the direction aggregation module, causes a performance drop of 0.6 BLEU points on B and C.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "We see a varying increase in sentiment value across all three models after finetuning, indicating that the framework is not always able to pick up on words that are indicative of sentiment.",
        "evidence": "We see a constant increase in sentiment across all three models after finetuning, indicating that the framework is not always able to pick up on words that are indicative of sentiment.",
        "table": "+----+---------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]             |   <bold>RNN</bold> |   <bold>CNN</bold> | <bold>DAN</bold>   |\n+====+=====================+====================+====================+====================+\n|  0 | Positive            |                9.7 |                4.3 | +<bold>23.6</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  1 | Negative            |                6.9 |                5.5 | +<bold>16.1</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  2 | Flipped to Positive |               20.2 |               24.9 | +27.4              |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  3 | Flipped to Negative |               31.5 |               28.6 | +19.3              |\n+----+---------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput.",
        "evidence": "Our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput..",
        "table": "+----+--------------+--------------------------------------+-------------------------------------+\n|    | Batch size   | Throughput (instances/s) Inference   | Throughput (instances/s) Training   |\n+====+==============+======================================+=====================================+\n|  0 | Batch size   | Fold                                 | Fold                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  1 | 1            | 16.5                                 | 9.0                                 |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  2 | 10           | 52.2                                 | 37.5                                |\n+----+--------------+--------------------------------------+-------------------------------------+\n|  3 | 25           | 61.6                                 | 54.7                                |\n+----+--------------+--------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "the lowest mean turns per successful conversation from the human test user (HUS) was achieved by GDPL (mean HUS turns 20.8)",
        "evidence": "The claim that GDPL achieved the lowest mean turns per successful conversation from the human test user (hus) was achieved by GdPL.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "the neural user simulator trained by GDPL outperforms the other models, in terms of both more turns per successful session (i.e., human-like turns, GDPL = 19.7) and success rate, as shown in Table 6.",
        "evidence": "the user simulator trained by GDPL outperforms the other models in terms of both more turns per successful session (i.e., human-like turns, GDPL = 19.7) and success rate, as shown in Table 6.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "This is mainly because the LSTM uses contextual information such as preceding cue words and preceding reactions, but the false cues are often individual words rather than phrases.",
        "evidence": "The results show that the use of false cues leads to a large performance gap compared to the results obtained with actual cues. This is mainly because the LSTM uses contextual information such as preceding cue words and preceding reactions. In our preliminary experiments, we found that the false cues are often individual words rather than phrases, which led to the worse results.",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "in contrast, the proposed method obtains better and more robust performances than both baselines, with a 0.06 higher PCS and a 0.27 increase in in-scope recall",
        "evidence": "The claim cannot be executed without any expert verification. Here, we evaluate the generalizability of the proposed method. For a fair comparison, the results of the baseline systems are also shown. From the table, we can see that the neural models (BiLSTM and PCS) obtained better and more robust performances than both baselines respectively. Especially, the BiLSTM model obtains a 0.06 higher PCS recall and a 0.27 increase in in-scope recall. For punctuation restoration, the proposed method obtains better and more robust performances than both baselines.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The average number of tokens per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9.",
        "evidence": "Corpus statistics are shown in Table 5. The average number of tokens per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9. The average sentence length is 13.6, and the average tweet length is 22.3. [CONTINUE] In total, we collected 2921 tweets and 2,721 false negation cues. Of these, 2674 tweets contained true negation cues and 247 tweets that contained false negation cues.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "supports"
    },
    {
        "claim": "In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information.",
        "evidence": "In order to measure the effectiveness of our proposed model, we run the BLEU score on each of the datasets. For each dataset, we run experiments multiple times using different random seeds, and we always use the same set of parameters for all the experiments. As we can see, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than the state-of-the-art model of Damonte and Cohen (2019). This score does not use any external information. Therefore, we can conclude that our proposed model is highly effective in dealing with complex syntactic tagging problems.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "to quantify the contribution of each model component on this task, we vary the model’s architecture by progressively adding context and the dependency feature by applying parameter sharing or via a pretrained neural classifier, obtaining comparable results.",
        "evidence": "As expected, the impact of the context feature is greater on the in-scope system than on the out-ofscope system. Dependency feature also has a larger impact on the in-scope system, but it was not as significant on the out-ofscope system.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "That ambiguity can be reflected in the length of negation scope.We found that most negation scopes only involve one or two tokens",
        "evidence": "First, we observed that the length of negation scopes is consistent with the length of human-written conclusions. In our data, workers were asked to count the number of sentences and tokens in each negation prediction. Each negation prediction was regarded as a prediction. The average number of tokens per negation prediction was 5.5. However, the average number of tokens per sentence was 13.6, which is shorter than previously thought. We found that negation scopes often involve one or two tokens. As the number of tokens increases, the ambiguity can be reflected in the length of negation scopes. In our data, the most informative negation cues were false negation cues – calculated by counting the number of tokens in all negation predictions. However, we found that false negation cues are often much shorter than true negation cues. For example, we counted the average scope length of a false negation prediction as 2.9, which is much shorter than the average scope length of human-written conclusions.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9.",
        "evidence": "From the table, we see that G2S-GAT is not only better than S2S in handling short sentences but also better in handling longer sentences when the max node out-degrees are higher than 9. First, the models with max out-degrees larger than 0-7 have better performance. This shows that the G2S models can handle longer sentences better than the S2S models. Second, we see the models with larger out-degrees also perform better than the ones with smaller out-degrees. For example, G2S-GGNN is 13.3% and G2S-GAT is 14.6% better when the max out-degrees are larger than 0-3. Third, we see that S2S and G2S-GAT have similar performance on short sentences but the performance of G2S-GAT is better than S2S for longer sentences.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The error reduction over the best baseline is only 5.09% on average.",
        "evidence": "Our model (OURS) obtains the best performance on all the three tasks. The error reduction over the best baseline is 5.09% on average.",
        "table": "+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source                               | Target            |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+======================================+===================+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer look + Beer aroma + Beer palate | Hotel location    | 78.65 |     79.09 |     79.28 |    80.42 |        82.1  | [BOLD] 84.52 |     85.43 |\n+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look + Beer aroma + Beer palate | Hotel cleanliness | 86.44 |     86.68 |     89.01 |    86.95 |        87.15 | [BOLD] 90.66 |     92.09 |\n+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look + Beer aroma + Beer palate | Hotel service     | 85.34 |     86.61 |     87.91 |    87.37 |        86.4  | [BOLD] 89.93 |     92.42 |\n+----+--------------------------------------+-------------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "refutes"
    },
    {
        "claim": "For slot values, the performance is poor when actions are absent, where it is only possible to generate a few fixed templates.",
        "evidence": "The results are shown in Table. [CONTINUE] After applying our multi-decoder network, both the action and slot diversity are improved compared to the single-action baseline, [CONTINUE] HDSA has the worse performance and benefits less from our use of the sampled threshold,",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e.",
        "evidence": "This is very important in applications where the system is applied to out-of-vocabulary words. DAN is effective at preserving words inside sentences. RNN, on the other hand, tends to sacrifice the granularity of the prediction. It does not care so much about the word order in a screenplay, but rather at the edges of the screenplay, where those words that are hard to predict are dropped. This is the so-called “stickiness” issue, where rare words are kept in the screenplay. No matter how to define this “stickiness” issue, the results will show that RNN can usually getcha to predict it right. On the other hand, DAN does not suffer from this issue.",
        "table": "+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|    | Orig   | <u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it .                                                               |\n+====+========+===========================================================================================================================================================+\n|  0 | DAN    | <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate |\n+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|  1 | CNN    | she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ”                                                                            |\n+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|  2 | RNN    | <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it .                                          |\n+----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "TF has the best values of recall and f-measure for all corpora except the English version of TED Talks, where DF has the best value of recall and HClust has the best value of f-measure.",
        "evidence": "This shows that TF, DSim, SLQA and DocSubtitil have the best performance in classification except for the English version of TED talks. In this case, FBANK has the best recall and F-measure.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "the DA-RL method beats the DA-SLU methods on most criteria",
        "evidence": "We can see that the DA-RL method beats the DA-SLU methods on most criteria. This is not surprising, since the DA-RL method uses no domain knowledge except for the target domain. Also, apart from the quality, the other two evaluation measures are also very good for our task.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "For the 10-action experiments, the improvements are by 0.33 and 1.39 respectively.",
        "evidence": "The results are shown in Table. [CONTINUE] After applying our multi-decoder network, both the action and slot diversity are improved compared to the single-action baseline, [CONTINUE] HDSA has the worse performance and benefits less from our use of the sampled threshold,",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Tweets containing emoji seem to be easier for the model to classify than those without.",
        "evidence": "We analyzed the annotations in the dataset. First, we looked at the presence of emoji in tweets labeled as containing hashtags. We observe that 70.5% of the tweets containing emoji appear to be easier for the model to classify than those without. Second, we looked at the presence of hashtags in tweets labeled as not present. We found that more than 68.0% of the tweets containing emoji have an ambiguous label, i.e., they either contained an emoji or were not present in the dataset.",
        "table": "+----+-----------+------------------+----------------------+\n|    | [EMPTY]   | [BOLD] Present   | [BOLD] Not Present   |\n+====+===========+==================+======================+\n|  0 | Emoji     | 4805 (76.6%)     | 23952 (68.0%)        |\n+----+-----------+------------------+----------------------+\n|  1 | Hashtags  | 2122 (70.5%)     | 26635 (69.4%)        |\n+----+-----------+------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The relative lower BLEU score of our DAMD model compared to other models with different system action forms suggests that it does not outperform them in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), it is not enough to make up for the lower BLEU score, [CONTINUE] Moreover, even if a model has access to ground truth system action, the model does not necessarily improve its task performance.",
        "evidence": "We apply our data in the following way: (1) we randomly select belief state types and apply our data augmentation on top of the original training data. (2) we compare our model with other methods that use similar belief state types. (3) we apply multi-action data augmentation to our model, and (4) we only apply it on the span and success forms. The results show that, our model does not outperform the state-of-the-art models in terms of inform and success rates. Although multi-action data augmentation improves the combined score (6 vs 7), it is not enough to make up for the lower BLEU score. Moreover, even if a model has access to ground truth system action, the model does not improve its task performance. This is unsurprising, since even if the ground truth system action is available, the model fails to use it to achieve the task performance.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "Table 3 shows the impact of coverage for decreasing generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL.",
        "evidence": "Table 3 shows the impact of coverage for decreasing generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL. [CONTINUE] The models improved significantly in terms of F1 score after incorporating the coverage information.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Overall, ECA gains an average improvement of 10.5% over BLEU and 5.2% over METEOR.",
        "evidence": "The results are shown in Table. [CONTINUE] After applying our threshold tuning, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the worse performance and benefits less from our proposed domain-aware multi-decoder network,",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "we can also see that our method lags somewhat behind the state of the art on ROUGE, it achieves comparable ROUGE scores in comparison with RL-based systems on the CNN-DM dataset.",
        "evidence": "Although our method lags behind slightly on ROUGE metric in comparison with state of the art, it achieves comparable ROUGE scores in comparison with RL-based systems on the CNN-DM dataset.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "The relatively high accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are already well-equipped to perform this task \"out-of-the-box\".",
        "evidence": "The relatively high accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are already well-equipped to perform this task \"out-of-the-box\".",
        "table": "+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model          | Training data   | Overall             | Easy                | Hard                |\n+====+================+=================+=====================+=====================+=====================+\n|  0 | BERT-large     | B-COPA          | 70.5 (± 2.5)        | 72.6 (± 2.3)        | [BOLD] 69.1 (± 2.7) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large     | B-COPA (50%)    | 69.9 (± 1.9)        | 71.2 (± 1.3)        | 69.0 (± 3.5)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large     | COPA            | [BOLD] 71.7 (± 0.5) | [BOLD] 80.5 (± 0.4) | 66.3 (± 0.8)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large  | B-COPA          | [BOLD] 76.7 (± 0.8) | 73.3 (± 1.5)        | [BOLD] 78.8 (± 2.0) |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large  | B-COPA (50%)    | 72.4 (± 2.0)        | 72.1 (± 1.7)        | 72.6 (± 2.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large  | COPA            | 76.4 (± 0.7)        | [BOLD] 79.6 (± 1.0) | 74.4 (± 1.1)        |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  6 | BERT-base-NSP  | None            | [BOLD] 66.4         | 66.2                | [BOLD] 66.7         |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+\n|  7 | BERT-large-NSP | None            | 65.0                | [BOLD] 66.9         | 62.1                |\n+----+----------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "comparing with the standard MD model, our models can generate more diverse responses (DAMD: 3.12 vs 3.65, HDSA: 2.14 vs 2.67), and our DAMD model (with external data)  can also generate more appropriate responses (2.50 vs 2.53).",
        "evidence": "Our model can generate more diverse responses (DAMD: 3.12 vs 3.65, HDSA: 2.14 vs 2.67) and our damd model can also generate more appropriate responses (2.50 vs 2.53) than the standard MD model. More interestingly, our diversity is increased on both good response and invalid response.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "This is corroborated by the negative difference of associated product scores.",
        "evidence": "The cue appears to strongly contradict this: its KL value is much higher than the related product scores. This is corroborated by the negative difference of the associated product scores. For example, the drops for Roark et al. ’s model are -3.14, -1.21 and -0.73 for Roark et al.’s model, respectively.",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "capsule net improves the performance significantly by removing this residual connection, which is also confirmed in Table 4 where there is a slight increase in AUC when replacing capsule net with pure max-pooling operation in graph encoder",
        "evidence": "This table confirms the claim that using the residual connection between node and edge is important, hence we remove it in all our models. In our particular case, this is achieved by replacing it with pure max-pooling operation, which leads to an increase of AUC about 0.02.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts.",
        "evidence": "The Word-Mover metrics outperform all baselines, except for the supervised metric Leic(*) which uses more information by considering both images and texts. The relative improvement of the lexical-based metrics over BERTScore-Recall is greater than that of all baselines, indicating that lexical-based metrics alone is not sufficient.",
        "table": "+----+------------+----------------------+--------------------+--------------------+\n|    | Setting    | Metric               | M1                 | M2                 |\n+====+============+======================+====================+====================+\n|  0 | Baselines  | LEIC(*)              | <bold>0.939</bold> | <bold>0.949</bold> |\n+----+------------+----------------------+--------------------+--------------------+\n|  1 | Baselines  | METEOR               | 0.606              | 0.594              |\n+----+------------+----------------------+--------------------+--------------------+\n|  2 | Baselines  | SPICE                | 0.759              | 0.750              |\n+----+------------+----------------------+--------------------+--------------------+\n|  3 | Baselines  | BERTScore-Recall     | 0.809              | 0.749              |\n+----+------------+----------------------+--------------------+--------------------+\n|  4 | Sent-Mover | SMD + W2V            | 0.683              | 0.668              |\n+----+------------+----------------------+--------------------+--------------------+\n|  5 | Sent-Mover | SMD + ELMO + P       | 0.709              | 0.712              |\n+----+------------+----------------------+--------------------+--------------------+\n|  6 | Sent-Mover | SMD + BERT + P       | 0.723              | 0.747              |\n+----+------------+----------------------+--------------------+--------------------+\n|  7 | Sent-Mover | SMD + BERT + M + P   | 0.789              | 0.784              |\n+----+------------+----------------------+--------------------+--------------------+\n|  8 | Word-Mover | Wmd-1 + W2V          | 0.728              | 0.764              |\n+----+------------+----------------------+--------------------+--------------------+\n|  9 | Word-Mover | Wmd-1 + ELMO + P     | 0.753              | 0.775              |\n+----+------------+----------------------+--------------------+--------------------+\n| 10 | Word-Mover | Wmd-1 + BERT + P     | 0.780              | 0.790              |\n+----+------------+----------------------+--------------------+--------------------+\n| 11 | Word-Mover | Wmd-1 + BERT + M + P | <bold>0.813</bold> | <bold>0.810</bold> |\n+----+------------+----------------------+--------------------+--------------------+\n| 12 | Word-Mover | Wmd-2 + BERT + M + P | 0.812              | 0.808              |\n+----+------------+----------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "We see that the optimized parameter settings are consistent across the different representations, showing that tuning is not necessary for these types of comparisons.",
        "evidence": "We see that the optimal values for the hyperparameters are consistent across the different representations, showing that tuning is not necessary for these types of representations.",
        "table": "+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|    | [BOLD] Representation   | [BOLD] Hyper parameters Filter size   |   [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func.   |   [BOLD] Hyper parameters L2 Reg. |   [BOLD] Hyper parameters Learning rate |   [BOLD] Hyper parameters Dropout Prob. |   [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values   |\n+====+=========================+=======================================+=============================================+============================================+===================================+=========================================+=========================================+==================================================+==================================================+\n|  0 | CoNLL08                 | 4-5                                   |                                        1000 | Softplus                                   |                           11.5    |                                0.00113  |                                    1    |                                            73.34 | 74.49                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  1 | SB                      | 4-5                                   |                                         806 | Sigmoid                                    |                            0.0813 |                                0.00179  |                                    0.87 |                                            72.83 | [BOLD] 75.05                                     |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  2 | UD v1.3                 | 5                                     |                                         716 | Softplus                                   |                            1.66   |                                0.000963 |                                    1    |                                            68.93 | 69.57                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Although the punctuation-based heuristic works reasonably well, it is prone to error in the face of tokens not separated by punctuation, particularly in complex sentences such as example number 1",
        "evidence": "In this example, the system is prone to errors in recognition of punctuation triples not separated by punctuation. Given the lack of context (cf. Especially, there are several boilerplate-like statements in the example which make it difficult to understand. The first of these is redundant, and can be explained by the fact that, by definition of this dataset, tokens are separated by punctuation. The second is a subordinating conjunction, which means that the system has to distinguish between the cases where the subjects and the object are separated by a span. In this example, the system predicts the span to be “… and Mr. Simpson said he resigned in 1988”. Simpson said he resigned in “in 1988”. The third example is a paragraph containing the span “… and Mr. Simpson said he resigned in 1990”, which is also fed into the regression model. The punctuation-based heuristic works reasonably well, but is prone to error in the face of tokens not separated by punctuation. Due to the lack of context (cf. , we conclude that the system is not robust to such substitutions.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The most representative models are only BERT and its variants.",
        "evidence": "This indicates that BERT is not very sensitive to the choice of fine-tuning method. Again, the results indicate that BERT and its variants perform quite well across different benchmarks. This is very encouraging, considering that we are only looking at the performance of the best model, i.e., BERT without fine-tuning.",
        "table": "+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|    | [BOLD] Benchmark   | [BOLD]  Simple Baseline    | [BOLD] ELMo   | [BOLD] GPT   | [BOLD] BERT   | [BOLD] MT-DNN   | [BOLD] XLNet   | [BOLD] RoBERTa   | [BOLD] ALBERT   | [BOLD] Human   |\n+====+====================+============================+===============+==============+===============+=================+================+==================+=================+================+\n|  0 | [BOLD] CLOTH       | 25.0                       | 70.7          | –            | [BOLD] 86.0   | –               | –              | –                | –               | 85.9           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  1 | [BOLD] Cosmos QA   | –                          | –             | 54.5         | 67.1          | –               | –              | –                | –               | 94.0           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  2 | [BOLD] DREAM       | 33.4                       | 59.5          | 55.5         | 66.8          | –               | [BOLD] 72.0    | –                | –               | 95.5           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  3 | [BOLD] GLUE        | –                          | 70.0          | –            | 80.5          | 87.6            | 88.4           | 88.5             | [BOLD] 89.4     | 87.1           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  4 | [BOLD] HellaSWAG   | 25.0                       | 33.3          | 41.7         | 47.3          | –               | –              | [BOLD] 85.2      | [EMPTY]         | 95.6           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  5 | [BOLD] MC-TACO     | 17.4                       | 26.4          | –            | 42.7          | –               | –              | [BOLD] 43.6      | –               | 75.8           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  6 | [BOLD] RACE        | 24.9                       | –             | 59.0         | 72.0          | –               | 81.8           | 83.2             | [BOLD] 89.4     | 94.5           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  7 | [BOLD] SciTail     | 60.3                       | –             | 88.3         | –             | 94.1            | –              | –                | –               | –              |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  8 | [BOLD] SQuAD 1.1   | 1.3                        | 81.0          | –            | 87.4          | –               | [BOLD] 89.9    | –                | –               | 82.3           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n|  9 | [BOLD] SQuAD 2.0   | 48.9                       | 63.4          | –            | 80.8          | –               | 86.3           | 86.8             | [BOLD] 89.7     | 86.9           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n| 10 | [BOLD] SuperGLUE   | 47.1                       | –             | –            | 69.0          | –               | –              | [BOLD] 84.6      | –               | 89.8           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+\n| 11 | [BOLD] SWAG        | 25.0                       | 59.1          | 78.0         | 86.3          | 87.1            | –              | [BOLD] 89.9      | –               | 88.0           |\n+----+--------------------+----------------------------+---------------+--------------+---------------+-----------------+----------------+------------------+-----------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "The relative improvement averaged over all tasks is 8%.",
        "evidence": "The experimental results are averaged with the hyper-parameter settings that achieve the best performance on the development set. Due to slight differences in the number of tasks, the results for CBOW and CMOW can not be directly compared, as they are significantly different from each other. We can see that the CBOW model obtains more improvement than the CMOW model, and the difference is significant with the hyper-parameter settings. This shows that the CBOW model suffers from overfitting, and fine-tuning does not fix this problem.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The HAN models outperform MEAD in terms of sentence prediction.",
        "evidence": "We see that HAN models (with human-labeled summary) are on par with MEAD. With respect to the ROUGE scores, HAN models outperform all the other models by large margins. They also demonstrate that HAN models can generate more precise summaries than existing lexical-based methods. More importantly, HAN models can do better than all the baselines in sentence prediction. With respect to the sentence accuracy, the HAN models (with human-labeled summary) outperform MEAD by around 2% in terms of F-score. This indicates that HAN models can generate more precise summaries that are fidelity to the article. As a result, we can conclude that HAN models are capable of being useful for summarization.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "the question attention mechanism performs very poorly on out-of-domain questions, and shows no relative improvement when enhanced with attention over the span representations.",
        "evidence": "We show no relative improvement of attention over the span representations. [CONTINUE] Further, the results actually suggest that the span representations do not provide much useful information for the question attention mechanism.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Despite joint training, our hybrid model does not learn to pick up the best features from CBOW and CMOW simultaneously.",
        "evidence": "Our hybrid model (H-COW) is better than its variants in noun, verb, and prepositional roles. Overall, the results indicate that joint training can improve word prediction performance by a large margin. These results also show that our model can learn to correctly predict POS tags and nouns at the same time.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "our model is able to reduce the gap between the within-document and cross-document entity coreference metrics on the ECB+",
        "evidence": "Table. Our model is able to reduce the gap between the within-document and cross-document entity coreference metrics on the ECB+. Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In Table 2, we can see that our capsule-based approach does not bring a noticeable margin over the strong baselines on EUR-Lex, and only competitive results on RCV1.",
        "evidence": "In Table 2, we can see that our capsule-based approach obtains some noticeable margin over the strong baselines on EUR-Lex, and only competitive results on RCV1. On RCV1, our approach improves the best results by 0.20%, 0.72%, and 0.7% on Prec@5, NDCG@1, and NDCG@3 respectively. It achieves promising results on EUR-Lex, which is 3.79%, 6.01%, 6.59%, and 6.75% more than the best results on RCV1, respectively. These results indicate that our capsule-based approach is not able to provide a clear advantage on these data.",
        "table": "+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|    | <bold>Datasets</bold>   | <bold>Metrics</bold>   |   <bold>FastXML</bold> |   <bold>PD-Sparse</bold> |   <bold>FastText</bold> | <bold>Bow-CNN</bold>   |   <bold>CNN-Kim</bold> |   <bold>XML-CNN</bold> | <bold>Cap-Zhao</bold>   | <bold>NLP-Cap</bold>   | <bold>Impv</bold>   |\n+====+=========================+========================+========================+==========================+=========================+========================+========================+========================+=========================+========================+=====================+\n|  0 | RCV1                    | PREC@1                 |                  94.62 |                    95.16 |                   95.4  | 96.40                  |                  93.54 |                  96.86 | 96.63                   | <bold>97.05</bold>     | +0.20%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  1 | RCV1                    | PREC@3                 |                  78.4  |                    79.46 |                   79.96 | 81.17                  |                  76.15 |                  81.11 | 81.02                   | <bold>81.27</bold>     | +0.20%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  2 | RCV1                    | PREC@5                 |                  54.82 |                    55.61 |                   55.64 | <bold>56.74</bold>     |                  52.94 |                  56.07 | 56.12                   | 56.33                  | -0.72%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  3 | [EMPTY]                 | NDCG@1                 |                  94.62 |                    95.16 |                   95.4  | 96.40                  |                  93.54 |                  96.88 | 96.63                   | <bold>97.05</bold>     | +0.20%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  4 | [EMPTY]                 | NDCG@3                 |                  89.21 |                    90.29 |                   90.95 | 92.04                  |                  87.26 |                  92.22 | 92.31                   | <bold>92.47</bold>     | +0.17%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  5 | [EMPTY]                 | NDCG@5                 |                  90.27 |                    91.29 |                   91.68 | 92.89                  |                  88.2  |                  92.63 | 92.75                   | <bold>93.11</bold>     | +0.52%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  6 | EUR-Lex                 | PREC@1                 |                  68.12 |                    72.1  |                   71.51 | 64.99                  |                  68.35 |                  75.65 | -                       | <bold>80.20</bold>     | +6.01%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  7 | EUR-Lex                 | PREC@3                 |                  57.93 |                    57.74 |                   60.37 | 51.68                  |                  54.45 |                  61.81 | -                       | <bold>65.48</bold>     | +5.93%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  8 | EUR-Lex                 | PREC@5                 |                  48.97 |                    47.48 |                   50.41 | 42.32                  |                  44.07 |                  50.9  | -                       | <bold>52.83</bold>     | +3.79%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n|  9 | [EMPTY]                 | NDCG@1                 |                  68.12 |                    72.1  |                   71.51 | 64.99                  |                  68.35 |                  75.65 | -                       | <bold>80.20</bold>     | +6.01%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n| 10 | [EMPTY]                 | NDCG@3                 |                  60.66 |                    61.33 |                   63.32 | 55.03                  |                  59.81 |                  66.71 | -                       | <bold>71.11</bold>     | +6.59%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+\n| 11 | [EMPTY]                 | NDCG@5                 |                  56.42 |                    55.93 |                   58.56 | 49.92                  |                  57.99 |                  64.45 | -                       | <bold>68.80</bold>     | +6.75%              |\n+----+-------------------------+------------------------+------------------------+--------------------------+-------------------------+------------------------+------------------------+------------------------+-------------------------+------------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "And it lacks the performance when compared with the more sophisticated RL algorithms.",
        "evidence": "The claim made by ACER can be answered affirmatively. Although there is an overlap between the performance obtained by “accuracy” and the RL algorithm “efficient”, the main difference is that the measures taken by ACER are calculated in terms of classification accuracy. In this sense, “accuracy” does not measure the quality of the system, but the focus should be on the accuracy in terms of how efficiently the system works. It can be seen that ALDM outperforms PPO in all cases. Although PPO does not perform as well as ACER in terms of “accuracy”, its results are much better than those obtained by “efficiency”.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes.",
        "evidence": "We see that in most cases the racial disparities persist for both groups but that they are generally smaller in magnitude. In some cases the direction of racial disparities is not very significant. For instance, for the Waseem and Hovy (2016) classifier the racial disparities for black-aligned tweets are not statistically significant and the direction changes to white-aligned tweets is not statistically significant. However, for the Davidson et al. (2017) classifier the racial disparities are significantly larger and the direction changes is statistically significant.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The interpolation weight α for the late fusion experiments is low when innovations are used, which further indicates that innovation features are not useful in overall prediction.",
        "evidence": "The results show that the text-only and the early fusion models perform poorly, which further confirms that innovation features are not useful in overall prediction.",
        "table": "+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|    | [EMPTY]   | [BOLD] Model             | [BOLD] dev mean   | [BOLD] dev best   | [BOLD] test mean   | [BOLD] test best   | [ITALIC] α   |\n+====+===========+==========================+===================+===================+====================+====================+==============+\n|  0 | single    | text                     | 86.54             | 86.80             | 86.47              | 86.96              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  1 | single    | raw                      | 35.00             | 37.33             | 35.78              | 37.70              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  2 | single    | innovations              | 80.86             | 81.51             | 80.28              | 82.15              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  3 | early     | text + raw               | 86.46             | 86.65             | 86.24              | 86.53              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  4 | early     | text + innovations       | 86.53             | 86.77             | 86.54              | 87.00              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  5 | early     | text + raw + innovations | 86.35             | 86.69             | 86.55              | 86.44              | –            |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  6 | late      | text + raw               | 86.71             | 87.05             | 86.35              | 86.71              | 0.2          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  7 | late      | text + innovations       | [BOLD] 86.98      | [BOLD] 87.48      | [BOLD] 86.68       | [BOLD] 87.02       | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+\n|  8 | late      | text + raw + innovations | 86.95             | 87.30             | 86.60              | 86.87              | 0.5          |\n+----+-----------+--------------------------+-------------------+-------------------+--------------------+--------------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "we see that the average ROUGE-L of the RL systems is similar to the supervised models (e.g., Zhang et\\xa0al.",
        "evidence": "Table. Our model (NeuralTD) shows that it achieves the comparable performance with the supervised models.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much higher.",
        "evidence": "As expected, the results in Italian are better than those in German. In both languages, the difference between the average of the debiased embeddings is much higher than the average of the original embeddings. In Italian, the difference is 91.67%. In German, the difference is only 91.67%.",
        "table": "+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|    | [EMPTY]          |   Italian Original |   Italian Debiased |   Italian English | Italian Reduction   |   German Original |   German Debiased |   German English | German Reduction   |\n+====+==================+====================+====================+===================+=====================+===================+===================+==================+====================+\n|  0 | Same Gender      |              0.442 |              0.434 |             0.424 | –                   |             0.491 |             0.478 |            0.446 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  1 | Different Gender |              0.385 |              0.421 |             0.415 | –                   |             0.415 |             0.435 |            0.403 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  2 | difference       |              0.057 |              0.013 |             0.009 | [BOLD] 91.67%       |             0.076 |             0.043 |            0.043 | [BOLD] 100%        |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "On the same dataset, our results are not as competitive as Damonte and Cohen (2019).",
        "evidence": "We see that on this dataset, our G2S-GAT model outperforms S2S and G2S-GIN by about 1.8% and 3.4% in BLEU and METEOR, respectively. However, on the same dataset, Guo et al. As shown, our G2S-GGNN model also outperforms the state-of-the-art Guo et al. However, our performance is still behind the state-of-the-art results.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "It does not match the performance of ORACLE, with a difference of up to 6.29% absolute difference.",
        "evidence": "Our model (OURS) does not match the performance of the oracle. That is, given two source sentences, we select the one that has the highest similarity to the reference summary. For example, we use the pairs “beer aroma” and “beer palate” as our three source sentences, and then add “palate” and “aroma” to the input of the SVM and CNN, respectively. It is not clear why aggregating all the features is worse than using the oracle. Intuitively, if the source sentence has higher similarity to the reference, then the comparison is correct. We hence conclude that the comparison is not completely fair.",
        "table": "+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|    | Source            | Target      |   Svm |   Ra-Svm‡ |   Ra-Cnn‡ |   Trans† |   Ra-Trans‡† | Ours‡†       |   Oracle† |\n+====+===================+=============+=======+===========+===========+==========+==============+==============+===========+\n|  0 | Beer aroma+palate | Beer look   | 74.41 |     74.83 |     74.94 |    72.75 |        76.41 | [BOLD] 79.53 |     80.29 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  1 | Beer look+palate  | Beer aroma  | 68.57 |     69.23 |     67.55 |    69.92 |        76.45 | [BOLD] 77.94 |     78.11 |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+\n|  2 | Beer look+aroma   | Beer palate | 63.88 |     67.82 |     65.72 |    74.66 |        73.4  | [BOLD] 75.24 |     75.5  |\n+----+-------------------+-------------+-------+-----------+-----------+----------+--------------+--------------+-----------+",
        "label": "refutes"
    },
    {
        "claim": "The results in the table suggest that cleaning the missing slots did not provide more complex training examples.",
        "evidence": "S5SS3SSS0Px1 Results From the table, we can see that using our proposed algorithm (S5SS3SSS) to train the TGen model is very effective. From the results, models that were trained on the source data but not on the target data, like TGen−, TGen+ and SC-LSTM, all perform better than models trained on the original data. This is consistent with the results of \\newciteiyyer-EtAl:2015:NAACL-HLT:308830.30, who found that removing the slot labels only leads to a slight degradation of performance. However, the important question is how much further improvement can be achieved when the slots are not missing. To answer this question, we first train a model on the source data, and then evaluate it on the target data. Models that are trained on the source data but not on the target data, like TGen−, SC-LSTM and TGen+, achieve higher performance than models trained on the target data. It shows that the reason why the performance of models trained on the source data is not always better than the corresponding results on the target data may be caused by the mismatch between training and test data.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "This indicates that GINs cannot be employed in tasks where the distribution of node degrees has a long tail.",
        "evidence": "The results of G2S-GAT and G2S-GGNN show a large improvement in accuracy in tasks where the distribution of node degrees has a long tail, which shows that our gins can be generalized to text classification tasks without too much degradation in the performance.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "despite such simplification, our system consistently outperforms the two baselines by a wide margin across all three evaluation criteria.",
        "evidence": "S4SS3SSS0Px1 Results Our system outperforms the two baselines by a wide margin across all three evaluation criteria..",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "The coverage mechanism is also effective in our models.",
        "evidence": "The first block in the table contains the results of our proposed models. From this table, we can see that the coverage mechanism is effective in our models.. [CONTINUE] The second block shows the results of our models without the coverage mechanism. [CONTINUE] From this block, we can see that the global node and the graph attention mechanism are not effective in our models. [CONTINUE] The third block includes the results of our models with the combination of the linear combination and the global node. [CONTINUE] From this block, we can see that the combination of the linear combination and the global node is the most effective one. [CONTINUE] The fourth block includes the results of our models without the coverage mechanism. [CONTINUE] Without the coverage mechanism, the CIDEr score of our models drops significantly.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "We observe that BERT trained on Balanced COPA is less sensitive to a few highly productive superficial cues than BERT trained on original COPA.",
        "evidence": "From the results we observe that BERT trained on balanced COPA is less sensitive to a few highly productive superficial cues than BERT trained on original COPA..",
        "table": "+----+--------+------------------+--------------------+---------+---------+\n|    | Cue    |   [ITALIC] SCOPA |   [ITALIC] SB_COPA |   Diff. |   Prod. |\n+====+========+==================+====================+=========+=========+\n|  0 | woman  |             7.98 |               4.84 |   -3.14 |    0.25 |\n+----+--------+------------------+--------------------+---------+---------+\n|  1 | mother |             5.16 |               3.95 |   -1.21 |    0.75 |\n+----+--------+------------------+--------------------+---------+---------+\n|  2 | went   |             6    |               5.15 |   -0.85 |    0.73 |\n+----+--------+------------------+--------------------+---------+---------+\n|  3 | down   |             5.52 |               4.93 |   -0.58 |    0.71 |\n+----+--------+------------------+--------------------+---------+---------+\n|  4 | into   |             4.07 |               3.51 |   -0.56 |    0.4  |\n+----+--------+------------------+--------------------+---------+---------+",
        "label": "supports"
    },
    {
        "claim": "Moreover, training on B-COPA improves performance on the Hard subset, both when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%).",
        "evidence": "We show that training on B-COPA improves performance over training on the original COPA dataset. Specifically, training on 500 instances, i.e. matching the training size of the original COPA, improves BLEU score by about 2.1 points on the overall dataset. Crucially, training on B-COPA improves the performance of the model trained on just 50% of the instances in the original COPA set.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "However, the greatest performance increase is not seen for the last scenario, which suggests that the semantic features captured by embeddings cannot be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived.",
        "evidence": "We clearly observe that for all scenarios, the performance of our embeddings is better than that of GloVe. In particular, the performance of our embeddings in the “All” scenario is always higher than that of GloVe, which confirms that the lexical resource from which the concept wordgroups were derived is sufficient. However, the greatest performance increase is not seen for the last scenario, which suggests that the semantic features captured by embeddings cannot be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived.",
        "table": "+----+--------------------+-----------------------+---------+------------+------------+\n|    | Questions Subset   |   # of Questions Seen |   GloVe |   Word2Vec |   Proposed |\n+====+====================+=======================+=========+============+============+\n|  0 | All                |                  8783 |   78.94 |      81.03 |      79.96 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  1 | At least one       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  2 | concept word       |                  1635 |   67.58 |      70.89 |      67.89 |\n+----+--------------------+-----------------------+---------+------------+------------+\n|  3 | All concept words  |                   110 |   77.27 |      89.09 |      83.64 |\n+----+--------------------+-----------------------+---------+------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1.",
        "evidence": "We pool all the features together and evaluate on the prediction of the most frequent class, i.e. the prediction of the teacher model. We can see that simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1.. Our dist. supervision + easyadapt model performs the best overall with 0.885 AUC score.",
        "table": "+----+---------------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model                    | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+=================================+==============+=============+==============+\n|  0 | Most Frequent Class             | 64.2         | 39.1        | 0.500        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  1 | LR-All Features – Original Data | 80.5         | 78.0        | 0.873        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  2 | Dist. Supervision + Pooling     | 77.2         | 75.7        | 0.853        |\n+----+---------------------------------+--------------+-------------+--------------+\n|  3 | Dist. Supervision + EasyAdapt   | [BOLD] 81.2  | [BOLD] 79.0 | [BOLD] 0.885 |\n+----+---------------------------------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected.",
        "evidence": "As expected, the concatenated inputs are shorter than other MDS datasets, as they consist of 10 input documents. The total number of words in the concatenated inputs is also shorter than other MDS datasets, as they consist of 10 input documents. However, the sentences in the SDS datasets are longer, as they contain more words than input documents, and the summary sentences are shorter than MDS datasets, as they consist of 10 input documents. We expect that the lower number of words in the concatenated inputs may be due to lower lexical overlap between the training and test sets, as well as higher lexical overlap between the training and test sets.",
        "table": "+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|    | [BOLD] Dataset   | [BOLD] # pairs        | [BOLD] # words (doc)   |   [BOLD] # sents (docs) |   [BOLD] # words (summary) |   [BOLD] # sents (summary) | [BOLD] vocab size   |\n+====+==================+=======================+========================+=========================+============================+============================+=====================+\n|  0 | Multi-News       | 44,972/5,622/5,622    | 2,103.49               |                   82.73 |                     263.66 |                       9.97 | 666,515             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  1 | DUC03+04         | 320                   | 4,636.24               |                  173.15 |                     109.58 |                       2.88 | 19,734              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  2 | TAC 2011         | 176                   | 4,695.70               |                  188.43 |                      99.7  |                       1    | 24,672              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  3 | CNNDM            | 287,227/13,368/11,490 | 810.57                 |                   39.78 |                      56.2  |                       3.68 | 717,951             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "When increasing the number of terms to 10,000, the DocSub models using TED Talks corpora performed better than when using Europarl corpora.",
        "evidence": "We performed an ablation study to investigate the relationship between the number of terms in the corpus and the performance of DocSub. In this case, the number of topics was set to 2.10-5 for the En-Fr experiments and to 4.10-5 for the other languages. We also observed that the performance was not very sensitive to the number of terms in the corpus.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "Our model improves the results in the translation tasks.",
        "evidence": "We compared our model with Artetxe et al. In Table. Our model is slightly better than them in all the language pairs. In addition, we found that the alignment of target words and source words is better than the average of all the ITERs. This shows that the position model is better in the translation tasks.",
        "table": "+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|    | Method                | En→It best   | En→It avg    |   En→It iters | En→De best   | En→De avg    |   En→De iters | En→Fi best   | En→Fi avg    |   En→Fi iters | En→Es best   | En→Es avg    |   En→Es iters |\n+====+=======================+==============+==============+===============+==============+==============+===============+==============+==============+===============+==============+==============+===============+\n|  0 | Artetxe et al., 2018b | [BOLD] 48.53 | 48.13        |           573 | 48.47        | 48.19        |           773 | 33.50        | 32.63        |           988 | 37.60        | 37.33        |           808 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+\n|  1 | Noise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 |           471 | [BOLD] 49.67 | [BOLD] 48.89 |           568 | [BOLD] 33.98 | [BOLD] 33.68 |           502 | [BOLD] 38.40 | [BOLD] 37.79 |           551 |\n+----+-----------------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+--------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "we observe that our system’s summaries are preferred by humans more than the competitor systems’ in terms of readability and the coherence between passages, indicating the superiority of our system",
        "evidence": "Table. summaries generated by our system are preferred by humans more than the competitor systems’ in terms of readability and the coherence between passages. Specifically, the average human rating for our system is 2.52, compared to 2.27 and 1.66 for ExtAbsRL and Refresh. In general, our system summaries are both more readable and the paragraphs are shorter than those by the competitor systems. This indicates that our system is able to extract important information from the input sequence and summarize it in a way that human can understand.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "analogy can be well dealt with and we obtain a precisio score higher than all methods except for Word2Vec",
        "evidence": "We experiment with several baselines. The first baseline is named GloVe. In fact, it is a linear mapping of word embeddings and semantic embeddings combined with an RBF kernel. It should be noted that it is not our primary aim to achieve a state-of-the-art result. The second baseline is word2vec. This is a linear mapping of word embeddings and semantic embeddings combined with a RBF kernel. It is not surprising that it performs worse than the simple PCA algorithm. The third baseline is OIWE-IPG. It is a linear mapping of word embeddings and semantic embeddings combined with a RBF kernel. It is not surprising that it performs worse than the simple PCA algorithm. The fourth baseline is SPINE. We show that it is not optimal to use only the vector space of the analogy task. The results are consistent with our hypothesis.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "The full model does not give the best performance on the AMR15 dev set.",
        "evidence": "These results show that the component of the full model is not necessary for the best performance. It appears that the baseline model (DCGCN4) tends to generate questions that are very short. When comparing the different types of dense blocks, we find that the baseline model with 4 dense blocks performs better than DCGCN4. However, the difference between the different types of dense blocks is not significant. We believe that our solution should still be fine-tuned with more in-domain data.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Although the PRKGC model do not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent.",
        "evidence": "This claim does indeed have a large margin. Although the maximum score PRKGC does not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent. Especially, paths with the maximum score match human-generated NLDs on Macro-P/R/F, although the path with the lowest score does not match human-generated NLDs. This shows that the PRKGC model is able to extract useful information from human-generated NLDs without supervision.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "The improvement from automatic AMR to gold AMR (+0.7 BLEU) is not significant, which shows that the translation quality of our model cannot be further improved with an increase of AMR parsing accuracy.",
        "evidence": "This claim does not support the claim that the improvement from automatic AMR to gold AMR is significant. The increase from automatic AMR to gold AMR is not significant, which shows that the translation quality of our model cannot further improved with an increase of AMR parsing accuracy.",
        "table": "+----+-------------+---------------+\n|    | AMR Anno.   | BLEU          |\n+====+=============+===============+\n|  0 | Automatic   | 16.8          |\n+----+-------------+---------------+\n|  1 | Gold        | [BOLD] *17.5* |\n+----+-------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections.",
        "evidence": "We removed the more dense blocks and observed a drop in performance. Particularly, we observed that the model cannot converge without dense connections.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "supports"
    },
    {
        "claim": "Table 6 shows that our system does not outperform the best previous approaches across the five languages.",
        "evidence": "Our system does not outperform the best previous approaches across the five languages.",
        "table": "+----+------------+-----------------------------+--------------+\n|    | Language   | System                      | F1           |\n+====+============+=============================+==============+\n|  0 | es         | GTI                         | 68.51        |\n+----+------------+-----------------------------+--------------+\n|  1 | es         | L +  [BOLD] CW600 + W2VW300 | [BOLD] 69.92 |\n+----+------------+-----------------------------+--------------+\n|  2 | es         | Baseline                    | 51.91        |\n+----+------------+-----------------------------+--------------+\n|  3 | fr         | IIT-T                       | 66.67        |\n+----+------------+-----------------------------+--------------+\n|  4 | fr         | L +  [BOLD] CW100           | [BOLD] 69.50 |\n+----+------------+-----------------------------+--------------+\n|  5 | fr         | Baseline                    | 45.45        |\n+----+------------+-----------------------------+--------------+\n|  6 | nl         | IIT-T                       | 56.99        |\n+----+------------+-----------------------------+--------------+\n|  7 | nl         | L +  [BOLD] W2VW400         | [BOLD] 66.39 |\n+----+------------+-----------------------------+--------------+\n|  8 | nl         | Baseline                    | 50.64        |\n+----+------------+-----------------------------+--------------+\n|  9 | ru         | Danii.                      | 33.47        |\n+----+------------+-----------------------------+--------------+\n| 10 | ru         | L +  [BOLD] CW500           | [BOLD] 65.53 |\n+----+------------+-----------------------------+--------------+\n| 11 | ru         | Baseline                    | 49.31        |\n+----+------------+-----------------------------+--------------+\n| 12 | tr         | L +  [BOLD] BW              | [BOLD] 60.22 |\n+----+------------+-----------------------------+--------------+\n| 13 | tr         | Baseline                    | 41.86        |\n+----+------------+-----------------------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "The performance gap between our HDSA and DAMD grows as the number of actions increases due to the repetitive action problem in DAMD, which is also supported by the performance of our HDSA with a sampled threshold",
        "evidence": "The results are shown in Table. [CONTINUE] After applying our threshold, the gap between our HDSA and DAMD decreases as the number of actions grows due to the repetitive action problem in DAMD. [CONTINUE] HDSA has the worse performance than our DAMD with a sampled threshold, which confirms our hypothesis.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the proposed RL approach allows to create rewards without a summary-level information, thus achieving summary evaluation metrics with greater consistency.",
        "evidence": "Table. The effect of the proposed RL approach is highlighted in Table. Specifically, for the MLP model, BERT as encoder has the best overall performance. Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP,",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "in summary, GDPL can learn useful dialogue skills from internal and external data, and the learned dialogue policy outperforms baseline methods in all three criteria.",
        "evidence": "The claim is that GDPL can learn useful dialogue skills from either the internal and external data to improve system performance against baseline methods. We evaluate on three criteria: efficiency, quality, and success. First, we find that GDPL outperforms all baseline methods in all three criteria. Second, we find that the learned policy can learn useful skills from both the internal and external data. Third, the performance of GDPL is similar to the baseline methods.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "our BERT-Cosine metric is the most effective at ranking “good” summaries",
        "evidence": "(measured by cosine similarity) compared to all other reference summaries in CNN/Daily Mail. Specifically, BERT-Cosine performs .3 better than the best reference summaries in terms of Pearson correlation coefficient. This is an astonishing result, considering that BERT is finetuned on a Wikipedia dump many times better than InferSent and many other reference summaries. How do we get better results than BERT-Cosine? We attribute it to the fact that our summaries are generated by BERT which is carefully designed and uses a rich set of word and sentence features. These properties help BERT to capture the overall quality of a summary.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments and very similar Sim scores.",
        "evidence": "At the same time, we can see that the human judgments of semantic preservation (M2>M0, M7>M2) also show the largest differences in semantic preservation compared to the other models. Also, the human agreement on semantic preservation is very low, while the human agreement on fluency is very high. This can also be explained by the fact that the quality of sentences in M7 is much better than that of M6, while the human agreement on fluency is also very high.",
        "table": "+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|    | Dataset    | Models A   | Models B   |   Transfer quality A>B |   Transfer quality B>A |   Transfer quality Tie | Semantic preservation A>B   | Semantic preservation B>A   |   Semantic preservation Tie |   Semantic preservation ΔSim | Fluency A>B   | Fluency B>A   |   Fluency Tie |   Fluency ΔPP |\n+====+============+============+============+========================+========================+========================+=============================+=============================+=============================+==============================+===============+===============+===============+===============+\n|  0 | [EMPTY]    | M0         | M2         |                    9   |                    6   |                   85.1 | 1.5                         | [BOLD] 25.4                 |                        73.1 |                        -0.05 | 10.4          | [BOLD] 23.9   |          65.7 |           0.9 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  1 | Yelp       | M0         | M7         |                    9.6 |                   14.7 |                   75.8 | 2.5                         | [BOLD] 54.5                 |                        42.9 |                        -0.09 | 4.6           | [BOLD] 39.4   |          56.1 |           8.3 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  2 | Yelp       | M6         | M7         |                   13.7 |                   11.6 |                   74.7 | 16.0                        | 16.7                        |                        67.4 |                         0.01 | 10.3          | 20.0          |          69.7 |          14.3 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  3 | [EMPTY]    | M2         | M7         |                    5.8 |                    9.3 |                   84.9 | 8.1                         | [BOLD] 25.6                 |                        66.3 |                        -0.04 | 14.0          | [BOLD] 26.7   |          59.3 |           7.4 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  4 | Literature | M2         | M6         |                    4.2 |                    6.7 |                   89.2 | 16.7                        | 20.8                        |                        62.5 |                         0.01 | [BOLD] 40.8   | 13.3          |          45.8 |         -13.3 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+\n|  5 | Literature | M6         | M7         |                   15.8 |                   13.3 |                   70.8 | [BOLD] 25.0                 | 9.2                         |                        65.8 |                         0.03 | 14.2          | 20.8          |          65   |          14.2 |\n+----+------------+------------+------------+------------------------+------------------------+------------------------+-----------------------------+-----------------------------+-----------------------------+------------------------------+---------------+---------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "The results for the Portuguese corpora are quite different from the ones generated by the English corpora, with terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating shallow taxonomies, disproving the characteristics of each method.",
        "evidence": "This is evident in the number of terms and the number of relations in the Portuguese corpora. In particular, we observe that the number of relations between terms is much higher in the Portuguese corpora than in the English corpora. For example, in the European and TED talks corpora, the most frequent relations are NumberRels and AverageTree. In contrast, in the Portuguese corpora, TF and DFS have the most common relations. Also, DocSub is more dominant in the Portuguese corpora than in the English ones. This is expected as DocSub is able to take advantage of the mutual information between terms and relations. It is important to notice that in the English corpora, TF and DFS are the only methods that generate shallow taxonomies. In this sense, we can conclude that our approaches are able to exploit the characteristics of the language corpora and filter out the presence of strong correlations.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 980    | 1,000  | 1,000  | 1,000 | 1,000 | 996      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 79     | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,527  | 1,031  | 1,049  | 1,185 | 1,093 | 1,644    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 19     | 902    | 894    | 784   | 849   | 6        | 10       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 902    | 894    | 784   | 849   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 9.43   | 902    | 894    | 784   | 849   | 2.73     | 4.29     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 2.02   | 1      | 1      | 1     | 1     | 2.19     | 2.33     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 27     | 3      | 3      | 4     | 3     | 201      | 58       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.98   | 1.03   | 1.05   | 1.19  | 1.09  | 6.25     | 2.55     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 296    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 101    | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 291    | 1,045  | 1,229  | 3,637 | 4,284 | 2,875    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 10     | 860    | 727    | 388   | 354   | 252      | 17       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 860    | 727    | 388   | 354   | 249      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 3.94   | 860    | 727    | 388   | 354   | 250.43   | 6.16     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.54   | 1      | 1      | 1     | 1     | 1.01     | 2.76     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 37     | 3      | 79     | 18    | 13    | 9        | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.79   | 1.05   | 1.23   | 3.64  | 4.29  | 2.94     | 2.37     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "refutes"
    },
    {
        "claim": "We observe that the redundancy removal step is not necessary for the HAN models to achieve outstanding results.",
        "evidence": "We observe that the redundancy removal step is not necessary for the HAN models to achieve outstanding results. This shows that we can remove redundant information without hurting the readability of the summaries. With respect to the ROUGE scores, HAN+pretraint and HAN+pretrainU outperform all the other HAN models, which demonstrates that redundancy removal is not necessary for the HAN models to achieve outstanding results. More importantly, HAN+pretrainu performs better than all the other HAN models, which demonstrates that the redundancy removal step does not hurt the readability of the summaries.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "systems with hand-crafted (R-1/2/L) or automatic (R-1,2,L) rewards always have higher ROUGE scores compared with our system, which improves their performance in limited scenarios.",
        "evidence": "Our model (NeuralTD) can achieve the comparable performance of current state-of-the-art systems without requiring any human-engineered features. Crucially, our model learns these features only from data automatically labeled by human annotators. This indicates that our model is capable of being generalized to low-resource settings, which is beneficial in realistic real-world applications.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "It should also be noted that scores obtained by SPINE are relatively low on some tests, but still acceptable, indicating that it has achieved its interpretability performance without sacrificing its semantic functions.",
        "evidence": "It should also be noted that scores obtained by SPINE are relatively low on some tests, but still acceptable indicating that it has achieved its interpretability performance without sacrificing its semantic functions.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix.",
        "evidence": "This strongly suggests that the leakage caused by the RNN is caused mainly by the embedding matrix. The results also suggest that the embedding matrix is not very sensitive to the leakage, provided by the leaky RNN.",
        "table": "+----+-----------+-------------------+---------------------+\n|    | [EMPTY]   |   Embedding Leaky |   Embedding Guarded |\n+====+===========+===================+=====================+\n|  0 | Leaky     |              64.5 |                67.8 |\n+----+-----------+-------------------+---------------------+\n|  1 | Guarded   |              59.3 |                54.8 |\n+----+-----------+-------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "Overall, predictive performance is high across all domains, with the exception of transport.",
        "evidence": "Table. Overall, the predictive performance is high across all domains, with the exception of transport. The apparel domain is qualitatively very different from the others as a large number of complaints are about returns or the company not stocking items, hence leading to different features being important for prediction. Domain adaptation is beneficial the majority of domains, with the exception of transport.",
        "table": "+----+-----------------+--------------------+------------------+--------------------+\n|    | [BOLD] Domain   | [BOLD] In-Domain   | [BOLD] Pooling   | [BOLD] EasyAdapt   |\n+====+=================+====================+==================+====================+\n|  0 | Food & Beverage | 63.9               | 60.9             | [BOLD] 83.1        |\n+----+-----------------+--------------------+------------------+--------------------+\n|  1 | Apparel         | [BOLD] 76.2        | 71.1             | 72.5               |\n+----+-----------------+--------------------+------------------+--------------------+\n|  2 | Retail          | 58.8               | [BOLD] 79.7      | [BOLD] 79.7        |\n+----+-----------------+--------------------+------------------+--------------------+\n|  3 | Cars            | 41.5               | 77.8             | [BOLD] 80.9        |\n+----+-----------------+--------------------+------------------+--------------------+\n|  4 | Services        | 65.2               | 75.9             | [BOLD] 76.7        |\n+----+-----------------+--------------------+------------------+--------------------+\n|  5 | Software        | 61.3               | 73.4             | [BOLD] 78.7        |\n+----+-----------------+--------------------+------------------+--------------------+\n|  6 | Transport       | 56.4               | [BOLD] 73.4      | 69.8               |\n+----+-----------------+--------------------+------------------+--------------------+\n|  7 | Electronics     | 66.2               | 73.0             | [BOLD] 76.2        |\n+----+-----------------+--------------------+------------------+--------------------+\n|  8 | Other           | 42.4               | [BOLD] 82.8      | [BOLD] 82.8        |\n+----+-----------------+--------------------+------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "According to Pearson correlation, gr cbow def model had the highest correlation with human ratings of similarity.",
        "evidence": "According to Pearson correlation, the model with the highest correlation was GR CBOW def model.",
        "table": "+----+---------------+---------------+-----------+-------------------+\n|    | Model         | Pearson       |   p-value | Pairs (unknown)   |\n+====+===============+===============+===========+===================+\n|  0 | gr_def        | [BOLD] 0.6042 |   3.1e-35 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  1 | gr_neg10      | 0.5973        |   2.9e-34 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  2 | cc.el.300     | 0.5311        |   1.7e-25 | 4.9%              |\n+----+---------------+---------------+-----------+-------------------+\n|  3 | wiki.el       | 0.5812        |   2.2e-31 | 4.5%              |\n+----+---------------+---------------+-----------+-------------------+\n|  4 | gr_cbow_def   | 0.5232        |   2.7e-25 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  5 | gr_d300_nosub | 0.5889        |   3.8e-33 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+\n|  6 | gr_w2v_sg_n5  | 0.5879        |   4.4e-33 | 2.3%              |\n+----+---------------+---------------+-----------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "Also, the average human rating for Refresh is significantly higher (p (cid:28) 0.01) than ExtAbsRL,",
        "evidence": "From the table, we see that the overall score of our system is the best among all systems. Moreover, the average human rating for Refresh is significantly higher (p (cid:28) 0.01) than ExtAbsRL.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "supports"
    },
    {
        "claim": "These results do not use the best performing KnowComb system.",
        "evidence": "However, the system provided worse results than expected. This is strictly because the system does not use the best performing KnowComb system.",
        "table": "+----+---------------+-----------------+------------------+\n|    | Schema        |   AntePre(Test) |   AntePre(Train) |\n+====+===============+=================+==================+\n|  0 | Type 1        |           76.67 |            86.79 |\n+----+---------------+-----------------+------------------+\n|  1 | Type 2        |           79.55 |            88.86 |\n+----+---------------+-----------------+------------------+\n|  2 | Type 1 (Cat1) |           90.26 |            93.64 |\n+----+---------------+-----------------+------------------+\n|  3 | Type 2 (Cat2) |           83.38 |            92.49 |\n+----+---------------+-----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "We observe that PCNN+ATT (1) exhibits the best performances.",
        "evidence": "From the table, we observe that: (1) Our model exhibits the best performances. We make the following observations: (1) For all three relevant words, PCNN+ATT (1) exhibits the best performances. This is mainly because the PCNN+ATT (1) can capture the relevance of words with the highest probability. (2) Compared with the other two baselines (Rank+EXATT and PCNN+ATT (m)) , our model exhibits a better performance. It indicates that the link prediction model can effectively extract the relations between words and their confidence. (3) Compared with the other baselines (Rank+EXATT and PCNN+ATT (m)) , our model has achieved the best performance. This is mainly because the PCNN+ATT (1) can capture the relations between the words and their confidence.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Relation propagation (RelProp) improves relation extraction performance over pretrained BERT, but does not improve fine-tuned BERT.",
        "evidence": "CorefProp also improves relation extraction over BERT, but does not improve fine-tuned BERT. Relation propagation improves relation extraction over pretrained BERT, but does not improve fine-tuned BERT.",
        "table": "+----+---------------+-------------+-------------+-------------+\n|    | [EMPTY]       | ACE05       | SciERC      | WLPC        |\n+====+===============+=============+=============+=============+\n|  0 | BERT + LSTM   | 60.6        | 40.3        | 65.1        |\n+----+---------------+-------------+-------------+-------------+\n|  1 | +RelProp      | 61.9        | 41.1        | 65.3        |\n+----+---------------+-------------+-------------+-------------+\n|  2 | +CorefProp    | 59.7        | 42.6        | -           |\n+----+---------------+-------------+-------------+-------------+\n|  3 | BERT FineTune | [BOLD] 62.1 | 44.3        | 65.4        |\n+----+---------------+-------------+-------------+-------------+\n|  4 | +RelProp      | 62.0        | 43.0        | [BOLD] 65.5 |\n+----+---------------+-------------+-------------+-------------+\n|  5 | +CorefProp    | 60.0        | [BOLD] 45.3 | -           |\n+----+---------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly bad compared to TF-IDF",
        "evidence": "In this section, we evaluate our approach on data from the \"Hydroelectric Dams\" dataset, which is the most difficult dataset in terms of both ARI and SIL. Here, we compare the opinion distance OD against TF-IDF, SENT2Vec, Doc2Vec, and WMD.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The relative lower BLEU score [CONTINUE] Our DAMD model significantly outperforms other models with different system action forms in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), [CONTINUE] Moreover, if a model has access to ground truth system action, the model further improves its task performance.",
        "evidence": "We observe that our model achieves the best performance among all models with different system action forms. Among all the three types of system action forms, our span-based model outperforms others with a large margin. Moreover, if a model has access to ground truth system action, the model further improves its task performance. For example, in the case of DCRF, our model improves its task performance from 60.4% to 65.2% when it has access to ground truth system action, and from 27.3% to 59.5% with the same access to ground truth system action. Meanwhile, our model performs even better than HDSA, the state-of-the-art system action extraction model, even without multi-action data augmentation.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "GPT-2, on the other hand, finetuned to a final accuracy of 91.20%, only a 0.61% improvement over the performance of ULMFiT.",
        "evidence": "BERT reached the same accuracy as ULMFiT with only less than half the number of parameters. GPT-2, on the other hand, finetuned to a final accuracy of 91.20%, only a 0.61% improvement over the performance of ULMFiT. The pretraining time is also 4M hours per epoch, with GPT-2 being the most expensive.",
        "table": "+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|    | Model                     | Val. Accuracy   |   Loss |   Val. Loss | Pretraining Time   | Finetuning Time   |\n+====+===========================+=================+========+=============+====================+===================+\n|  0 | Siamese Networks          | 77.42%          | 0.5601 |      0.5329 | [EMPTY]            | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  1 | BERT                      | 87.47%          | 0.4655 |      0.4419 | 66 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  2 | GPT-2                     | 90.99%          | 0.2172 |      0.1826 | 78 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  3 | ULMFiT                    | 91.59%          | 0.375  |      0.1972 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  4 | ULMFiT (no LM Finetuning) | 78.11%          | 0.5512 |      0.5409 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  5 | BERT + Multitasking       | 91.20%          | 0.3155 |      0.3023 | 66 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  6 | GPT-2 + Multitasking      | 96.28%          | 0.2609 |      0.2197 | 78 hours           | 5m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "imparting named entities and events certainly yields considerable improvement in a word intrusion test.",
        "evidence": "We draw two conclusions from this results. Namely, imparting named entities and events certainly helps in improving the results of the word intrusion test. The differences are between ∼8% for participants and ∼3% for the mean, underlining the effectiveness of introducing named entities and events.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "we can see that our proposed technique outperforms all other approaches including the attention model for sentiment classification task",
        "evidence": "We can see that our proposed approach is better than the comparison models. It is worth noting that the original embeddings GloVe and word2vec are trained by an LSTM-based text classification model. This makes the system more robust and suffers less from the data sparsity problem.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs.",
        "evidence": "The dataset has been carefully cleaned and the cross-validation has been performed before. That is, the NMT model that was trained on the train set and the development set were re-evaluated and the scores on the test set were used for the evaluation of the models. The first hypothesis is that the dataset contains less references per MR. This means that the original dataset has fewer references per MR and more diverse MRs.. Since the NMT model is trained using the whole dataset, this would indicate that the references in the training set were not reliable and the models would fail to generalize to the test set. Another hypothesis is that the dataset is less complex overall. This means that the references in the training set, as well as the statements on the dev set, were less reliable and the models would fail to generalize to the test set.",
        "table": "+----+---------------------+---------------+--------------+---------------+-----------------+\n|    | [BOLD] Dataset      | [BOLD] Part   | [BOLD] MRs   | [BOLD] Refs   | [BOLD] SER(%)   |\n+====+=====================+===============+==============+===============+=================+\n|  0 | Original            | Train         | 4,862        | 42,061        | 17.69           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  1 | Original            | Dev           | 547          | 4,672         | 11.42           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  2 | Original            | Test          | 630          | 4,693         | 11.49           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  3 | [0.5pt/2pt] Cleaned | Train         | 8,362        | 33,525        | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  4 | [0.5pt/2pt] Cleaned | Dev           | 1,132        | 4,299         | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  5 | [0.5pt/2pt] Cleaned | Test          | 1,358        | 4,693         | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "The first set of results in Table 3 shows that the hierarchical right/left branching baselines dominate the completely right/left branching ones.",
        "evidence": "We focus on the results on the inter-domain compare with all the baselines. The first set of results shows that the hierarchical right/left branching baselines dominate the completely right/left branching ones. The right branching baseline is slightly better than the left branching baseline, indicating by looking at the results on the intra- and inter-domain metrics, that the hierarchical right/left branching baselines dominate the completely right/left branching ones.",
        "table": "+----+--------------------------------+--------------------------------+--------------------------------+\n|    | Approach                       | RST-DTtest                     | Instr-DTtest                   |\n+====+================================+================================+================================+\n|  0 | Right Branching                | 54.64                          | 58.47                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  1 | Left Branching                 | 53.73                          | 48.15                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  2 | Hier. Right Branch.            | [BOLD] 70.82                   | [BOLD] 67.86                   |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  3 | Hier. Left Branch.             | 70.58                          | 63.49                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  4 | [BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  5 | HILDAHernault et al. ( 2010 )  | 83.00                          | —                              |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  6 | DPLPJi and Eisenstein ( 2014 ) | 82.08                          | —                              |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  7 | CODRAJoty et al. ( 2015 )      | 83.84                          | [BOLD] 82.88                   |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  8 | Two-StageWang et al. ( 2017 )  | [BOLD] 86.00                   | 77.28                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n|  9 | [BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 10 | Two-StageRST-DT                | ×                              | 73.65                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 11 | Two-StageInstr-DT              | 74.48                          | ×                              |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 12 | Two-StageOurs(avg)             | 76.42                          | [BOLD] 74.22                   |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 13 | Two-StageOurs(max)             | [BOLD] 77.24                   | 73.12                          |\n+----+--------------------------------+--------------------------------+--------------------------------+\n| 14 | Human Morey et al. ( 2017 )    | 88.30                          | —                              |\n+----+--------------------------------+--------------------------------+--------------------------------+",
        "label": "supports"
    },
    {
        "claim": "this result shows the effectiveness of the causality-centric training objective and evaluation metrics in the COPA task; RoBERTa-large (finetuned) achieves substantial improvements (90.6%) over the previous state-of-the-art methods.",
        "evidence": "Recent studies show that BERT and RoBERTa achieve substantial improvements in the COPA task.",
        "table": "+----+----------------------------------------+------------+\n|    | Model                                  | Accuracy   |\n+====+========================================+============+\n|  0 | BigramPMI Goodwin et al. ( 2012 )      | 63.4       |\n+----+----------------------------------------+------------+\n|  1 | PMI Gordon et al. ( 2011 )             | 65.4       |\n+----+----------------------------------------+------------+\n|  2 | PMI+Connectives Luo et al. ( 2016 )    | 70.2       |\n+----+----------------------------------------+------------+\n|  3 | PMI+Con.+Phrase Sasaki et al. ( 2017 ) | 71.4       |\n+----+----------------------------------------+------------+\n|  4 | BERT-large Wang et al. ( 2019 )        | 70.5       |\n+----+----------------------------------------+------------+\n|  5 | BERT-large Sap et al. ( 2019 )         | 75.0       |\n+----+----------------------------------------+------------+\n|  6 | BERT-large Li et al. ( 2019 )          | 75.4       |\n+----+----------------------------------------+------------+\n|  7 | RoBERTa-large (finetuned)              | 90.6       |\n+----+----------------------------------------+------------+\n|  8 | BERT-large (finetuned)*                | 76.5 ± 2.7 |\n+----+----------------------------------------+------------+\n|  9 | RoBERTa-large (finetuned)*             | 87.7 ± 0.9 |\n+----+----------------------------------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "The model performs significantly better when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function.",
        "evidence": "The claim made by the authors is that their model trained with hinge loss instead of cross-entropy loss outperformed a model trained with both losses, indicating the importance of the loss function. We trained the base model, the hierarchical model, and the LSTM models with 4 layers of SRU and 2 layers of LSTM respectively. These results indicate that a model trained with hinge loss instead of cross-entropy loss achieves better results than a model trained with both losses, indicating the importance of the loss function.",
        "table": "+----+----------------------------+---------------------+------------------------------+------------------------+\n|    | [BOLD] Model               | [BOLD] Parameters   | [BOLD] Validation AUC@0.05   | [BOLD] Test AUC@0.05   |\n+====+============================+=====================+==============================+========================+\n|  0 | Base                       | 8.0M                | [BOLD] 0.871                 | 0.816                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  1 | 4L SRU → 2L LSTM           | 7.3M                | 0.864                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  2 | 4L SRU → 2L SRU            | 7.8M                | 0.856                        | [BOLD] 0.829           |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  3 | Flat → hierarchical        | 12.4M               | 0.825                        | 0.559                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  4 | Cross entropy → hinge loss | 8.0M                | 0.765                        | 0.693                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  5 | 6.6M → 1M examples         | 8.0M                | 0.835                        | 0.694                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  6 | 6.6M → 100K examples       | 8.0M                | 0.565                        | 0.417                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  7 | 200 → 100 negatives        | 8.0M                | 0.864                        | 0.647                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+\n|  8 | 200 → 10 negatives         | 8.0M                | 0.720                        | 0.412                  |\n+----+----------------------------+---------------------+------------------------------+------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Finally, not all emoji are beneficial for this task.",
        "evidence": "It shows for instance that some emoji, like “mask”, “two_hearts” and “heart”, which are named in the SQuAD dataset, are better for this task than others, but some, like cry “crying” and “worrying”, are not. In other words, there is a mix between positive and negative groups. It is clear that some emoji are helpful for this task, but some are not. It should be noted, that the number of emojis in the dataset is only through the self-attention mechanism, which is not the same as the classification task. Still, there are many cases where using all emoji is better than using only one emoji, for example, “unamused” is only through the self-attention mechanism.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The results in Table 7 show that the method is comparable to state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction.",
        "evidence": "We compared our method with the state of the art BiLSTM model from France et al. The method makes use of gold negation cues from the WordNet dataset for scope prediction. This is a very strong baseline, as previous work by Pourdamghani et al. However, the results show that our method performs comparable to the state of the art, for both in-scope and out-ofscope tokens.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "DAMD shows the effectiveness of capturing large-scale action patterns",
        "evidence": "While the results are not as good as those of DCS, we still see the improvement of applying our domain-adaptive delexcalization and domain-aware belief span modeling approach over a strong Seq2Seq baseline as well as a strong hierarchical-sequential neural network model, HDSA. The first three rows of the table show the results of our domain adaptation approach and the next three rows show the results of applying our data augmentation. HDSA achieves the best performance in terms of inform and success rates, and our domain-aware belief span modeling improves upon the strong Seq2Seq baseline in terms of both inform and success rate. Moreover, even without performing multi-action data augmentation, our model still achieves the best performance.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "As can be seen in the results presented in Table 3, the models using softmax and sparsemax in the output attention layer outperform the models using TVMAX.",
        "evidence": "As can be seen, the models using TVMAX in the output attention layer outperform the models using softmax and sparsemax in all the categories. Moreover, the results are slightly superior in the case of bounding box categories. This is perhaps due to the fact that the bounding box dimensions are in the lower-level of the image encoder, which gives the model more information about the object proposals.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3.",
        "evidence": "We compared our model against others with the same amount of Gigaword sentences (200k) and achieved a BLEU score of 32.23, which is highly competitive with state-of-the-art results.",
        "table": "+----+-----------------------+-------------------------+---------------------+\n|    | <bold>Model</bold>    | <bold>External</bold>   | <bold>BLEU</bold>   |\n+====+=======================+=========================+=====================+\n|  0 | Konstas et al. (2017) | 200K                    | 27.40               |\n+----+-----------------------+-------------------------+---------------------+\n|  1 | Song et al. (2018)    | 200K                    | 28.20               |\n+----+-----------------------+-------------------------+---------------------+\n|  2 | Guo et al. (2019)     | 200K                    | 31.60               |\n+----+-----------------------+-------------------------+---------------------+\n|  3 | G2S-GGNN              | 200K                    | <bold>32.23</bold>  |\n+----+-----------------------+-------------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are clas [CONTINUE] sified as racist across groups, although the rates remain low.",
        "evidence": "The results for Waseem (2016) are shown in Table 8. For Waseem (2016) we see that there is no statistically significant difference in the estimated rates at which tweets are clas [continue] sified as racist across groups. However, the rates remain low, and are similar in all cases. Similarly, we see low rates of racism and sexism for all other classes.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.001 |                0.003 |      -20.818 | ***          |                                  0.505 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.083 |                0.048 |      101.636 | ***          |                                  1.724 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.001 |                0.001 |        0.035 | [EMPTY]      |                                  1.001 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.023 |                0.012 |       64.418 | ***          |                                  1.993 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.002 |                0.001 |        4.047 | ***          |                                  1.12  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.049 |                0.019 |      120.986 | ***          |                                  2.573 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.173 |                0.065 |      243.285 | ***          |                                  2.653 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.032 |                0.023 |       39.483 | ***          |                                  1.396 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.111 |                0.061 |      122.707 | ***          |                                  1.812 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.178 |                0.08  |      211.319 | ***          |                                  2.239 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.028 |                0.015 |       63.131 | ***          |                                  1.854 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Increasing the window size to 10 reduces the F1 score marginally (A3−A4).",
        "evidence": "Yes, increasing the window size to 10 reduces the F1 score marginally (A3−A4). Results(a3−A4) show that the models with a window size of 5 outperform BiLSTM-CNN and standard attention system(A1−A2) without a window size of 10.",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] BI+IS decoding with single-domain trained models achieves gains over both the naive uniform approach and over oracle single-domain models.",
        "evidence": "The claim is that decoding with single-domain trained models achieves gains over both the naive uniform approach and over oracle single-domain models.. It is clear that the oracle models (EWC) do not provide significant improvements over the uniform approach. This can be explained by the fact that single-domain models are not trained on multiple domains.",
        "table": "+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|    | [BOLD] Language pair   | [BOLD] Model type   |   [BOLD] Oracle model |   [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS   |\n+====+========================+=====================+=======================+================================================+================================================+\n|  0 | es-en                  | Unadapted           |                  36.4 |                                           34.7 | 36.6                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  1 | es-en                  | No-reg              |                  36.6 |                                           34.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  2 | es-en                  | EWC                 |                  37   |                                           36.3 | [BOLD] 37.2                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  3 | en-de                  | Unadapted           |                  36.4 |                                           26.8 | 38.8                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  4 | en-de                  | No-reg              |                  41.7 |                                           31.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  5 | en-de                  | EWC                 |                  42.1 |                                           38.6 | [BOLD] 42.0                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "These results indicate that dense connections do not play a significant role in our model.",
        "evidence": "These results show that the simple network connections do not play a significant role in our model. It may be explained by the fact that the CNN and RNN features extracted from the image feature extractor are computed by concatenating the image feature and the word feature extracted by the baseline model. Therefore, the feature extracted by the baseline model can sufficiently represent the image feature and/or the word feature.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "refutes"
    },
    {
        "claim": "Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are not able to learn the task to a high degree.",
        "evidence": "We also experiment with the extent to which B-COPA 50% exposes the model to more diverse training instances than the original COPA due to the high overlap between mirrored alternatives. We observe that, once superficial cues [continue] are removed, the model is not able to learn the task to a high degree.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU).",
        "evidence": "Here, we compare our model against the neural models: GRU and ATR, both of which use attention. However, the proposed OLRN consistently outperforms these two baselines. For example, we outperform ATR with 1.33 BLEU score on the train set, but only 1.15 BLEU score on the decode set. In this case, OLRN is on par with GRU at +0.45 BLEU.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Lemma-based targets with POS disambiguation perform best on WN-N when dependency-based contexts are used; the difference to lemmatized targets without disambiguation is statistically significant (p < .1).",
        "evidence": "The table shows that lemmatized targets with POS disambiguation perform best on WN-N compared to targets without disambiguation. The difference is statistically significant (p < .1) in all cases. [CONTINUE] Lemmatized targets significantly outperform type targets in terms of F-measure in all cases. In comparison, type targets without disambiguation achieve only comparable performance with lemmatized targets in terms of F-measure. [CONTINUE] As can be seen, the POS disambiguation-enhanced targets significantly outperform the type and lemmatized targets in terms of F-measure on WN-N when dependency-based contexts are used. The difference is statistically significant (p < .1) in all cases.",
        "table": "+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|    | [EMPTY]      | WN-N P            | WN-N R            | WN-N F            | WN-V P            | WN-V R            | WN-V F            | VN P              | VN R              | VN F              |\n+====+==============+===================+===================+===================+===================+===================+===================+===================+===================+===================+\n|  0 | Context: w2  | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  1 | type         | .700              | .654              | .676              | .535              | .474              | .503              | .327              | .309              | .318              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  2 | x+POS        | .699              | .651              | .674              | .544              | .472              | .505              | .339              | .312              | .325              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  3 | lemma        | .706              | .660              | .682              | .576              | .520              | .547              | .384              | .360              | .371              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  4 | x+POS        | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  5 | Context: dep | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  6 | type         | .712              | .661              | .686              | .545              | .457              | .497              | .324              | .296              | .310              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  7 | x+POS        | .715              | .659              | .686              | .560              | .464              | .508              | .349              | .320              | .334              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  8 | lemma        | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591              | .512              | .548              | .408              | .371              | .388              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  9 | x+POS        | .722              | .666              | .693              | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] We empirically found that self-attention was the most efficient in the 3rd stage.",
        "evidence": "We then repeated the similar experiments on the ResNet-34 dataset. We empirically found that self-attention was the most efficient in the 3rd stage.. Batch size: 1, 2, 3.",
        "table": "+----+------------------------------------------+---------------------+-----------------+\n|    | [BOLD] ResNet-34                         | [BOLD] Eval set %   | [BOLD] #param   |\n+====+==========================================+=====================+=================+\n|  0 | Baseline (No SA)Anderson et al. ( 2018 ) | 55.00               | 0M              |\n+----+------------------------------------------+---------------------+-----------------+\n|  1 | SA (S: 1,2,3 - B: 1)                     | 55.11               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  2 | SA (S: 1,2,3 - B: 2)                     | 55.17               | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+\n|  3 | [BOLD] SA (S: 1,2,3 - B: 3)              | [BOLD] 55.27        | } 0.107M        |\n+----+------------------------------------------+---------------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "The results in Table 5 show that the frequency whitelists perform better than the random and clustering whitelists when the true response is added.",
        "evidence": "The experiments show that the frequency and clustering whitelists perform similar when the true response is added. The frequency whitelists perform slightly better than the random whitelists when the true response is added. This can be explained by the fact that the frequency estimator does not have access to the true response in the frequency estimators. Similar results can be seen from the BLEU scores. The BLEU scores show that the clustering and frequency whitelists have similar performance when the true response is added.",
        "table": "+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|    | [BOLD] Whitelist   |   [BOLD] R@1 |   [BOLD] R@3 |   [BOLD] R@5 |   [BOLD] R@10 |   [BOLD] BLEU |\n+====+====================+==============+==============+==============+===============+===============+\n|  0 | Random 10K+        |        0.252 |        0.4   |        0.472 |         0.56  |         37.71 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  1 | Frequency 10K+     |        0.257 |        0.389 |        0.455 |         0.544 |         41.34 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  2 | Clustering 10K+    |        0.23  |        0.376 |        0.447 |         0.541 |         37.59 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  3 | Random 1K+         |        0.496 |        0.663 |        0.728 |         0.805 |         59.28 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  4 | Frequency 1K+      |        0.513 |        0.666 |        0.726 |         0.794 |         67.05 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  5 | Clustering 1K+     |        0.481 |        0.667 |        0.745 |         0.835 |         61.88 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  6 | Frequency 10K      |        0.136 |        0.261 |        0.327 |         0.42  |         30.46 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  7 | Clustering 10K     |        0.164 |        0.292 |        0.36  |         0.457 |         31.47 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  8 | Frequency 1K       |        0.273 |        0.465 |        0.55  |         0.658 |         47.13 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+\n|  9 | Clustering 1K      |        0.331 |        0.542 |        0.65  |         0.782 |         49.26 |\n+----+--------------------+--------------+--------------+--------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "These results use the best performing KnowComb system.",
        "evidence": "Here, we rely on the best performing KnowComb system from Zhao et al. The results clearly show that, for all four types of events, the results of Type 1 and Type 2 are significantly better than those of Type 1, with the case of Cat1 being slightly better than Cat2. Also, the results show that it is better to use the training set for all cases.",
        "table": "+----+---------------+-----------------+------------------+\n|    | Schema        |   AntePre(Test) |   AntePre(Train) |\n+====+===============+=================+==================+\n|  0 | Type 1        |           76.67 |            86.79 |\n+----+---------------+-----------------+------------------+\n|  1 | Type 2        |           79.55 |            88.86 |\n+----+---------------+-----------------+------------------+\n|  2 | Type 1 (Cat1) |           90.26 |            93.64 |\n+----+---------------+-----------------+------------------+\n|  3 | Type 2 (Cat2) |           83.38 |            92.49 |\n+----+---------------+-----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "BERT achieved a final accuracy of 91.20%, now marginally comparable to ULMFiT's full performance.",
        "evidence": "BERT achieved a final accuracy of 91.20%, now marginally comparable to ULMFiT's full performance. ULMFiT trained for over 11 hours, with an average of 2.5 epochs per epoch. GPT-2, on the other hand, trained for only 5 epochs, achieves a final accuracy of 96.28%, now marginally comparable to ULMFiT's full performance.",
        "table": "+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|    | Model                     | Val. Accuracy   |   Loss |   Val. Loss | Pretraining Time   | Finetuning Time   |\n+====+===========================+=================+========+=============+====================+===================+\n|  0 | Siamese Networks          | 77.42%          | 0.5601 |      0.5329 | [EMPTY]            | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  1 | BERT                      | 87.47%          | 0.4655 |      0.4419 | 66 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  2 | GPT-2                     | 90.99%          | 0.2172 |      0.1826 | 78 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  3 | ULMFiT                    | 91.59%          | 0.375  |      0.1972 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  4 | ULMFiT (no LM Finetuning) | 78.11%          | 0.5512 |      0.5409 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  5 | BERT + Multitasking       | 91.20%          | 0.3155 |      0.3023 | 66 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  6 | GPT-2 + Multitasking      | 96.28%          | 0.2609 |      0.2197 | 78 hours           | 5m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "Our model does not outperform the previous state-of-the-art models on both datasets in terms of F1 score.",
        "evidence": "We can see that our model is not the best on both datasets. Particularly, on the NYT10 dataset, the nearest neighboring model (PCNN) gets the better F1 score than our model. On the NYT11 dataset, the same thing as before has happened, but our model still gets the best F1 score.",
        "table": "+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|    | Model                 | NYT10 Prec.   | NYT10 Rec.   | NYT10 F1      | NYT11 Prec.   | NYT11 Rec.   | NYT11 F1      |\n+====+=======================+===============+==============+===============+===============+==============+===============+\n|  0 | CNN zeng2014relation  | 0.413         | 0.591        | 0.486         | 0.444         | 0.625        | 0.519         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  1 | PCNN zeng2015distant  | 0.380         | [BOLD] 0.642 | 0.477         | 0.446         | 0.679        | 0.538†        |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  2 | EA huang2016attention | 0.443         | 0.638        | 0.523†        | 0.419         | 0.677        | 0.517         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  3 | BGWA jat2018attention | 0.364         | 0.632        | 0.462         | 0.417         | [BOLD] 0.692 | 0.521         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  4 | BiLSTM-CNN            | 0.490         | 0.507        | 0.498         | 0.473         | 0.606        | 0.531         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  5 | Our model             | [BOLD] 0.541  | 0.595        | [BOLD] 0.566* | [BOLD] 0.507  | 0.652        | [BOLD] 0.571* |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "This indicates that PMeans can better detect informative sentences, and PMeans-RNN can better find informative words in extracted sentence.",
        "evidence": "Table. We also compare our model against 2 baselines: (1) CNN encoder, which uses CNN features; (2) MLP encoder, which uses BERT features; (3) PSM encoder, which uses PSM word embeddings. It can be seen that PSM can better detect informative sentences than the other baselines, which confirms that extracting sentences from compressed text can be more efficient than extracting sentences from all text. The scores of all methods are comparable.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our results indicate that neither beam search nor diversity-enhancing decoding schemes can generate multiple actions well.",
        "evidence": "The results show that diversity-enhancing decoding scheme can generate multiple actions well. This is true for both single-action and 10-action generation.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "GPT-2, on the other hand, finetuned to a final accuracy of 96.28%, a full 4.69% improvement over the performance of ULMFiT.",
        "evidence": "BERT achieved an accuracy of 91.20%, which is now comparable to ULMFiT's full accuracy. GPT-2, on the other hand, finetuned to a final accuracy of 96.28%, a full 4.69% improvement over the performance of ULMFiT. We also calculate the time in which GPT-2 trained for 78 epochs, versus ULMFiT’s training time of 11 epochs. The results also show that multitasking helps to improve the performance of GPT-2, with a gain of 4.69% accuracy.",
        "table": "+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|    | Model                     | Val. Accuracy   |   Loss |   Val. Loss | Pretraining Time   | Finetuning Time   |\n+====+===========================+=================+========+=============+====================+===================+\n|  0 | Siamese Networks          | 77.42%          | 0.5601 |      0.5329 | [EMPTY]            | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  1 | BERT                      | 87.47%          | 0.4655 |      0.4419 | 66 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  2 | GPT-2                     | 90.99%          | 0.2172 |      0.1826 | 78 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  3 | ULMFiT                    | 91.59%          | 0.375  |      0.1972 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  4 | ULMFiT (no LM Finetuning) | 78.11%          | 0.5512 |      0.5409 | 11 hours           | 2m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  5 | BERT + Multitasking       | 91.20%          | 0.3155 |      0.3023 | 66 hours           | 4m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+\n|  6 | GPT-2 + Multitasking      | 96.28%          | 0.2609 |      0.2197 | 78 hours           | 5m per epoch      |\n+----+---------------------------+-----------------+--------+-------------+--------------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "We find that when we train STagBL with only its main task—with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance increases typically by a few percentage points.",
        "evidence": "In this section, detailed analysis of the effect of STAGBL’s main task, the ‘natural subtask’, is presented. We can see that, as expected, the model performs well when presented with only the main task. However, with respect to the auxiliary subtasks, the results are mixed. Moreover, provided with only the main task (Y-3), the performance is worse than when we include the auxiliary subtasks (Y-3:Y<italic>C</italic>-). However, if we add the task Y-3:Y<italic>R</italic>-, the performance improves by several percentage points. This means that the task Y-3:Y<italic>R is crucial to the main task. It also shows that the model is sensitive to the change of the auxiliary task. In general, provided with the auxiliary task Y-3:Y<italic>C</italic>-, the performance of the main task also improves. However, the improvement is usually smaller than that of the auxiliary task Y-3:Y<italic>C.",
        "table": "+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]                                         | C-F1 100%          | C-F1 50%           | R-F1 100%          | R-F1 50%           | F1 100%            | F1 50%             |\n+====+=================================================+====================+====================+====================+====================+====================+====================+\n|  0 | Y-3                                             | 49.59              | 65.37              | 26.28              | 37.00              | 34.35              | 47.25              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  1 | Y-3:Y<italic>C</italic>-1                       | 54.71              | 66.84              | 28.44              | 37.35              | 37.40              | 47.92              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  2 | Y-3:Y<italic>R</italic>-1                       | 51.32              | 66.49              | 26.92              | 37.18              | 35.31              | 47.69              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  3 | Y-3:Y<italic>C</italic>-3                       | <bold>54.58</bold> | 67.66              | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold> |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  4 | Y-3:Y<italic>R</italic>-3                       | 53.31              | 66.71              | 26.65              | 35.86              | 35.53              | 46.64              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  5 | Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95              | <bold>67.84</bold> | 27.90              | 39.71              | 36.54              | 50.09              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  6 | Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55              | 67.60              | 28.30              | 38.26              | 37.26              | 48.86              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "the blue marker represents the ratio of the \"Full\" score, and the orange marker denotes the ratio of “Other”.",
        "evidence": "We can see that the ratio of “other” is higher than that of “full”, which is consistent with the argument identification task.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "When we increase the DCGCN blocks from 1 to 4, the model performance does not necessarily increase on AMR15 development set.",
        "evidence": "We perform an ablation study on the AMR15 development set to validate our ideas. In particular, we vary the number of DCGCN layers used. We can see that, increasing the number of DCGCN blocks from 1 to 4, decreases the model performance. This is consistent with our hypothesis that adding more GCN layers does not necessarily increase the model performance. Since the baseline GCN model (DCGCN1) performs slightly better than GCNs1+DCGCN3 (28.0 vs. 24.8), we may believe that increasing the number of DCGCN blocks is sufficient. However, further increasing the number of GCN blocks beyond 4, with or without increased DCGCN, does not lead to better performance.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Table 5 shows improvements on data without domain labelling using our adaptive decoding schemes with unadapted models trained only on one domain [CONTINUE] Uniform ensembling under-performs all oracle models except es-en Bio, especially on general domains.",
        "evidence": "Table 5 shows improvements on data without domain labelling. [CONTINUE] Uniform ensembling under-performs all oracle models except es-en bio, especially on general domains. [CONTINUE] Identity-BI and BI have similar improvements, and BI with λ as in the previous section is on par with the oracle model.",
        "table": "+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|    | [BOLD] Decoder configuration   | [BOLD] es-en  [BOLD] Health   | [BOLD] es-en  [BOLD] Bio   | [BOLD] en-de  [BOLD] News   | [BOLD] en-de  [BOLD] TED   | [BOLD] en-de  [BOLD] IT   |\n+====+================================+===============================+============================+=============================+============================+===========================+\n|  0 | Oracle model                   | 35.9                          | 36.1                       | 37.8                        | 24.1                       | 39.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  1 | Uniform                        | 33.1                          | 36.4                       | 21.9                        | 18.4                       | 38.9                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  2 | Identity-BI                    | 35.0                          | 36.6                       | 32.7                        | 25.3                       | 42.6                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  3 | BI                             | 35.9                          | 36.5                       | 38.0                        | 26.1                       | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  4 | IS                             | [BOLD] 36.0                   | 36.8                       | 37.5                        | 25.6                       | 43.3                      |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+\n|  5 | BI + IS                        | [BOLD] 36.0                   | [BOLD] 36.9                | [BOLD] 38.4                 | [BOLD] 26.4                | [BOLD] 44.7               |\n+----+--------------------------------+-------------------------------+----------------------------+-----------------------------+----------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "It can be observed that the learned reward function does not have good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise.",
        "evidence": "We can see that the learned reward function does not perform well in that the dialog gets a full score on each metric, and negative otherwise. We can see that the inform and match scores are very low, and the success probabilities are very high. This indicates that the learned reward function does not have good interpretability.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned [CONTINUE] tweets predicted to belong to this class.",
        "evidence": "The results show that the Waseem and Hovy (2016) classifier is sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned [continue] tweets predicted to belong to this class.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.01  |                0.01  |       -0.632 | [EMPTY]      |                                  0.978 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.963 |                0.944 |       20.064 | ***          |                                  1.02  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.011 |                0.011 |       -1.254 | [EMPTY]      |                                  0.955 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.349 |                0.29  |       28.803 | ***          |                                  1.203 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.012 |                0.012 |       -0.162 | [EMPTY]      |                                  0.995 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.017 |                0.015 |        4.698 | ***          |                                  1.152 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.988 |                0.991 |       -6.289 | ***          |                                  0.997 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.099 |                0.091 |        6.273 | ***          |                                  1.091 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.074 |                0.027 |       46.054 | ***          |                                  2.728 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.925 |                0.968 |      -41.396 | ***          |                                  0.956 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.01  |                0.01  |        0     | [EMPTY]      |                                  1     |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The 'alternating' LSTM layout we chose for our submission actually outperformed the 'traditional' one in terms of both single model and ensemble performance.",
        "evidence": "For our submission, we actually preferred the 'alternating' LSTM layout over the 'traditional' one.. This LSTM layout we chose for our submission actually outperformed the 'traditional' one in terms of both single model and ensemble performance.. Remarkably, the 'traditional' LSTM that has the same number of layers as our submission (LSTM-800) performs better than our 'alternating' LSTM (LSTM-400) which has four layers of LSTM layers. In the same way, the 'traditional' LSTM that has more layers of LSTM layers (LSTM-800) performs better than our 'alternating' LSTM (LSTM-400 Dropout) which has fewer layers of LSTM layers.",
        "table": "+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|    | ID LSTM-800        |   5-fold CV 70.56 | Δ 0.66   | Single model 67.54   | Δ 0.78   | Ensemble 67.65   | Δ 0.30   |\n+====+====================+===================+==========+======================+==========+==================+==========+\n|  0 | LSTM-400           |             70.5  | 0.60     | [BOLD] 67.59         | 0.83     | [BOLD] 68.00     | 0.65     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  1 | IN-TITLE           |             70.11 | 0.21     | [EMPTY]              | [EMPTY]  | 67.52            | 0.17     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  2 | [BOLD] SUBMISSION  |             69.9  | –        | 66.76                | –        | 67.35            | –        |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  3 | NO-HIGHWAY         |             69.72 | −0.18    | 66.42                | −0.34    | 66.64            | −0.71    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  4 | NO-OVERLAPS        |             69.46 | −0.44    | 65.07                | −1.69    | 66.47            | −0.88    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  5 | LSTM-400-DROPOUT   |             69.45 | −0.45    | 65.53                | −1.23    | 67.28            | −0.07    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  6 | NO-TRANSLATIONS    |             69.42 | −0.48    | 65.92                | −0.84    | 67.23            | −0.12    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  7 | NO-ELMO-FINETUNING |             67.71 | −2.19    | 65.16                | −1.60    | 65.42            | −1.93    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "Firstly, we use a simple rule-based classifier where each word after punctuation marks that are not parenthesis, brackets, or quotes, is predicted to be in the scope of a negation.",
        "evidence": "The results shown in the table show that the method is more effective for negation prediction than the other two prediction methods. From the table we can see that the accuracy of the rule-based system is better than the prediction by the PCS method when the words after punctuation marks are predicted to be in the scope of a negation. The results show that the method is very effective at predicting negations when the words after punctuation marks are not in the scope of a negation.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Lin-SVM outperforms other classifiers in extracting most relations.",
        "evidence": "Lin-SVM according to the table outperforms other classifiers in extracting most relations. ann after combining sentence embeddings(SSE) with BOC(Wiki-PubMed-PMC)+SE.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Though ALDM obtains a higher inform F1 and match rate than PPO, it does not get a significant improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3.",
        "evidence": "Though ALDM obtains a higher inform F1 and match rate than PPO, it does not get a significant improvement on task success. We believe that this is because the task success is conditioned to the alignment between the input and output sequences. This also confirms our hypothesis thatalignment can help to align the input and output sequences better.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "LRN does not accelerate the training over LSTM and SRU by about 20%.",
        "evidence": "The main claim of our model is that it achieves comparable performance with the state-of-the-art GPU acceleration method by Zhang et al. In our experiments, we use a GPU model with 4 Intel Xeon E5-2630 processor (8 threads, 16 cores each) and 32 GB of RAM.",
        "table": "+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|    | Model                 | #Params   | AmaPolar ERR   | AmaPolar Time   | Yahoo ERR    | Yahoo Time   | AmaFull ERR   | AmaFull Time   | YelpPolar ERR   | YelpPolar Time   |\n+====+=======================+===========+================+=================+==============+==============+===============+================+=================+==================+\n|  0 | Zhang et al. ( 2015 ) | -         | 6.10           | -               | 29.16        | -            | 40.57         | -              | 5.26            | -                |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  1 | LSTM                  | 227K      | [BOLD] 4.37    | 0.947           | [BOLD] 24.62 | 1.332        | 37.22         | 1.003          | 3.58            | 1.362            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  2 | GRU                   | 176K      | 4.39           | 0.948           | 24.68        | 1.242        | [BOLD] 37.20  | 0.982          | [BOLD] 3.47     | 1.230            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  3 | ATR                   | 74K       | 4.78           | 0.867           | 25.33        | 1.117        | 38.54         | 0.836          | 4.00            | 1.124            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  4 | SRU                   | 194K      | 4.95           | 0.919           | 24.78        | 1.394        | 38.23         | 0.907          | 3.99            | 1.310            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  5 | LRN                   | 151K      | 4.98           | [BOLD] 0.731    | 25.07        | [BOLD] 1.038 | 38.42         | [BOLD] 0.788   | 3.98            | [BOLD] 1.022     |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Across unigrams, part-of-speech patterns and word clusters, we see a distinctive pattern emerging around pronoun usage.",
        "evidence": "First, we can see that pronouns used in complaints are not unique to pronoun usage. Across unigrams, part-of-speech patterns and word clusters, we can see a distinctive pattern emerging around pronoun usage.. Complaints use more possessive pronouns, indicating that the model is able to identify the part-of-speech pattern that complaints users tend to use. Another pattern seeing in complaints was that of “good karma” followed by “lol”, which might suggest that the model is able to identify that the user is being served with more than mere pronouns. Lastly, various part-of-speech patterns (word clusters) were also observed, with “not” being used most often followed by “he”, “retrieve”, “won’t”, “regime change” etc.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The Waseem and Hovy (2016) classifier is not particularly sensitive to the word \"b*tch\" with only 1% of black-aligned and 1% of white-aligned tweets predicted to belong to this class.",
        "evidence": "The results show that the Waseem and Hovy (2016) classifier is not particularly sensitive to the word \"b*tch\" with only 1% of black-aligned and 1% of white-aligned tweets predicted to belong to this class.",
        "table": "+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|    | Dataset                  | Class             |   ˆ [ITALIC] piblack |   ˆ [ITALIC] piwhite |   [ITALIC] t | [ITALIC] p   |   ˆ [ITALIC] piblackˆ [ITALIC] piwhite |\n+====+==========================+===================+======================+======================+==============+==============+========================================+\n|  0 | [ITALIC] Waseem and Hovy | Racism            |                0.01  |                0.01  |       -0.632 | [EMPTY]      |                                  0.978 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  1 | [EMPTY]                  | Sexism            |                0.963 |                0.944 |       20.064 | ***          |                                  1.02  |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  2 | [ITALIC] Waseem          | Racism            |                0.011 |                0.011 |       -1.254 | [EMPTY]      |                                  0.955 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  3 | [EMPTY]                  | Sexism            |                0.349 |                0.29  |       28.803 | ***          |                                  1.203 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  4 | [EMPTY]                  | Racism and sexism |                0.012 |                0.012 |       -0.162 | [EMPTY]      |                                  0.995 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  5 | [ITALIC] Davidson et al. | Hate              |                0.017 |                0.015 |        4.698 | ***          |                                  1.152 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  6 | [EMPTY]                  | Offensive         |                0.988 |                0.991 |       -6.289 | ***          |                                  0.997 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  7 | [ITALIC] Golbeck et al.  | Harassment        |                0.099 |                0.091 |        6.273 | ***          |                                  1.091 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  8 | [ITALIC] Founta et al.   | Hate              |                0.074 |                0.027 |       46.054 | ***          |                                  2.728 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n|  9 | [EMPTY]                  | Abusive           |                0.925 |                0.968 |      -41.396 | ***          |                                  0.956 |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+\n| 10 | [EMPTY]                  | Spam              |                0.01  |                0.01  |        0     | [EMPTY]      |                                  1     |\n+----+--------------------------+-------------------+----------------------+----------------------+--------------+--------------+----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The total number of words in the concatenated inputs is longer than other MDS datasets, as those consist of 10 input documents, but shorter than SDS datasets, as expected.",
        "evidence": "As expected, the concatenated inputs are shorter than other MDS datasets, as they consist of 10 input documents. The total number of words in the concatenated inputs is longer than other MDS datasets, as those consist of 10 input documents, but shorter than SDS datasets, as expected. Our CNN is the largest in terms of word count and number of pairs, followed by DUC data and TAC 2011 data. In terms of number of sentences, it is also the largest one, followed by Multi-News, but larger than SDS datasets.",
        "table": "+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|    | [BOLD] Dataset   | [BOLD] # pairs        | [BOLD] # words (doc)   |   [BOLD] # sents (docs) |   [BOLD] # words (summary) |   [BOLD] # sents (summary) | [BOLD] vocab size   |\n+====+==================+=======================+========================+=========================+============================+============================+=====================+\n|  0 | Multi-News       | 44,972/5,622/5,622    | 2,103.49               |                   82.73 |                     263.66 |                       9.97 | 666,515             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  1 | DUC03+04         | 320                   | 4,636.24               |                  173.15 |                     109.58 |                       2.88 | 19,734              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  2 | TAC 2011         | 176                   | 4,695.70               |                  188.43 |                      99.7  |                       1    | 24,672              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  3 | CNNDM            | 287,227/13,368/11,490 | 810.57                 |                   39.78 |                      56.2  |                       3.68 | 717,951             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M).",
        "evidence": "The BiLSTM model refers to a linear-chain language model containing 650 hidden nodes. The BiLSTM model in this paper is similar to the BiLSTM model in \\newcitekonstas2017neural. However, the suffix “geb” refers to the neural embedding model, which is different from the model described in the previous section. Moreover, the GCN and the get models need more parameters than the BiLSTM model. This can be explained by the fact that when the query is short, the BLEU score is worse. When the query is long, the BLEU score is better. Therefore, the more parameters the model needs, the better the performance. The similar conclusion can be drawn for the Meteor dataset. We believe that this is mainly because the Meteor dataset is so small that the word embeddings are good enough.",
        "table": "+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|    | <bold>Model</bold>                                   |   <bold>BLEU</bold> |   <bold>METEOR</bold> | <bold>Size</bold>   |\n+====+======================================================+=====================+=======================+=====================+\n|  0 | biLSTM                                               |               22.5  |                 30.42 | 57.6M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|  1 | <italic>GEt</italic> + biLSTM                        |               26.33 |                 32.62 | 59.6M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|  2 | <italic>GEb</italic> + biLSTM                        |               26.12 |                 32.49 | 59.6M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+\n|  3 | <italic>GEt</italic> + <italic>GEb</italic> + biLSTM |               27.37 |                 33.3  | 61.7M               |\n+----+------------------------------------------------------+---------------------+-----------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation.",
        "evidence": "Our results showed that the Stanford basic (SB) representation provide the best results, closely followed by the CoNLL08 representation. The feature maps of the CoNLL08 representation are fixed, therefore we cannot evaluate the performance of different feature sizes in the same way as we do for the Stanford basic one. With the exception of the Sigmoid dropout, in all our experiments we use the default values for the dropout probability, since it leads to the best F1-score.",
        "table": "+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|    | [BOLD] Representation   | [BOLD] Hyper parameters Filter size   |   [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func.   |   [BOLD] Hyper parameters L2 Reg. |   [BOLD] Hyper parameters Learning rate |   [BOLD] Hyper parameters Dropout Prob. |   [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values   |\n+====+=========================+=======================================+=============================================+============================================+===================================+=========================================+=========================================+==================================================+==================================================+\n|  0 | CoNLL08                 | 4-5                                   |                                        1000 | Softplus                                   |                           11.5    |                                0.00113  |                                    1    |                                            73.34 | 74.49                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  1 | SB                      | 4-5                                   |                                         806 | Sigmoid                                    |                            0.0813 |                                0.00179  |                                    0.87 |                                            72.83 | [BOLD] 75.05                                     |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  2 | UD v1.3                 | 5                                     |                                         716 | Softplus                                   |                            1.66   |                                0.000963 |                                    1    |                                            68.93 | 69.57                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "On the NYT11 dataset, m = 4 gives the best performance.",
        "evidence": "The claim-premise can be evaluated using the accuracy of the relation classifier. From the results, we can see that for the NYT10 dataset, m=4 gives the best performance. For the NYT11 dataset, m=4 also gives the best performance. These results indicate that our method can well capture the relationship between the claim and test set.",
        "table": "+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|    |   [ITALIC] m |   NYT10 Prec. |   NYT10 Rec. | NYT10 F1     |   NYT11 Prec. |   NYT11 Rec. | NYT11 F1     |\n+====+==============+===============+==============+==============+===============+==============+==============+\n|  0 |            1 |         0.541 |        0.595 | [BOLD] 0.566 |         0.495 |        0.621 | 0.551        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  1 |            2 |         0.521 |        0.597 | 0.556        |         0.482 |        0.656 | 0.555        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  2 |            3 |         0.49  |        0.617 | 0.547        |         0.509 |        0.633 | 0.564        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  3 |            4 |         0.449 |        0.623 | 0.522        |         0.507 |        0.652 | [BOLD] 0.571 |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+\n|  4 |            5 |         0.467 |        0.609 | 0.529        |         0.488 |        0.677 | 0.567        |\n+----+--------------+---------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Since only 20% of the tweets are used as negative training samples, we cannot use all negative tweets for development phase.",
        "evidence": "This claim is significant for both positive and negative sentiment (f1-score of 0.87 vs. 0.72 for positive sentiment and F1-score of 0.83 vs. 0.72 for negative sentiment). It is also very apparent that the proposed CNN-LSTMOur-neg-Ant improves upon all baseline models, pushing F1-score to 0.78 from 0.72.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks.",
        "evidence": "To verify our claim that the more GCN layers we construct, the better the performance of our model : (i) n=6,m=3 gives the best performance with n=6; (ii) m=6 and 2 give the best performance; (iii) stacking more GCN blocks does not affect the results too much; (iv) on the BLEU score, increasing m from 3 to 6 improves the result only slightly; and (v) the best BLEU score of 2.5/3.3 is obtained by stacking 2 GCN blocks with 6 GCN layers.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    |   [ITALIC] Block |   [ITALIC] n |   [ITALIC] m | B           | C           |\n+====+==================+==============+==============+=============+=============+\n|  0 |                1 |            1 |            1 | 17.6        | 48.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 |                1 |            1 |            2 | 19.2        | 50.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 |                1 |            2 |            1 | 18.4        | 49.1        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 |                1 |            1 |            3 | 19.6        | 49.4        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 |                1 |            3 |            1 | 20.0        | 50.5        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 |                1 |            3 |            3 | 21.4        | 51.0        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 |                1 |            3 |            6 | 21.8        | 51.7        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 |                1 |            6 |            3 | 21.7        | 51.5        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 |                1 |            6 |            6 | 22.0        | 52.1        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 |                2 |            3 |            6 | [BOLD] 23.5 | 53.3        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 |                2 |            6 |            3 | 23.3        | [BOLD] 53.4 |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 |                2 |            6 |            6 | 22.0        | 52.1        |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "In some cases it seems to make no difference in results, e.g., Europarl in Portuguese which did not increase the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in no increase of f-measure from F=0.5555 in DF to F=0.6403 in TF.",
        "evidence": "This claim does not seem to be very sensitive to the choice of corpus, F-measure and tf-measure. In some cases it seems to make no difference in results, e.g., the Portuguese classification for TED talks. In other words, the F-measures and tf-measures of the Portuguese corpus are very low, only around 0.3% and 0.5% higher than the corresponding results of the other three datasets.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The most interesting ones are mask, rage, and cry, which significantly decrease accuracy.",
        "evidence": "The most interesting ones are mask, rage, and cry, which significantly decrease accuracy. [CONTINUE] They appear in the emoji categories of “Heart” and “Two_hearts”, which represent the Heart emoji. [CONTINUE] Further, contrary to intuition, “Heart” and “Two_hearts” are more closely related to each other than “Mace” and “Rage”.",
        "table": "+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|    | [BOLD] Emoji alias   |   [BOLD] N |   [BOLD] emoji # |   [BOLD] emoji % |   [BOLD] no-emoji # |   [BOLD] no-emoji % | [BOLD] Δ%   |\n+====+======================+============+==================+==================+=====================+=====================+=============+\n|  0 | mask                 |        163 |              154 |            94.48 |                 134 |               82.21 | - 12.27     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  1 | two_hearts           |         87 |               81 |            93.1  |                  77 |               88.51 | - 4.59      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  2 | heart_eyes           |        122 |              109 |            89.34 |                 103 |               84.43 | - 4.91      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  3 | heart                |        267 |              237 |            88.76 |                 235 |               88.01 | - 0.75      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  4 | rage                 |         92 |               78 |            84.78 |                  66 |               71.74 | - 13.04     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  5 | cry                  |        116 |               97 |            83.62 |                  83 |               71.55 | - 12.07     |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  6 | sob                  |        490 |              363 |            74.08 |                 345 |               70.41 | - 3.67      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  7 | unamused             |        167 |              121 |            72.46 |                 116 |               69.46 | - 3.00      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  8 | weary                |        204 |              140 |            68.63 |                 139 |               68.14 | - 0.49      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n|  9 | joy                  |        978 |              649 |            66.36 |                 629 |               64.31 | - 2.05      |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 10 | sweat_smile          |        111 |               73 |            65.77 |                  75 |               67.57 | 1.80        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+\n| 11 | confused             |         77 |               46 |            59.74 |                  48 |               62.34 | 2.60        |\n+----+----------------------+------------+------------------+------------------+---------------------+---------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Finally, Table 5 shows the F1 score of the (in-scope, out-of-scope) negation scopes using Punctuation, our Proposed model and BiLSTM classifier.",
        "evidence": "The results obtained by our model show an F1 score of 0.85 for the in-scope negation scopes and 0.97 for the out-of-scope scopes. The PCS tagger achieved an F1 score of 0.72 for the in-scope scopes and 0.72 for the out-of-scope scopes.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the average length of “good” summaries is higher than that of other summaries.",
        "evidence": "The claim from the claim section is that the average length of “good” summaries is higher than that of other summaries. We compare different methods in terms of Rouge-1, ROUGE-2, BLEU-4, BLEU-5 and METEOR. From the table, we can see that the average length of summaries created by other methods is also higher than that of our summaries. For example, the average length of InferSent-Cosine and BERT-Cosine summaries is about .115 and .162, respectively. InferSent-Cosine has better results than BERT-Cosine in terms of the G-Pre and G-Rec measures. However, BERT-Cosine has also better results than all the other methods in terms of the correlation coefficient. This indicates that the Cosine metric may be not the perfect metric to evaluate the quality of summaries.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "We observe that the B3 metric is harsher than the other two and is most suitable when a very high precision of entity identification is desired.",
        "evidence": "Table 3 compares the performance of our approaches against the strong lemma baseline and the disjoint approach. Our joint approach is significantly better than the other approaches including the strong lemma baseline at all three metrics. [CONTINUE] Lemma clustering is most suitable when high precision of entity identification is desired, similar to the observations in [CONTINUE] the joint approach is better than all other approaches at entity detection.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are able to learn the task to a high degree.",
        "evidence": "We also experiment with the extent to which B-COPA 50% exposes the model to more diverse training instances than the original COPA due to the high overlap between mirrored alternatives. We observe that, by training on B-COPA 50%, the model is able to learn the task to a high degree. BLEU scores increase slightly when trained on B-COPA 50% instead of the original COPA due to the similarity of mirrored alternatives. The RoBERTa-Large-FT model also performs better when trained on B-COPA 50% compared to training on the original COPA due to the similarity between mirrored alternatives. These results show that once superficial cues [continue] are removed, the models are able to learn the task to a high degree..",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table 1).",
        "evidence": "Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings. Among all feature baselines, ANN outperforms BOC using the same set of embeddings. SVM and LR perform better than BoC embeddings with the exception of ASM embeddings.",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5).",
        "evidence": "While POS tagging is better than Sem tagging for all conditions, there are no consistent differences between the two tasks. We believe that this is because Sem tagging does not benefit from additional supervision from label smoothing. At the highest layer, where the supervision is strongest, POS tagging and Sem tagging both improve by around 0.9 BLEU. However, at lower layers, they improve slightly more.",
        "table": "+----+-------+-------+----------+----------+----------+----------+----------+\n|    | Uni   | POS   |   0 87.9 |   1 92.0 |   2 91.7 |   3 91.8 |   4 91.9 |\n+====+=======+=======+==========+==========+==========+==========+==========+\n|  0 | Uni   | SEM   |     81.8 |     87.8 |     87.4 |     87.6 |     88.2 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  1 | Bi    | POS   |     87.9 |     93.3 |     92.9 |     93.2 |     92.8 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  2 | Bi    | SEM   |     81.9 |     91.3 |     90.8 |     91.9 |     91.9 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  3 | Res   | POS   |     87.9 |     92.5 |     91.9 |     92   |     92.4 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  4 | Res   | SEM   |     81.9 |     88.2 |     87.5 |     87.6 |     88.5 |\n+----+-------+-------+----------+----------+----------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms.",
        "evidence": "As we can see, DocSubgroups is able to link more relations than TF and Dsim, but as discussed in the last section, this is not a very strong baseline. In fact, DoubleTree is closer to AverageDepth than DocSubdepth on the TED talks dataset, which is also the case on the Europarl datasets. The main reason for this is that, as we only consider the number of terms, the differences between train and test sets are not significant. In other words, we would expect DocSubgroups and DoubleTree to be more coherent than DoubleTree and AverageDeep on their own. We make several observations. First, using MaxDepth or AverageDepth does not always help. The main reason is that, when too many training terms are involved, the number of topics is increased, and thus the similarity between two terms tends to be lost. In other words, the semantic (or syntactic) similarity is not measured by the number of terms, but by the similarity of their relations. Considering that the length of terms in the TED talks is usually quite short (1,000), the proposed measures are not strictly necessary. Second, DocSubgroups and DoubleTree are better than TF and DepthCohesion on the German and English parallel datasets, but they are worse on the TED talks dataset. This is not surprising, since the TED talks are deliberately constructed in a way that each term should have a high semantic similarity with every other term. Third, considering AverageDepth or DocSubdepth does not help much either. It is also not surprising that DF and HLUST have more hierarchical structures compared to the other two, as well as the number of relations. Still, one of the main reasons for the inferior performance of DocSubgroups compared to DF and HLUST is due to the hierarchical structure, as we will see in the next section.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 957    | 1,000  | 1,000  | 1,000 | 1,000 | 836      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 44     | 1      | 1      | 1     | 1     | 43       | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,588  | 1,025  | 1,028  | 1,185 | 1,103 | 1,184    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 21     | 921    | 901    | 788   | 835   | 8        | 15       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 921    | 901    | 788   | 835   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 11.82  | 921    | 901    | 788   | 835   | 3.05     | 8.46     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 1.78   | 1      | 1      | 1     | 1     | 2.62     | 1.77     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 20     | 2      | 3      | 4     | 3     | 88       | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.99   | 1.03   | 1.03   | 1.19  | 1.10  | 4.20     | 2.38     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 476    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 164    | 2      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 521    | 1,029  | 1,331  | 3,025 | 3,438 | 3,802    | 1,009    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 16     | 915    | 658    | 454   | 395   | 118      | 12       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 913    | 658    | 454   | 395   | 110      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 5.82   | 914    | 658    | 454   | 395   | 112.24   | 5.95     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.75   | 1      | 1      | 1     | 1     | 1.05     | 2.02     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 25     | 2      | 77     | 13    | 12    | 66       | 98       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.83   | 1.03   | 1.36   | 3.03  | 3.44  | 6.64     | 2.35     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "This is especially true in the case of DAN where we see a decrease as the decoder repeatedly predicts words having low sentiment value.",
        "evidence": "The RNN appears to predict the sentiment better than the CNN, especially when the latter predicts words having a low sentiment value. This is especially true for the negative case, where the DAN prediction has a decrease of 23.6%.",
        "table": "+----+---------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]             |   <bold>RNN</bold> |   <bold>CNN</bold> | <bold>DAN</bold>   |\n+====+=====================+====================+====================+====================+\n|  0 | Positive            |                9.7 |                4.3 | +<bold>23.6</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  1 | Negative            |                6.9 |                5.5 | +<bold>16.1</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  2 | Flipped to Positive |               20.2 |               24.9 | +27.4              |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  3 | Flipped to Negative |               31.5 |               28.6 | +19.3              |\n+----+---------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "It achieves competitive results using only the title and body text, in comparison to the R-1,2,L reward systems that integrate multi-task models (Narayan et\\xa0al.",
        "evidence": "The results show that our neuralTD system achieves competitive results, in comparison to the state-of-the-art systems that integrate multi-task models (Narayan et\\xa0al.. Our model, which is trained only on the development set, outperforms all of these systems, including the more sophisticated R-1,2,L reward systems that integrate multi-task models Narayan et\\xa0al..",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "However, EWC outperforms no-reg and L2 on News, not only reducing forgetting but giving 0.5 BLEU improvement over the baseline News model.",
        "evidence": "Here, the News and TED models are fine-tuned using the same training set. However, EWC outperforms No-Reg and L2 on News, not only reducing forgetting but giving 0.5 BLEU improvement over the baseline model. We conclude that the quality of model fine-tuning is critical to the quality of downstream tasks.",
        "table": "+----+-----------+--------------------------+---------------+--------------+-------------+\n|    |   [EMPTY] | [BOLD] Training scheme   | [BOLD] News   | [BOLD] TED   | [BOLD] IT   |\n+====+===========+==========================+===============+==============+=============+\n|  0 |         1 | News                     | 37.8          | 25.3         | 35.3        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  1 |         2 | TED                      | 23.7          | 24.1         | 14.4        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  2 |         3 | IT                       | 1.6           | 1.8          | 39.6        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  3 |         4 | News and TED             | 38.2          | 25.5         | 35.4        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  4 |         5 | 1 then TED, No-reg       | 30.6          | [BOLD] 27.0  | 22.1        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  5 |         6 | 1 then TED, L2           | 37.9          | 26.7         | 31.8        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  6 |         7 | 1 then TED, EWC          | [BOLD] 38.3   | [BOLD] 27.0  | 33.1        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  7 |         8 | 5 then IT, No-reg        | 8.0           | 6.9          | 56.3        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  8 |         9 | 6 then IT, L2            | 32.3          | 22.6         | 56.9        |\n+----+-----------+--------------------------+---------------+--------------+-------------+\n|  9 |        10 | 7 then IT, EWC           | 35.8          | 24.6         | [BOLD] 57.0 |\n+----+-----------+--------------------------+---------------+--------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The Logistic Regression model achieved the best results with a F1-score of 0.679 on the training dataset and a F1-score of 0.572 on the test dataset.",
        "evidence": "Baseline Models We evaluated baseline models using the provided training dataset. The Baseline model used count-based vectorizer for finding suitable classifiers for the given problem. We can see that the logistic regression model achieved the best results on the training dataset and also achieved a F1-score of 0.572 on the test dataset..",
        "table": "+----+---------------------------------------------------------+---------------------+--------------------+\n|    | [BOLD] Model                                            |   [BOLD] F1 (train) |   [BOLD] F1 (test) |\n+====+=========================================================+=====================+====================+\n|  0 | [BOLD] Multinomial Naive Bayes (using Count Vectorizer) |               0.641 |              0.517 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  1 | [BOLD] Logistic Regression (using Count Vectorizer)     |               0.679 |              0.572 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  2 | [BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer)     |               0.695 |              0.576 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  3 | [BOLD] LSTM (128 LSTM Units)                            |               0.731 |              0.591 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  4 | [BOLD] Provided Baseline                                |               0.72  |              0.267 |\n+----+---------------------------------------------------------+---------------------+--------------------+\n|  5 | [BOLD] ULMFit*                                          |               0.861 |              0.701 |\n+----+---------------------------------------------------------+---------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "These result reveal that there exist trade-offs between the different metrics and that a DLM-based algorithm is better suited to solve the user simulation problem than reinforcement learning",
        "evidence": "From the table, we can see that GDPL performs better than the other methods and that there exist trade-offs between the three metrics. We believe that a DLM-based algorithm is better suited to solve the user simulation task than reinforcement learning.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "Furthermore, the scope length of negative instances is at the range of 0-8 tokens, with an average scope length of 2.9 tokens.",
        "evidence": "In order to verify whether the length of negation cues is reasonable, we calculated the lexical length of all negation cues. We observe that the average scope length of negative instances is at the range of 0 - 8 tokens. The average scope length of true negation cues is 2674 tokens. This is significantly longer than the average scope length of sentences. Further, the number of false negation cues is 247, which is significantly longer than the average scope length.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "We gain further improvement by adding monolingual data and get an even higher accuracy of 75.5%, which is 10.1 points higher than the best language model.",
        "evidence": "Compared to the SOTA, the fine-tuned model gets an even better accuracy of 75.5%, which is 10.1 points higher than the best language model.",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Thus, after taking the depth in KG into consideration, the precision increases to 19.47%, which increases the AUC score to 0.413.",
        "evidence": "Thus, the depth in KG is also taken into account. In particular, the precision increases to 19.47% when d=32 and the AUC score increases to 0.413.. The training time increases from 4 hours to 20 hours.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "Our joint model outperforms all the base [CONTINUE] lines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points.",
        "evidence": "Our joint model outperforms all the base [CONTINUE] lines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points..",
        "table": "+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|    | [BOLD] Model                     | R       | MUC P   | [ITALIC] F1   | B3 P    | CEAF- [ITALIC] e P   | CoNLL  [ITALIC] F1   |\n+====+==================================+=========+=========+===============+=========+======================+======================+\n|  0 | [BOLD] Baselines                 | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  1 | Cluster+Lemma                    | 75.5    | 79.9    | 73.6          | 85      | 71.7                 | 76.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  2 | CV Cybulska and Vossen ( 2015a ) | -       | 75      | 64            | 78      | -                    | 73                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  3 | KCP Kenyon-Dean et al. ( 2018 )  | 71      | 71      | 69            | 67      | 67                   | 69                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  4 | Cluster+KCP                      | 77.4    | 79.3    | 71.5          | 87.2    | 66.4                 | 73.6                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  5 | [BOLD] Model Variants            | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  6 | Disjoint                         | 80.3    | 83.6    | 75.9          | 86      | 71.9                 | 78.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  7 | Joint                            | 81      | 84.5    | 77.3          | 85.1    | 73.8                 | [BOLD] 79.5          |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "Interestingly, the size and type of whitelist have a significant effect on performance, indicating that all the whitelists do not contain responses appropriate to a variety of conversational contexts.",
        "evidence": "The results show that the size and type of whitelists have a significant effect on performance. A closer look at the results revealed that all the whitelists contain responses appropriate to a variety of conversational contexts. The size and type of whitelists have a significant effect on performance, indicating that all the whitelists contain responses appropriate to a variety of conversational contexts. Interestingly, the size and type of whitelists seem to have little effect on the performance of the system. This may be due to the fact that all the whitelists contain responses appropriate to the conversational contexts.",
        "table": "+----+--------------------+----------------+---------------+--------------+-----------------+\n|    | [BOLD] Whitelist   | [BOLD] Great   | [BOLD] Good   | [BOLD] Bad   | [BOLD] Accept   |\n+====+====================+================+===============+==============+=================+\n|  0 | Freq. 1K           | 54%            | 26%           | 20%          | 80%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  1 | Cluster. 1K        | 55%            | 21%           | 23%          | 77%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  2 | Freq. 10K          | 56%            | 24%           | 21%          | 80%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  3 | Cluster. 10K       | 57%            | 23%           | 20%          | 80%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  4 | Real response      | 60%            | 24%           | 16%          | 84%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+",
        "label": "refutes"
    },
    {
        "claim": "Most of the false negation cues correspond to contracted negations (e.g., “haven’t”).",
        "evidence": "The total number of false negation cues in the gold data set is 2921. Of those, 247 are true negation cues and the remaining 247 are false negation cues. Most of the false negation cues are from short tweets. We calculate the average scope length and average sentence length of tweets in our evaluation dataset. Each tweet has an average scope length of 2.9 tokens and an average sentence length of 13.6 tokens. So, a naïve approach might conclude that the tweets are all false negation cues. This is not true, however, because the amount of false negation cues is relatively small. The average number of false negation cues is 22.3% of the total.",
        "table": "+----+-------------------------+--------+\n|    | Total negation cues     |   2921 |\n+====+=========================+========+\n|  0 | True negation cues      | 2674   |\n+----+-------------------------+--------+\n|  1 | False negation cues     |  247   |\n+----+-------------------------+--------+\n|  2 | Average scope length    |    2.9 |\n+----+-------------------------+--------+\n|  3 | Average sentence length |   13.6 |\n+----+-------------------------+--------+\n|  4 | Average tweet length    |   22.3 |\n+----+-------------------------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "We find that the effect of syntactic structure is consistent across the different relation types.",
        "evidence": "We use the development data to evaluate the effect of structural information on F1. Results indicate that the syntactic structure does not significantly affect the model’s performance for different relation types. For the Part_whole relation, the F1 score improves by +40.76%. Similar to the previous case, the model-feature structure contributes more than +21.11% for the Utility relation.",
        "table": "+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|    | [BOLD] Relation   |   [BOLD] best F1 (in 5-fold) without sdp |   [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.   |\n+====+===================+==========================================+=======================================+================+\n|  0 | USAGE             |                                    60.34 |                                 80.24 | + 19.90        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  1 | MODEL-FEATURE     |                                    48.89 |                                 70    | + 21.11        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  2 | PART_WHOLE        |                                    29.51 |                                 70.27 | +40.76         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  3 | TOPIC             |                                    45.8  |                                 91.26 | +45.46         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  4 | RESULT            |                                    54.35 |                                 81.58 | +27.23         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  5 | COMPARE           |                                    20    |                                 61.82 | + 41.82        |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+\n|  6 | macro-averaged    |                                    50.1  |                                 76.1  | +26.00         |\n+----+-------------------+------------------------------------------+---------------------------------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "Coverage helps the model improve its EM by 1.5 and its F1 by 0.5.",
        "evidence": "Table 5 shows that [CONTINUE] The coverage helps the model improve its EM by 1.5 and F1 by 0.5 on the in-domain SQuAD and the out-of-domain QA-SRL dataset.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We first use order-based feature which is relative to PPO to show our improvement.",
        "evidence": "ALDM is significantly better than PPO in all aspects (sign test, p-value < 0.01) which is consistent with our expectation.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] In addition, the presence of verbs in past participle (VBN) is the most distinctive part-of-speech pattern of complaints.",
        "evidence": "Top unigrams and part-of-speech patterns of complaints and not complaints are presented in Table 1. [CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] However, verbs are a distinctive part-of-speech pattern of complaints more than not complaints. Several examples are provided in Table 1. [CONTINUE] For example, the presence of the verbs “error”, “repair”, “won’t”, “regime change” are typical parts-of-speech patterns of complaints. [CONTINUE] Also, other standard part-of-speech patterns of complaints are not distinctive of not complaints (thank, great, win, POSEMO, AFFECT, ASSENT) but just unigrams of complaints. [CONTINUE] In addition, the presence of verbs in past participle (VBN) is also distinctive. [CONTINUE] Complaints use more standard part-of-speech patterns than not complaints. [CONTINUE] As expected, the presence of verbs outside the past participle clusters is a distinctive part-of-speech pattern of complaints. [CONTINUE] Complaints use more standard part-of-speech patterns for verbs outside the past participle cluster. [CONTINUE] In addition, other standard part-of-speech patterns distinctive of complaints are not verbs, like “good, great”, “won’t”, “regime change”, “day”, “sentence”, etc. [CONTINUE] However, unigrams of verbs outside the past participle cluster are not distinctive of complaints.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "for example, for BERT, the error rates for all the runs are negative with at most 0.05% accuracy loss and at most 0.12% accuracy gain",
        "evidence": "The results only show that pretrained BERT and RoBERTa models significantly outperform the baseline for all configurations. Moreover, the results on the hard data are consistently better than the results on the easy data. Since BERT is not pre-trained trained on a large amount of corpora, its performance drop on the hard data is much lower than that of RoBERTa.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is only marginally better than both the other baseline models.",
        "evidence": "This gap is largest in KP20k, the largest of the four datasets, where our GAN model is only marginally better than both the other baseline models.",
        "table": "+----+-----------+--------------+--------------+--------------+-------------+\n|    | Model     | Inspec       | Krapivin     | NUS          | KP20k       |\n+====+===========+==============+==============+==============+=============+\n|  0 | Catseq    | 0.87803      | 0.781        | 0.82118      | 0.804       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  1 | Catseq-RL | 0.8602       | [BOLD] 0.786 | 0.83         | 0.809       |\n+----+-----------+--------------+--------------+--------------+-------------+\n|  2 | GAN       | [BOLD] 0.891 | 0.771        | [BOLD] 0.853 | [BOLD] 0.85 |\n+----+-----------+--------------+--------------+--------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "FME outperforms the AME model, confirming the importance of word embeddings adaptation.",
        "evidence": "The main takeaway from our results is that the AME model is significantly better than the FME model in all cases. In particular, we observe significant improvement in R@1 and R@10. For instance, in the case of English to German (i.e., task) adaptation, AME outperforms FME by +1.8% in R@1 and +3.5% in R@10. This strongly suggests that word embeddings adaptation is crucial for NMT.",
        "table": "+----+-----------+---------------+---------------+----------------+---------------+---------------+----------------+\n|    | [EMPTY]   | EN → DE R@1   | EN → DE R@5   | EN → DE R@10   | DE → EN R@1   | DE → EN R@5   | DE → EN R@10   |\n+====+===========+===============+===============+================+===============+===============+================+\n|  0 | FME       | 51.4          | 76.4          | 84.5           | 46.9          | 71.2          | 79.1           |\n+----+-----------+---------------+---------------+----------------+---------------+---------------+----------------+\n|  1 | AME       | [BOLD] 51.7   | [BOLD] 76.7   | [BOLD] 85.1    | [BOLD] 49.1   | [BOLD] 72.6   | [BOLD] 80.5    |\n+----+-----------+---------------+---------------+----------------+---------------+---------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "Patt model could not generate relations for all terms because terms must to be in a pattern in order to have their taxonomic relation identified.",
        "evidence": "This indicates that attention based methods may be more generalized to untranslated and recursive concepts. This is especially true when performing analysis on heterogeneous tag distributions. In other words, one should map each concept to its nearest integer in the matrix R, and use the cosine similarity between the two vectors to find their taxonomic relation. From this table, we can see that the DocSubtree graph can be used to generate relations for concepts which are not sparse in the sense that X∈Rd×d is, in fact, a trigram bag-of-words. However, this is not the case with concepts which are dense in the sense that X∈Rd×d is a trigram bag-of-words. In fact, for concepts which are sparse in the sense that X∈Rd×d is a trigram bag-of-words, the DocSubtree graph can be used to identify relations, but only between X and Y. For example, if the terms “chocolate milk” and “milk chocolate” are not sparse in the sense that X∈Rd×d, they are counted as two different terms. On the other hand, for concepts which are dense in the sense that X∈Rd×d is a trigram bag-of-words, they can be counted as two different terms. This greatly limits the application of the \\glsVD algorithm here.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 957    | 1,000  | 1,000  | 1,000 | 1,000 | 836      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 44     | 1      | 1      | 1     | 1     | 43       | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,588  | 1,025  | 1,028  | 1,185 | 1,103 | 1,184    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 21     | 921    | 901    | 788   | 835   | 8        | 15       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 921    | 901    | 788   | 835   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 11.82  | 921    | 901    | 788   | 835   | 3.05     | 8.46     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 1.78   | 1      | 1      | 1     | 1     | 2.62     | 1.77     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 20     | 2      | 3      | 4     | 3     | 88       | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.99   | 1.03   | 1.03   | 1.19  | 1.10  | 4.20     | 2.38     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 476    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 164    | 2      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 521    | 1,029  | 1,331  | 3,025 | 3,438 | 3,802    | 1,009    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 16     | 915    | 658    | 454   | 395   | 118      | 12       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 913    | 658    | 454   | 395   | 110      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 5.82   | 914    | 658    | 454   | 395   | 112.24   | 5.95     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.75   | 1      | 1      | 1     | 1     | 1.05     | 2.02     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 25     | 2      | 77     | 13    | 12    | 66       | 98       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.83   | 1.03   | 1.36   | 3.03  | 3.44  | 6.64     | 2.35     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "However, the slightly increased invalid response percentage for the DAMD (+) model compared to the HDSA (+) model suggests that data augmentation may not be the most effective approach. We also observe that HDSA (+) outperforms DAMD in both diversity and appropriateness scores.",
        "evidence": "We see that HDSA (+) has a slightly higher response percentage than DAMD (+). This may suggest that data augmentation may not be the most effective approach. We also observe that HDSA (+) outperforms DAMD in both diversity and appropriateness scores. This may suggest that HDSA may generate more diverse responses than the existing approaches. However, the slightly increased invalid response percentage for the DAMD (+) model compared to the HDSA (+) model may not be the most effective approach.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "When using our learned reward, the generated summaries have significantly higher average human ratings than when using ROUGE as rewards.",
        "evidence": "Table. We simply use ROUGE as the reward in our experiments. The summaries generated using our learned reward have significantly higher average human ratings than when using ROUGE as rewards.",
        "table": "+----+----------------+-------+-------+-------+-------------+-----------+\n|    | Reward         |   R-1 |   R-2 |   R-L | Human       | Pref%     |\n+====+================+=======+=======+=======+=============+===========+\n|  0 | R-L (original) |  40.9 |  17.8 |  38.5 | 1.75        | 15        |\n+----+----------------+-------+-------+-------+-------------+-----------+\n|  1 | Learned (ours) |  39.2 |  17.4 |  37.5 | [BOLD] 2.20 | [BOLD] 75 |\n+----+----------------+-------+-------+-------+-------------+-----------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Mentions of time are specific of complaints (been, still, on, days, Temporal References cluster).",
        "evidence": "Top unigrams and part-of-speech features (unigrams) for complaints are: [url] Complaints : User mentions of time are specific of complaints. For example, the user mentions “been” and “been on” in the first part of the claim and “lol” or “good” in the second part. Other unigrams (verbs) are like “no”, “he”, “correct”, “won’t”, “regime change”, etc.",
        "table": "+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Feature   | [BOLD] Complaints  [ITALIC] r     | [BOLD] Not Complaints  [BOLD] Feature   | [BOLD] Not Complaints  [ITALIC] r   |\n+====+=====================================+===================================+=========================================+=====================================+\n|  0 | [BOLD] Unigrams                     | [BOLD] Unigrams                   | [BOLD] Unigrams                         | [BOLD] Unigrams                     |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  1 | not                                 | .154                              | [URL]                                   | .150                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  2 | my                                  | .131                              | !                                       | .082                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  3 | working                             | .124                              | he                                      | .069                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  4 | still                               | .123                              | thank                                   | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  5 | on                                  | .119                              | ,                                       | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  6 | can’t                               | .113                              | love                                    | .064                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  7 | service                             | .112                              | lol                                     | .061                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  8 | customer                            | .109                              | you                                     | .060                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n|  9 | why                                 | .108                              | great                                   | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 10 | website                             | .107                              | win                                     | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 11 | no                                  | .104                              | ’                                       | .058                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 12 | ?                                   | .098                              | she                                     | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 13 | fix                                 | .093                              | :                                       | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 14 | won’t                               | .092                              | that                                    | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 15 | been                                | .090                              | more                                    | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 16 | issue                               | .089                              | it                                      | .052                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 17 | days                                | .088                              | would                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 18 | error                               | .087                              | him                                     | .047                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 19 | is                                  | .084                              | life                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 20 | charged                             | .083                              | good                                    | .046                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 21 | [BOLD] POS (Unigrams and Bigrams)   | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)       | [BOLD] POS (Unigrams and Bigrams)   |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 22 | VBN                                 | .141                              | UH                                      | .104                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 23 | $                                   | .118                              | NNP                                     | .098                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 24 | VBZ                                 | .114                              | PRP                                     | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 25 | NN_VBZ                              | .114                              | HT                                      | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 26 | PRP$                                | .107                              | PRP_.                                   | .076                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 27 | PRP$_NN                             | .105                              | PRP_RB                                  | .067                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 28 | VBG                                 | .093                              | NNP_NNP                                 | .062                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 29 | CD                                  | .092                              | VBP_PRP                                 | .054                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 30 | WRB_VBZ                             | .084                              | JJ                                      | .053                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+\n| 31 | VBZ_VBN                             | .084                              | DT_JJ                                   | .051                                |\n+----+-------------------------------------+-----------------------------------+-----------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Our summaries are notably shorter than in other works, about 260 words on average.",
        "evidence": "It can be observed that our summaries are notably shorter than in other works, about 260 words on average. Moreover, they are also significantly shorter than in other works, ranging from 109.58 words (DUC03+04) to 99.70 words (TAC 2011) on average, and also significantly shorter than in Multi-News, with only 82.73 words on average.",
        "table": "+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|    | [BOLD] Dataset   | [BOLD] # pairs        | [BOLD] # words (doc)   |   [BOLD] # sents (docs) |   [BOLD] # words (summary) |   [BOLD] # sents (summary) | [BOLD] vocab size   |\n+====+==================+=======================+========================+=========================+============================+============================+=====================+\n|  0 | Multi-News       | 44,972/5,622/5,622    | 2,103.49               |                   82.73 |                     263.66 |                       9.97 | 666,515             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  1 | DUC03+04         | 320                   | 4,636.24               |                  173.15 |                     109.58 |                       2.88 | 19,734              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  2 | TAC 2011         | 176                   | 4,695.70               |                  188.43 |                      99.7  |                       1    | 24,672              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  3 | CNNDM            | 287,227/13,368/11,490 | 810.57                 |                   39.78 |                      56.2  |                       3.68 | 717,951             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 74.2%, 0.3 points less than the accuracy of the FINE-TUNED-LM model.",
        "evidence": "We can see that the use of discriminative training, even without additional monolingual data, leads to better performance than that of the best language model. The accuracies of the Spanish-only-LM, English-only-LM, and All:CS-Last-LM models are all significantly better than that of the best language model: the Spanish-only-LM achieves an accuracy of 74.2%, 0.3 points less than the accuracy of the fine-tuned-LM model. Similarly, the English-only-LM, Spanish-only-LM, and All:Shuffled-LM models achieve significantly better accuracies than that of the best language model: the Spanish-only-LM achieves an accuracy of 74.2%, 0.3 points less than the accuracy of the fine-tuned-LM model..",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "It does not come close to VGS on paraphrase retrieval, and it does not correlate with the visual modality better.",
        "evidence": "This does not come close to predicting the correct paraphrase. We believe that this is because the visual modality better captures sub-tasks than semantic modalities. We believe that the sub-tasks depend on the image modality more than the semantic modality does, to the extent that different sub-tasks can be solved by different visual modalities. However, the sub-tasks still require the image modality to be able to answer the paraphrase prediction correctly. We believe that this will be left as the future work.",
        "table": "+----+-------------+-----------------+---------------+------------+\n|    | [EMPTY]     | Recall@10 (%)   | Median rank   | RSAimage   |\n+====+=============+=================+===============+============+\n|  0 | VGS         | 27              | 6             | 0.4        |\n+----+-------------+-----------------+---------------+------------+\n|  1 | SegMatch    | [BOLD] 10       | [BOLD] 37     | [BOLD] 0.5 |\n+----+-------------+-----------------+---------------+------------+\n|  2 | Audio2vec-U | 5               | 105           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  3 | Audio2vec-C | 2               | 647           | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  4 | Mean MFCC   | 1               | 1,414         | 0.0        |\n+----+-------------+-----------------+---------------+------------+\n|  5 | Chance      | 0               | 3,955         | 0.0        |\n+----+-------------+-----------------+---------------+------------+",
        "label": "refutes"
    },
    {
        "claim": "ACER and PPO do not obtain high performance in inform F1 and match rate.",
        "evidence": "From the table, we can see that GDPL achieves the best performance in Inform F1 and match rate over all baselines. GDPL even surpasses human in completing 7 out of 30 agenda turns. By comparing GDPL with other baselines, we can see that incorporating goal information is crucial to get high performance. For example, ACER, PPO and ALDM, ACER achieves only 19.04% and 28.9% in inform F1 and match rate, respectively, as compared to 19.99% and 94.97% in GDPL. PPO is also not effective in completing the task. Although ALDM gets 12.47% in inform F1 and 61.2% in match rate, it is still worse than PPO. This result also indicates that PPO does not utilize the goal information well.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "In general, the performance of the model does not drop substantially as we remove more dense connections.",
        "evidence": "We removed the more dense blocks from the DCGCN4 model, and evaluated the performance of the model. In particular, we replaced each dense block with another dense block and evaluated the performance. We see that the performance of the model does not drop very much when we remove more dense blocks. This indicates that the model does not heavily require very dense connections.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "refutes"
    },
    {
        "claim": "After removing the graph attention module, our model gives 22.9 BLEU points.",
        "evidence": "It shows that after removing the graph attention module, the BLEU score drops from 22.9 to 24.2. It also shows that the linear combination module and the graph attention module both contribute to the performance.",
        "table": "+----+---------------------------------+---------+---------+\n|    | [BOLD] Model                    | B       | C       |\n+====+=================================+=========+=========+\n|  0 | DCGCN4                          | 25.5    | 55.4    |\n+----+---------------------------------+---------+---------+\n|  1 | Encoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  2 | -Linear Combination             | 23.7    | 53.2    |\n+----+---------------------------------+---------+---------+\n|  3 | -Global Node                    | 24.2    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  4 | -Direction Aggregation          | 24.6    | 54.6    |\n+----+---------------------------------+---------+---------+\n|  5 | -Graph Attention                | 24.9    | 54.7    |\n+----+---------------------------------+---------+---------+\n|  6 | -Global Node&Linear Combination | 22.9    | 52.4    |\n+----+---------------------------------+---------+---------+\n|  7 | Decoder Modules                 | [EMPTY] | [EMPTY] |\n+----+---------------------------------+---------+---------+\n|  8 | -Coverage Mechanism             | 23.8    | 53.0    |\n+----+---------------------------------+---------+---------+",
        "label": "refutes"
    },
    {
        "claim": "On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes.",
        "evidence": "Also, the number of distinct MRs rose sharply after reannotation. The variance in the number of attributes also increased.",
        "table": "+----+---------------------+---------------+--------------+---------------+-----------------+\n|    | [BOLD] Dataset      | [BOLD] Part   | [BOLD] MRs   | [BOLD] Refs   | [BOLD] SER(%)   |\n+====+=====================+===============+==============+===============+=================+\n|  0 | Original            | Train         | 4,862        | 42,061        | 17.69           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  1 | Original            | Dev           | 547          | 4,672         | 11.42           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  2 | Original            | Test          | 630          | 4,693         | 11.49           |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  3 | [0.5pt/2pt] Cleaned | Train         | 8,362        | 33,525        | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  4 | [0.5pt/2pt] Cleaned | Dev           | 1,132        | 4,299         | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+\n|  5 | [0.5pt/2pt] Cleaned | Test          | 1,358        | 4,693         | (0.00)          |\n+----+---------------------+---------------+--------------+---------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics do not perform as well as the part of speech tags.",
        "evidence": "This claim is supported by the results of the neural network models which performed the best in the task of sentiment prediction. Specifically, the best results were obtained by the LSTM model with LSTM layer, max-pooling layer, bidirectional LSTM layer, and max-pooling factor with kernel size 3. This model achieved the best results in terms of accuracy, F1, and area under the ROC curve (AUC). This also supports the claim that syntactic and semantic features do not perform as well as the part of speech tags. The Word2Vec topics outperform the LIWC dictionaries, but only by a small amount. From this table, we can see that syntactic features such as Word2Vec do help sentiment analysis, but they do not help the LSTM model in achieving the best results.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "In fact, DocSub had worse results in precision when using both Europarl and Ted Talks corpora in English, where DF reached best values of precision and f-measure.",
        "evidence": "This table shows that for English, DocSubtask had worse results than TF and Dsim when using the English version of Europarl. In contrast, it was worse than F-Measure and F-value for DocSubtask when using the German and TED talks corpora.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "However, our model generates shorter sentences than human arguments, with about 15 words per sentence compared to 22 words per sentence for human arguments.",
        "evidence": "Results: Our model is generally better than human results, achieving a ∼3%∼4% improvement on b-2 retrieval and ∼1.5%∼3.5% on mtr retrieval for b-4. retrieval. Generally, our models generate sentences around 15 words per sentence, which is shorter than human sentences and ∼5 words per sentence for system arguments. We can see that results with oracle retrieval are better than those without (cf. Generally, our models are ∼3%∼4% better than human results with the use of PSG feature. However, the gap w/ oracle retrieval is larger, ∼5%∼10% depending on the best b-2 retrieval. We believe that this is because “sentence” is the most difficult part of system retrieval (cf. For retrieval, “sentence” retrieves b-2 and “item” retrieval b-4, while “item” retrieves b-1 and “item” retrieval b-2. In terms of mtr, our retrieval is better than those of retrieval (cf. Generally, results with our models are ∼2% better than those of retrieval otherwise. Comparing our results with human results, we still have a huge gap to human results, which is mainly due to the complexity of human arguments.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "These results show that our model is not as effective in terms of using automatically generated AMR graphs.",
        "evidence": "We just perform an experiment to show that our model is not as effective as the one in using automatically generated AMR graphs. We simply combine DCGCN(ensemble) and DCGCN(single) in our experiments, and examine the results. For comparison, we also include the most recent baseline, Tree2str, which requires the manual annotation of an AMR graph. From the results, we can see that our single model is slightly worse than the baseline, but still better than all the other models, which means that our GCN model is able to leverage the information from the AMR graphs. Moreover, DCGCN(ensemble) performs better than GCN(single), which shows the effectiveness of our sequence-to-sequence framework. Although there is still a large gap between our model and the other baselines, we believe this is because our model is able to leverage the information from two GCN layers, which is rare in AMR graphs.",
        "table": "+----+------------------------------------+-------------------+-------------+\n|    | [BOLD] Model                       | [BOLD] External   | B           |\n+====+====================================+===================+=============+\n|  0 | Seq2SeqK (Konstas et al.,  2017 )  | -                 | 22.0        |\n+----+------------------------------------+-------------------+-------------+\n|  1 | GraphLSTM (Song et al.,  2018 )    | -                 | 23.3        |\n+----+------------------------------------+-------------------+-------------+\n|  2 | GCNSEQ (Damonte and Cohen,  2019 ) | -                 | 24.4        |\n+----+------------------------------------+-------------------+-------------+\n|  3 | DCGCN(single)                      | -                 | 25.9        |\n+----+------------------------------------+-------------------+-------------+\n|  4 | DCGCN(ensemble)                    | -                 | [BOLD] 28.2 |\n+----+------------------------------------+-------------------+-------------+\n|  5 | TSP (Song et al.,  2016 )          | ALL               | 22.4        |\n+----+------------------------------------+-------------------+-------------+\n|  6 | PBMT (Pourdamghani et al.,  2016 ) | ALL               | 26.9        |\n+----+------------------------------------+-------------------+-------------+\n|  7 | Tree2Str (Flanigan et al.,  2016 ) | ALL               | 23.0        |\n+----+------------------------------------+-------------------+-------------+\n|  8 | SNRG (Song et al.,  2017 )         | ALL               | 25.6        |\n+----+------------------------------------+-------------------+-------------+\n|  9 | Seq2SeqK (Konstas et al.,  2017 )  | 0.2M              | 27.4        |\n+----+------------------------------------+-------------------+-------------+\n| 10 | GraphLSTM (Song et al.,  2018 )    | 0.2M              | 28.2        |\n+----+------------------------------------+-------------------+-------------+\n| 11 | DCGCN(single)                      | 0.1M              | 29.0        |\n+----+------------------------------------+-------------------+-------------+\n| 12 | DCGCN(single)                      | 0.2M              | [BOLD] 31.6 |\n+----+------------------------------------+-------------------+-------------+\n| 13 | Seq2SeqK (Konstas et al.,  2017 )  | 2M                | 32.3        |\n+----+------------------------------------+-------------------+-------------+\n| 14 | GraphLSTM (Song et al.,  2018 )    | 2M                | 33.6        |\n+----+------------------------------------+-------------------+-------------+\n| 15 | Seq2SeqK (Konstas et al.,  2017 )  | 20M               | 33.8        |\n+----+------------------------------------+-------------------+-------------+\n| 16 | DCGCN(single)                      | 0.3M              | 33.2        |\n+----+------------------------------------+-------------------+-------------+\n| 17 | DCGCN(ensemble)                    | 0.3M              | [BOLD] 35.3 |\n+----+------------------------------------+-------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Furthermore, this bias is seemingly not aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics.",
        "evidence": "From this table, we see that the bias is relatively aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics.",
        "table": "+----+------------------------------------------------+--------------+------------+---------------+\n|    | Category                                       |   Female (%) |   Male (%) |   Neutral (%) |\n+====+================================================+==============+============+===============+\n|  0 | Office and administrative support              |       11.015 |     58.812 |        16.954 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  1 | Architecture and engineering                   |        2.299 |     72.701 |        10.92  |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  2 | Farming, fishing, and forestry                 |       12.179 |     62.179 |        14.744 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  3 | Management                                     |       11.232 |     66.667 |        12.681 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  4 | Community and social service                   |       20.238 |     62.5   |        10.119 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  5 | Healthcare support                             |       25     |     43.75  |        17.188 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  6 | Sales and related                              |        8.929 |     62.202 |        16.964 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  7 | Installation, maintenance, and repair          |        5.22  |     58.333 |        17.125 |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  8 | Transportation and material moving             |        8.81  |     62.976 |        17.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n|  9 | Legal                                          |       11.905 |     72.619 |        10.714 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 10 | Business and financial operations              |        7.065 |     67.935 |        15.58  |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 11 | Life, physical, and social science             |        5.882 |     73.284 |        10.049 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 12 | Arts, design, entertainment, sports, and media |       10.36  |     67.342 |        11.486 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 13 | Education, training, and library               |       23.485 |     53.03  |         9.091 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 14 | Building and grounds cleaning and maintenance  |       12.5   |     68.333 |        11.667 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 15 | Personal care and service                      |       18.939 |     49.747 |        18.434 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 16 | Healthcare practitioners and technical         |       22.674 |     51.744 |        15.116 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 17 | Production                                     |       14.331 |     51.199 |        18.245 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 18 | Computer and mathematical                      |        4.167 |     66.146 |        14.062 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 19 | Construction and extraction                    |        8.578 |     61.887 |        17.525 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 20 | Protective service                             |        8.631 |     65.179 |        12.5   |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 21 | Food preparation and serving related           |       21.078 |     58.333 |        17.647 |\n+----+------------------------------------------------+--------------+------------+---------------+\n| 22 | Total                                          |       11.76  |     58.93  |        15.939 |\n+----+------------------------------------------------+--------------+------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
        "evidence": "Our re-implementation of the Clustering+LEmma baseline provides a strong improvement upon the strong lemma baseline of 3.8 points in CoNLL F1 score.. Our joint model also achieves a gain of 3.8 points in CoNLL F1 score.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "As shown in Table 5, as the required derivation step increases, the PRKGC+NS model suffers from predicting answer entities and generating correct NLDs.",
        "evidence": "As shown in Table 5, as the required derivation step increases, the PRKGC+NS model suffers from predicting answer entities and generating correct NLDs. [CONTINUE] This indicates that the challenge of RC-QEDE is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive an answer.",
        "table": "+----+--------------------+----------------+--------------------+\n|    |   # gold NLD steps |   Answer Prec. |   Derivation Prec. |\n+====+====================+================+====================+\n|  0 |                  1 |           79.2 |               38.4 |\n+----+--------------------+----------------+--------------------+\n|  1 |                  2 |           64.4 |               48.6 |\n+----+--------------------+----------------+--------------------+\n|  2 |                  3 |           62.3 |               41.3 |\n+----+--------------------+----------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "As filtering out multiple hypernyms might remove also correct relations, the recall values for all corpora are very low.",
        "evidence": "As filtering out multiple hypernyms might remove also correct relations, the recall values are very low.. Only the TF dataset has a very high recall value.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS |     TF |     DF | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+========+========+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1038 | 0.017  | 0.049  | 0.0641 | 0.0641 | 0.0613        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.041  | 0.027  | 0.027  | 0.1154        |   0.0661 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.6185        | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.6308        | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 |   0.5295 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  4 | R         | EN     | Europarl  | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0011        | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 |   0.0018 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0012        | 0.0008 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0016 |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0003        | 0.0009 | 0.0009 | 0.001  | 0.001  | [BOLD] 0.0017 |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  8 | F         | EN     | Europarl  | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0022        | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 |   0.0036 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0024        | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 |   0.0023 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0005        | 0.0018 | 0.0018 | 0.002  | 0.0021 | [BOLD] 0.0034 |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+--------+--------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Our model achieves higher recall@0.2 and better area under the ROC curve.",
        "evidence": "Our model achieves a higher recall@0.2 and better area under the ROC curve than the ablations of -word-att and -capsule.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "coreference is thus a very challenging task with low precision and recall over the entire system",
        "evidence": "The task of coreference is thus much harder given the large difference in precision and recall between the lemma and the disjoint models. The CLUSTER+LEmma baseline achieved a very high F1 score of 67.4% on the CoNLL test set, which is similar to the F1 score of 70% reported by \\newciteDBLP:journals/corr/abs-1812-00158. Our joint model improves upon the baseline slightly even with relatively low precision.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 4 shows that GDPL has the largest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves less like the human.",
        "evidence": "We also compare GDPL to the baselines. GDPL has the largest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves less like the human.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Interestingly, the size and type of whitelist seem to have little effect on performance, indicating that all the whitelists contain responses appropriate to a variety of conversational contexts.",
        "evidence": "It seems that the size and type of whitelists have little effect on performance. Whitelists that contain responses from a variety of contexts contain responses similar to the majority of conversational contexts. Moreover, the size and type of whitelists seem to have little effect on performance.",
        "table": "+----+--------------------+----------------+---------------+--------------+-----------------+\n|    | [BOLD] Whitelist   | [BOLD] Great   | [BOLD] Good   | [BOLD] Bad   | [BOLD] Accept   |\n+====+====================+================+===============+==============+=================+\n|  0 | Freq. 1K           | 54%            | 26%           | 20%          | 80%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  1 | Cluster. 1K        | 55%            | 21%           | 23%          | 77%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  2 | Freq. 10K          | 56%            | 24%           | 21%          | 80%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  3 | Cluster. 10K       | 57%            | 23%           | 20%          | 80%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+\n|  4 | Real response      | 60%            | 24%           | 16%          | 84%             |\n+----+--------------------+----------------+---------------+--------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] It is perceptible that GDPL has better performance than GDPL-sess on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL also outperforms GDPL-discr",
        "evidence": "We compared GDPL with its variants, GDPL-SESS, GDPL-Discr and GDPL. It is clear that GDPL has better performance than GDPL-Sess on the task success and is comparable regarding the dialog turns, while GDPL also outperforms GDPL-Discr. On the task success, GDPL is on par with GDPL-Discr. GDPL also has a similar performance regarding the dialog turns as GDPL-Sess.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "Our vector representation is the state of the art, given a sufficient amount of training time.",
        "evidence": "We evaluated our method on three standard benchmark datasets, including GloVe, word2vec, OIWE-IPG and SOV. For the GloVe vectors, we used 300 dimensions, which is the same as we used for the word embedding. We reported results for the word2vec representation, showing that with enough training time our proposed vector representation outperforms all the other approaches. The results for the word2sense representation are slightly worse, due to the fact that their model is not trained on the same data.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "Third, the learned reward functions based on ROUGE scores worked well in most cases, especially in a direct regression model with CNN-RNN encoder.",
        "evidence": "The table shows that the proposed reward functions based on ROUGE scores worked well in most cases. Especially, MLP with BERT as en(2018) coder outperformed the other embedding based ones (SimRED and MLP) in terms of Reg. Loss, Precision Loss, and Reg. Loss-recall. Since the state-of-the-art model is BERT+MLP, we argue that the proposed reward function is the best fit.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "More than 1000 participants are asked to evaluate 10 random dialog sessions generated by each model.",
        "evidence": "In this section we analyze the claim made by ACER and PPO regarding the quality and success of their dialog policies. More specifically, we choose 1000 randomly generated dialogs from each model and group them into 10 baselines and 1000 participants. We can see that both ACER and PPO significantly outperform ACER in terms of both quality and success. Moreover, PPO is able to achieve higher quality in dialogs even though they are inferior in terms of efficiency.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": ", For Matching Fail and Success, the negative score in other rows implies that the two partitions cannot obtain any reward if the corresponding metric is not satisfied by all sessions in the partition, showing that satisfying Matching Fail, Matching Success, and Success are the most important, followed by Informativeness.",
        "evidence": "S5SS0SSS0Px4 Results Our partition success rate is extremely high, e.g., close to 100%. The other partition, especially the “other” partition, also achieved a very low success rate. This indicates that the matching failure is the most important criterion, followed by informativeness.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our single model DCGCN(single) does not outperform all the single models, as it only achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively.",
        "evidence": "As shown in the table, our model DCGCN(single) does not achieve the best performance on the en-de and encs tasks. We believe that this is because the task of encoder prediction is more difficult than the paraphrase identification task, as more contextual information needs to be captured in the model. The other possible reasons are that (i) the amount of training data is much larger than that of other single models, (ii) the paraphrasing problem is hard but the models in our method do not need much of additional unlabeled data to improve the performance, (iii) the model is capable of using the ensemble information to improve the results of the single model, as it achieves 19.0 and 12.1 BLEU points on en-de and encs tasks, respectively..",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] LRN accelerates the training over LSTM and SRU by about 20%,",
        "evidence": "The claim makes use of the recent strong results by Zhang et al. From the table, we can see that: 1) LRSM consistently outperforms GRU and ATR in terms of averaged parameters with a negligible increase in model parameters. This seems to indicate that the regularization effect introduced by RNN is useful to reducing the training time. 2) Although SRU has comparable performance with LSTM in terms of average parameter size, our gains are about 20% times over LSTM trained with the same seed. We believe this is because the underlying word representations of LSTM and SRU are very similar.",
        "table": "+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|    | Model                 | #Params   | AmaPolar ERR   | AmaPolar Time   | Yahoo ERR    | Yahoo Time   | AmaFull ERR   | AmaFull Time   | YelpPolar ERR   | YelpPolar Time   |\n+====+=======================+===========+================+=================+==============+==============+===============+================+=================+==================+\n|  0 | Zhang et al. ( 2015 ) | -         | 6.10           | -               | 29.16        | -            | 40.57         | -              | 5.26            | -                |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  1 | LSTM                  | 227K      | [BOLD] 4.37    | 0.947           | [BOLD] 24.62 | 1.332        | 37.22         | 1.003          | 3.58            | 1.362            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  2 | GRU                   | 176K      | 4.39           | 0.948           | 24.68        | 1.242        | [BOLD] 37.20  | 0.982          | [BOLD] 3.47     | 1.230            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  3 | ATR                   | 74K       | 4.78           | 0.867           | 25.33        | 1.117        | 38.54         | 0.836          | 4.00            | 1.124            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  4 | SRU                   | 194K      | 4.95           | 0.919           | 24.78        | 1.394        | 38.23         | 0.907          | 3.99            | 1.310            |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+\n|  5 | LRN                   | 151K      | 4.98           | [BOLD] 0.731    | 25.07        | [BOLD] 1.038 | 38.42         | [BOLD] 0.788   | 3.98            | [BOLD] 1.022     |\n+----+-----------------------+-----------+----------------+-----------------+--------------+--------------+---------------+----------------+-----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "for example, DAMD with full supervision achieves the best performance (Combined Score), showing the importance of action supervision.",
        "evidence": "The results demonstrate that our system has much better performance than the baselines, indicating the importance of action supervision. For example, in the case of co-training, our system improves inform and success rates by 5.2% and 2.8% respectively over the strong baseline (Seq2Seq + Attention). The results indicate that our system can generate more diverse belief stateforms, especially for the One-hot action form, which has a larger improvement in success rate and BLEU score. Moreover, multi-action data augmentation further improves the performance, especially for the span action form, which has a 4.5% improvement in success rate and 2.7% improvement in BLEU score.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "For other attributes such as sentiment distribution and sentiment reliability, the F1 metric based on positive sentiment is comparatively low, because instances of neutral sentiment are simply ignored in calculating the F1 score.",
        "evidence": "This claim can only be answered by understanding the deep learning models’ output distributions. The precision, recall and F1 scores of other attributes are much lower than those of positive sentiment, because neutral sentiment is simply ignored in calculating the F1 score.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet does not show much better performance.",
        "evidence": "It can be seen that our model ZSGNet performs much better than the other models on classes that are more common like \"clothing\" and \"bodyparts\".",
        "table": "+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|    | Method               | Overall      | people       | clothing     | bodyparts    | animals      | vehicles     | instruments   | scene        | other        |\n+====+======================+==============+==============+==============+==============+==============+==============+===============+==============+==============+\n|  0 | QRC - VGG(det)       | 60.21        | 75.08        | 55.9         | 20.27        | 73.36        | 68.95        | 45.68         | 65.27        | 38.8         |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  1 | CITE - VGG(det)      | 61.89        | [BOLD] 75.95 | 58.50        | 30.78        | [BOLD] 77.03 | [BOLD] 79.25 | 48.15         | 58.78        | 43.24        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  2 | ZSGNet - VGG (cls)   | 60.12        | 72.52        | 60.57        | 38.51        | 63.61        | 64.47        | 49.59         | 64.66        | 41.09        |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n|  3 | ZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87        | [BOLD] 66.18 | [BOLD] 45.27 | 73.79        | 71.38        | [BOLD] 58.54  | [BOLD] 66.49 | [BOLD] 45.53 |\n+----+----------------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "our extractive summarizer trained with reinforcement learning is rated higher by humans.",
        "evidence": "Table 1 shows the human ratings for the different systems. Our extractive summarizer trained with reinforcement learning is rated higher by humans than the Refresh and ExtAbsRL system.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Similarly, when using discriminative trainthe FINE-TUNED-DISCRIMINATIVE model ing, outperforms the CS-ONLY-DISCRIMINATIVE model.",
        "evidence": "Here, the fine-tuned-discriminative model ing outperforms the comparable CS-only-discriminative model. Fine-tuned-discriminative model also outperforms the Spanish-only-LM and English-only-LMs without using any language model training data.",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%, [CONTINUE] which is far above the UnsupEmb and MFT baselines.",
        "evidence": "We see that the activation representations in the first layer of our seq2seq model improve word-level representations by around 1.5% accuracy, which is much higher than the accuracy of the unsupemb and mft baselines. Our attention model with layer 3 outperforms the unsupemb and mft baselines by around 2.1% accuracy on average, which is much higher than the accuracy of the unsupemb and mft baselines.",
        "table": "+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|    | [ITALIC] k           | Ar                   | Es                   | Fr                   | Ru                   | Zh                   | En                   |\n+====+======================+======================+======================+======================+======================+======================+======================+\n|  0 | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  1 | 0                    | 88.0                 | 87.9                 | 87.9                 | 87.8                 | 87.7                 | 87.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  2 | 1                    | 92.4                 | 91.9                 | 92.1                 | 92.1                 | 91.5                 | 89.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  3 | 2                    | 91.9                 | 91.8                 | 91.8                 | 91.8                 | 91.3                 | 88.3                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  4 | 3                    | 92.0                 | 92.3                 | 92.1                 | 91.6                 | 91.2                 | 87.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  5 | 4                    | 92.1                 | 92.4                 | 92.5                 | 92.0                 | 90.5                 | 86.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  6 | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  7 | 0                    | 81.9                 | 81.9                 | 81.8                 | 81.8                 | 81.8                 | 81.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  8 | 1                    | 87.9                 | 87.7                 | 87.8                 | 87.9                 | 87.7                 | 84.5                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  9 | 2                    | 87.4                 | 87.5                 | 87.4                 | 87.3                 | 87.2                 | 83.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 10 | 3                    | 87.8                 | 87.9                 | 87.9                 | 87.3                 | 87.3                 | 82.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 11 | 4                    | 88.3                 | 88.6                 | 88.4                 | 88.1                 | 87.7                 | 82.1                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 12 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 13 | [EMPTY]              | 32.7                 | 49.1                 | 38.5                 | 34.2                 | 32.1                 | 96.6                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "the mean KL divergence decreases from 2.098 to 0.238 as we apply more model components to the user simulator, where DP-MBCM and GP-MBCM model the human dialog policy using the LSTM-DQN framework and ACER and PPO model the human dialog policy using the Actor-Critic framework, ALDM and GDPL model the human dialog policy using the Actor-Critic framework,",
        "evidence": "We also apply more model components to the user simulator. ACER and PPO are more effective than ALDM and GDPL. GDPL improves the mean KL divergence of the dialog policy from 2.098 to 0.238, which is even larger than the improvement of the human dialog policy using the LSTM-DQN framework.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] It is perceptible that GDPL-sess has better performance than GDPL on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL-discr also outperforms GDPL.",
        "evidence": "We compared GDPL-sess with GDPL on the task success and is comparable regarding the dialog turns. It is perceptible that GDPL-sess has better performance than GDPL on the task success and is comparable regarding the dialog turns. GDPL also outperforms GDPL-discr on the task success and has comparable performance with GDPL on the dialog turns. Although GDPL does not outperform GDPL on task success, it shows that GDPL has better performance on dialog turns.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "For example, the greedy agent says the magic words like “I want to book an experience” at the beginning of the conversation.",
        "evidence": "The results show that GDPL has the best performance in every metric. In particular, GDPL gets better performance than ACER and PPO in every metric, which verifies the effectiveness of our sequence-to-sequence approach for dialog generation.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees, but the throughput of the linear dataset increases more significantly when the batch size increases from 1 to 25.",
        "evidence": "Our implementation is designed to optimize for data throughput, that is, to minimize the number of inputs required for the algorithm before training the feature vectors and then use the learned knowledge to predict the features from the data. As a result, our implementation can train input trees of balanced trees with greater throughput than input data of unbalanced trees. The throughput of the linear dataset is increased more significantly when the batch size increases from 1 to 25..",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "we also did try VADER (NLTK implementation), but we ended up with low performance since VADER is not trained for Spanish language and it is implemented for social media",
        "evidence": "For training, we randomly sample positive and negative tweets from the training set and evaluate on test set. For training, the SVM-w/o neg. baseline achieves 0.72 and F1-score of 0.64 for positive sentiment and 0.83 and 0.72 for negative sentiment, respectively. Our proposed CNN-LSTMOur-neg-Ant improves the baseline F1-score by 0.78% for positive sentiment and F1-score by 0.87% for negative sentiment. This shows that our model works very well for detecting negated tweets.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Specifically, BERT+MLP+Pref does not significantly outperform (p < 0.05) all the other models that do not use BERT+MLP.",
        "evidence": "The table shows that BERT+MLP+Pref does not significantly outperform (p < 0.05) all the other models that do not use BERT+MLP. Specifically, BERT+MLP+Pref improves the Reg. Loss of all the models by at least 1.05% over the SimRED model, which is significantly better than the results of the no-transfer model (p < 0.05). As stated earlier, BERT+MLP is significantly better than all the other models that do not use BERT+MLP, which is a strong statement. Moreover, models that use BERT+MLP generally outperform models that use MLP, i.e., baseline models that do not use BERT+MLP. This shows that BERT is effective in combination with MLP.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] RELIS significantly outperforms the other RL-based systems.",
        "evidence": "The claim makes reference to experiments carried out using retrieval-based techniques including ICSI, PriorSum, TCSum, TCSum− and SRSum. It is not surprising that RELIS is significantly better than all the other RL-based systems. For example, we can see a huge improvement of RELIS>DUC’01 over ICSI> and over PriorSum>; further, RELIS shows good performances on other datasets as well, and it is competitive with the other RL-based systems.",
        "table": "+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|    | [EMPTY]   | DUC’01 <italic>R</italic>1   | DUC’01 <italic>R</italic>2   | DUC’02 <italic>R</italic>1   | DUC’02 <italic>R</italic>2   | DUC’04 <italic>R</italic>1   | DUC’04 <italic>R</italic>2   |\n+====+===========+==============================+==============================+==============================+==============================+==============================+==============================+\n|  0 | ICSI      | 33.31                        | 7.33                         | 35.04                        | 8.51                         | 37.31                        | 9.36                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  1 | PriorSum  | 35.98                        | 7.89                         | 36.63                        | 8.97                         | 38.91                        | 10.07                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  2 | TCSum     | <bold>36.45</bold>           | 7.66                         | 36.90                        | 8.61                         | 38.27                        | 9.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  3 | TCSum−    | 33.45                        | 6.07                         | 34.02                        | 7.39                         | 35.66                        | 8.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  4 | SRSum     | 36.04                        | 8.44                         | <bold>38.93</bold>           | <bold>10.29</bold>           | 39.29                        | 10.70                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  5 | DeepTD    | 28.74                        | 5.95                         | 31.63                        | 7.09                         | 33.57                        | 7.96                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  6 | REAPER    | 32.43                        | 6.84                         | 35.03                        | 8.11                         | 37.22                        | 8.64                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  7 | RELIS     | 34.73                        | <bold>8.66</bold>            | 37.11                        | 9.12                         | <bold>39.34</bold>           | <bold>10.73</bold>           |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+",
        "label": "supports"
    },
    {
        "claim": "We find EWC outperforms the L2 approach",
        "evidence": "Here, we compare our approach (EWC) against the L2 approach. Health and Bio are models trained separately for each problem, with the latter being our re-implementation of the L2 approach. We find that EWC outperforms the L2 approach in all cases. Moreover, models trained through EWC outperform the L2 approach in health, and the no-reg approach in both.",
        "table": "+----+-----------+--------------------------+-----------------+--------------+\n|    |   [EMPTY] | [BOLD] Training scheme   | [BOLD] Health   | [BOLD] Bio   |\n+====+===========+==========================+=================+==============+\n|  0 |         1 | Health                   | [BOLD] 35.9     | 33.1         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  1 |         2 | Bio                      | 29.6            | 36.1         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  2 |         3 | Health and Bio           | 35.8            | 37.2         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  3 |         4 | 1 then Bio, No-reg       | 30.3            | 36.6         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  4 |         5 | 1 then Bio, L2           | 35.1            | 37.3         |\n+----+-----------+--------------------------+-----------------+--------------+\n|  5 |         6 | 1 then Bio, EWC          | 35.2            | [BOLD] 37.8  |\n+----+-----------+--------------------------+-----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] MIL-ND achieves higher precision, recall, and F1 than MIL, [CONTINUE] Using its confidence at test time (τ MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall).",
        "evidence": "We compared our approach against two baselines. The first one is named “Name Matching”, which corresponds to the conventional attentional matching approach. In this table, ‘Mil-ND’ is our baseline model, ‘Model 2’ is our second model, and ‘τ MIL-ND’ refers to our proposed confidence-aware model which uses the cross-entropy loss for task-specific confidence estimation. It can be seen that our confidence-aware model outperforms the baseline significantly. Specifically, in the ‘all’ setting, our model achieves 3.51% improvements in precision over MIL, and 5.95% improvement in recall.",
        "table": "+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|    | System                     | All P        | All R        | All F1             | In  [ITALIC] E+ P   | In  [ITALIC] E+ R   | In  [ITALIC] E+ F1   |\n+====+============================+==============+==============+====================+=====================+=====================+======================+\n|  0 | Name matching              | 15.03        | 15.03        | 15.03              | 29.13               | 29.13               | 29.13                |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  1 | MIL (model 1)              | 35.87        | 35.87        | 35.87 ±0.72        | 69.38               | 69.38               | 69.38 ±1.29          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  2 | MIL-ND (model 2)           | 37.42        | [BOLD] 37.42 | 37.42 ±0.35        | 72.50               | [BOLD] 72.50        | [BOLD] 72.50 ±0.68   |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  3 | [ITALIC] τMIL-ND (model 2) | [BOLD] 38.91 | 36.73        | [BOLD] 37.78 ±0.26 | [BOLD] 73.19        | 71.15               | 72.16 ±0.48          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  4 | Supervised learning        | 42.90        | 42.90        | 42.90 ±0.59        | 83.12               | 83.12               | 83.12 ±1.15          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007).",
        "evidence": "Perhaps the most widely utilized feature in our model is the phrase-based statistical machine translation model based on Moses The results show that our DCGCN model outperforms all the baselines, without using any ensembles. In particular, we outperform the state-of-the-art PBMT system from Beck et al.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "Manual features reduce recall, but do not help the system to improve accuracy and precision.",
        "evidence": "The results show that the manual features significantly help the system to improve accuracy and precision. The best F-Measure achieved using our approach is 69.44%, which is significantly better than the Local baseline using the same number of features.",
        "table": "+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|    | System                | Accuracy         | Precision       | Recall          | F-Measure        |\n+====+=======================+==================+=================+=================+==================+\n|  0 | Local                 | 63.97%           | 64.27%          | 64.50%          | 63.93%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  1 | Manual                | 64.25%           | [BOLD] 70.84%∗∗ | 48.50%          | 57.11%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  2 | Wiki                  | 67.25%           | 66.51%          | 69.50%          | 67.76%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  3 | Local-Manual          | 65.75%           | 67.96%          | 59.50%          | 62.96%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  4 | Wiki-Local            | 67.40%           | 65.54%          | 68.50%          | 66.80%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  5 | Wiki-Manual           | 67.75%           | 70.38%          | 63.00%          | 65.79%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  6 | [ITALIC] Our Approach | [BOLD] 69.25%∗∗∗ | 68.76%          | [BOLD] 70.50%∗∗ | [BOLD] 69.44%∗∗∗ |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "The best results are shown in bold.",
        "evidence": "The claim-premise extracted by CLUSTER is that: (1) Our joint model improves upon the state-of-the-art in all the four metrics. (2) The performance of the disjoint model is similar to that of the unified model, (3) The joint model is better than the disjoint model in all the three metrics. This indicates that the disjoint model captures the inconsistencies between the claim and the ground truth.",
        "table": "+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|    | [BOLD] Model   |    R |   MUC P |   [ITALIC] F1 |   B3 P |   CEAF- [ITALIC] e P | CoNLL  [ITALIC] F1   |\n+====+================+======+=========+===============+========+======================+======================+\n|  0 | Cluster+Lemma  | 70.1 |    83   |          60   |   84.9 |                 52.5 | 67.4                 |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  1 | Disjoint       | 65.3 |    80.8 |          61.6 |   78.2 |                 58.3 | 70                   |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+\n|  2 | Joint          | 65.4 |    80.9 |          63.3 |   76.4 |                 61.3 | [BOLD] 71.2          |\n+----+----------------+------+---------+---------------+--------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "The highest values of precision are achieved by DSim model, and the highest recalls are obtained by HClust and Patt models.",
        "evidence": "This claim is strongly supported by every baseline model. Specifically, we observe that, for English talks, the highest F-score values are obtained by the HCLust model (4.10-5) while for the other baseline models (SLQS, TF, DOCSub and PATTR) they are achieved by the Dsim and PATT models (with 4.10-5 being the maximum). The results are consistent with our hypothesis. In other words, in the resource-constrained setting, the proposed HCLust and PATT models are able to achieve better results than the SLQS and TF models, which is in turn consistent with our hypothesis. Finally, the difference between the F-score of the two baseline models is not significant.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "This suggests that lemma features enhance (cross-document) coreference performance more than simple cluster features.",
        "evidence": "The results show that lemma features enhance (cross-document) coreference performance more than simple cluster features, when used in combination with other features. We observe that lemma features and cluster features improve over the results of the cluster-based method, which indicates that the lemma features and cluster features capture complementary information that is not discovered by the cluster-based method.",
        "table": "+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|    | [BOLD] Model                     | R       | MUC P   | [ITALIC] F1   | B3 P    | CEAF- [ITALIC] e P   | CoNLL  [ITALIC] F1   |\n+====+==================================+=========+=========+===============+=========+======================+======================+\n|  0 | [BOLD] Baselines                 | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  1 | Cluster+Lemma                    | 75.5    | 79.9    | 73.6          | 85      | 71.7                 | 76.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  2 | CV Cybulska and Vossen ( 2015a ) | -       | 75      | 64            | 78      | -                    | 73                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  3 | KCP Kenyon-Dean et al. ( 2018 )  | 71      | 71      | 69            | 67      | 67                   | 69                   |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  4 | Cluster+KCP                      | 77.4    | 79.3    | 71.5          | 87.2    | 66.4                 | 73.6                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  5 | [BOLD] Model Variants            | [EMPTY] | [EMPTY] | [EMPTY]       | [EMPTY] | [EMPTY]              | [EMPTY]              |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  6 | Disjoint                         | 80.3    | 83.6    | 75.9          | 86      | 71.9                 | 78.5                 |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+\n|  7 | Joint                            | 81      | 84.5    | 77.3          | 85.1    | 73.8                 | [BOLD] 79.5          |\n+----+----------------------------------+---------+---------+---------------+---------+----------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the relation identification component yields better performance compared to Rank+ExATT.",
        "evidence": "The relation identification component of our model yields the best results. It is clear from the results that PCNN+ATT (1) and our model achieve better results than Rank+ExAtt..",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "Our agent outperforms the comparison agents with a large margin.",
        "evidence": "Comparison agents. We can see from the table that our agent outperforms the comparison agents by a large margin.. Our agent only needs 60% of the votes for a better result, while the comparison agents need at most 62% for a better result.",
        "table": "+----+----------------+------------------+----------------+-----------------+\n|    | [EMPTY]        | Ours Better(%)   |   No Prefer(%) |   Ours Worse(%) |\n+====+================+==================+================+=================+\n|  0 | Retrieval-Stgy | [BOLD] 62        |             22 |              16 |\n+----+----------------+------------------+----------------+-----------------+\n|  1 | PMI            | [BOLD] 54        |             32 |              14 |\n+----+----------------+------------------+----------------+-----------------+\n|  2 | Neural         | [BOLD] 60        |             22 |              18 |\n+----+----------------+------------------+----------------+-----------------+\n|  3 | Kernel         | [BOLD] 62        |             26 |              12 |\n+----+----------------+------------------+----------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "GDPL-sess and GDPL-discr mark both pretraining strategies, while GDPL marks the ensemble model.",
        "evidence": "We see that GDPL achieves the highest score in Inform F1 and Success, while GDPL-Discr achieves the highest score in Inform F1 and Success. In the ensemble model, GDPL is comparable to GDPL-sess and GDPL-discr in terms of match rate, while GDPL outperforms GDPL-sess by a large margin in terms of inform F1 and success rate.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Dual2seq is not significantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much smaller under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU).",
        "evidence": "We continue to show the result of fine-tuning BERT with the small-scale version of BLEU and the large-scale version of BLEU. First, we can see that Dual2seq is not significantly better than Seq2seq in both settings. In both cases, the improvement is much smaller under the small-scale setting (-3.2 BLEU) than that under the large-scale setting (-1.7 BLEU). And Dual2seq is slightly better than Seq2seq in the full setting too. However, the improvement is not as large as that under the small-scale setting, Dual2seq is at least 5% better than Seq2seq in both settings.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F1 points on events,",
        "evidence": "Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F1 points on events. [CONTINUE] The results of the derivations of our model are consistent with the results of the original paper. Our joint model outperforms all the base [CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F1 points on events,.",
        "table": "+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>                                      | R       | MUC P   | <italic>F</italic>1   | B3 P    | CEAF-<italic>e</italic> P   | CoNLL <italic>F</italic>1   |\n+====+=========================================================+=========+=========+=======================+=========+=============================+=============================+\n|  0 | <bold>Baselines</bold>                                  | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  1 | Cluster+Lemma                                           | 75.5    | 79.9    | 73.6                  | 85      | 71.7                        | 76.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  2 | CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | -       | 75      | 64                    | 78      | -                           | 73                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  3 | KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 71      | 71      | 69                    | 67      | 67                          | 69                          |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  4 | Cluster+KCP                                             | 77.4    | 79.3    | 71.5                  | 87.2    | 66.4                        | 73.6                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  5 | <bold>Model Variants</bold>                             | [EMPTY] | [EMPTY] | [EMPTY]               | [EMPTY] | [EMPTY]                     | [EMPTY]                     |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  6 | Disjoint                                                | 80.3    | 83.6    | 75.9                  | 86      | 71.9                        | 78.5                        |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+\n|  7 | Joint                                                   | 81      | 84.5    | 77.3                  | 85.1    | 73.8                        | <bold>79.5</bold>           |\n+----+---------------------------------------------------------+---------+---------+-----------------------+---------+-----------------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "Our proposed method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set.",
        "evidence": "We compare our proposed method against many state-of-the-art systems. In overall, our proposed method is comparable to GloVe in semantic analogy test set and slightly better in syntactic test set.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "supports"
    },
    {
        "claim": "Without using the dense connections in the last two blocks, the score drops to 23.8.",
        "evidence": "The contribution of the last two blocks is important. Without using the dense connections in the last two blocks, the score drops to 23.8.. These results indicate that the connective is crucial for the model.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "supports"
    },
    {
        "claim": "It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions.",
        "evidence": "It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] It is clear from Table 5 that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings.",
        "evidence": "From the table, we can see that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings..",
        "table": "+----+----------------+-------+-------+-------+-------------+-----------+\n|    | Reward         |   R-1 |   R-2 |   R-L | Human       | Pref%     |\n+====+================+=======+=======+=======+=============+===========+\n|  0 | R-L (original) |  40.9 |  17.8 |  38.5 | 1.75        | 15        |\n+----+----------------+-------+-------+-------+-------------+-----------+\n|  1 | Learned (ours) |  39.2 |  17.4 |  37.5 | [BOLD] 2.20 | [BOLD] 75 |\n+----+----------------+-------+-------+-------+-------------+-----------+",
        "label": "supports"
    },
    {
        "claim": "at a recall of 1, the d=32 setting already achieve a precision of over 0.8 with a significant gap of 0.2 when compared to the best performance of d=8, indicating that at this recall level, the d=32 model seems to be more effective at identifying mappings.",
        "evidence": "We can see that at the recall level, the d=32 model already achieve a precision of over 0.8 with a significant gap of 0.2 when compared to the best performance of d=8. This shows that at this recall level, the d=32 model seems to be more effective at identifying mappings.",
        "table": "+----+---------------+-------+-------+-------+-------+--------+\n|    | Recall        |   0.1 |   0.2 |   0.3 |   AUC | Time   |\n+====+===============+=======+=======+=======+=======+========+\n|  0 | [ITALIC] d=1  | 0.602 | 0.487 | 0.403 | 0.367 | 4h     |\n+----+---------------+-------+-------+-------+-------+--------+\n|  1 | [ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.37  | -      |\n+----+---------------+-------+-------+-------+-------+--------+\n|  2 | [ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h    |\n+----+---------------+-------+-------+-------+-------+--------+\n|  3 | [ITALIC] d=8  | 0.65  | 0.519 | 0.422 | 0.405 | 8h     |\n+----+---------------+-------+-------+-------+-------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "As an explanation for these differences, we believe that the mixtures of different task profiles allowed participants to learn more detailed topic-dependent aspects (better than a single vector model), particularly in relation to the use of language in subject-oriented communication.",
        "evidence": "Therefore, we believe that the mixtures of different task profiles allowed participants to learn more detailed topic-dependent aspects (better than a single vector model) particularly in relation to the use of language in subject-oriented communication. We use the development set for this investigation.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "As expected, the average ranking of samegender pairs is significantly lower than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller.",
        "evidence": "Table As expected, the average ranking of samegender pairs is significantly lower than that of different-gender pairs. In Italian, the difference is much smaller. German, on the other hand, has a much smaller difference between the average ranking of samegender pairs and different-gender pairs.",
        "table": "+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|    | [EMPTY]   | Italian Same-gender   | Italian Diff-Gender   | Italian difference   | German Same-gender   | German Diff-Gender   | German difference   |\n+====+===========+=======================+=======================+======================+======================+======================+=====================+\n|  0 | 7–10      | Og: 4884              | Og: 12947             | Og: 8063             | Og: 5925             | Og: 33604            | Og: 27679           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  1 | 7–10      | Db: 5523              | Db: 7312              | Db: 1789             | Db: 7653             | Db: 26071            | Db: 18418           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  2 | 7–10      | En: 6978              | En: 2467              | En: -4511            | En: 4517             | En: 8666             | En: 4149            |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  3 | 4–7       | Og: 10954             | Og: 15838             | Og: 4884             | Og: 19271            | Og: 27256            | Og: 7985            |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  4 | 4–7       | Db: 12037             | Db: 12564             | Db: 527              | Db: 24845            | Db: 22970            | Db: -1875           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  5 | 4–7       | En: 15891             | En: 17782             | En: 1891             | En: 13282            | En: 17649            | En: 4367            |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  6 | 0–4       | Og: 23314             | Og: 35783             | Og: 12469            | Og: 50983            | Og: 85263            | Og: 34280           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  7 | 0–4       | Db: 26386             | Db: 28067             | Db: 1681             | Db: 60603            | Db: 79081            | Db: 18478           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+\n|  8 | 0–4       | En: 57278             | En: 53053             | En: -4225            | En: 41509            | En: 62929            | En: 21420           |\n+----+-----------+-----------------------+-----------------------+----------------------+----------------------+----------------------+---------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases.",
        "evidence": "As shown in Table 1, the training throughput of the linear dataset is better than the throughput on the balanced dataset. As the batch size increases, the throughput on the balanced dataset also increases, and the overall throughput is much higher than the throughput on the linear dataset.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "supports"
    },
    {
        "claim": "However, training on B-COPA does not necessarily improve performance on the Hard subset, even when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%).",
        "evidence": "However, training on B-COPA does not improve performance on the Hard subset. Crucially, RoBERTa-Large-FT even performs worse than B-Large-FT on the Easy subset. This is surprising, as the original COPA is not more challenging than B-COPA. Overall, we believe that matching the quality of the original COPA dataset, i.e., training on 500 instances in B-COPA, improves model performance on the Hard subset. However, matching the quality of the training data is not sufficient, as the performance of B-COPA 50% is only slightly better than the model trained on the original COPA (500 instances). This suggests that the quality of the baseline model (BERT-Large-FT) is not directly dependent on the quality of the training data.",
        "table": "+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|    | Model            | Training data   | Overall             | Easy                | Hard                |\n+====+==================+=================+=====================+=====================+=====================+\n|  0 | BERT-large-FT    | B-COPA          | 74.5 (± 0.7)        | 74.7 (± 0.4)        | [BOLD] 74.4 (± 0.9) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  1 | BERT-large-FT    | B-COPA (50%)    | 74.3 (± 2.2)        | 76.8 (± 1.9)        | 72.8 (± 3.1)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  2 | BERT-large-FT    | COPA            | [BOLD] 76.5 (± 2.7) | [BOLD] 83.9 (± 4.4) | 71.9 (± 2.5)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  3 | RoBERTa-large-FT | B-COPA          | [BOLD] 89.0 (± 0.3) | 88.9 (± 2.1)        | [BOLD] 89.0 (± 0.8) |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  4 | RoBERTa-large-FT | B-COPA (50%)    | 86.1 (± 2.2)        | 87.4 (± 1.1)        | 85.4 (± 2.9)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+\n|  5 | RoBERTa-large-FT | COPA            | 87.7 (± 0.9)        | [BOLD] 91.6 (± 1.1) | 85.3 (± 2.0)        |\n+----+------------------+-----------------+---------------------+---------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "word vectors generated using our proposed word embedding method using high dimensional, sparse vectors are shown to perform well when used in analogy completion tasks.",
        "evidence": "We present results from several recent works on word embeddings. We report results with GloVe Pennington et al. and word2vec Plank et al. It can be seen from the table that the sparse sparse vectors generated by our method perform significantly better than the word embedding baselines.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "we can find that capsule can provide more quantitative performance for our triple prediction task.",
        "evidence": "From the table, we can find that the capsule can provide more quantitative performance for our triple prediction task.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "for example, DAMD + multi-action data augmentation performs much better than all the other models, suggesting that it is critical to carefully model system actions.",
        "evidence": "The results show that our model DAMD significantly outperforms all the other models, which confirms that it is crucial to carefully model system actions. Moreover, even with single-hot system action forms, our model performs much better than the state-of-the-art HDSA system, which has the advantage of having access to the whole belief state.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In both cases, the new embeddings perform better than the original ones.",
        "evidence": "We claim that embeddings trained in the target language better capture semantic information than those trained in the source language. In both cases, the new embeddings are better than the original ones. In the Italian dataset, the German embeddings perform worse than the original ones. In the English dataset, they perform similarly.",
        "table": "+----+-----------+----------------+------------------+---------------+-----------------+\n|    | [EMPTY]   |   Italian Orig | Italian Debias   |   German Orig | German Debias   |\n+====+===========+================+==================+===============+=================+\n|  0 | SimLex    |          0.28  | [BOLD] 0.288     |         0.343 | [BOLD] 0.356    |\n+----+-----------+----------------+------------------+---------------+-----------------+\n|  1 | WordSim   |          0.548 | [BOLD] 0.577     |         0.547 | [BOLD] 0.553    |\n+----+-----------+----------------+------------------+---------------+-----------------+",
        "label": "supports"
    },
    {
        "claim": "In some cases it seems to make difference in results, e.g., Europarl in Portuguese which increased the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in an increase of f-measure from F=0.5555 in DF to F=0.6403 in TF.",
        "evidence": "This claim does indeed lead to improvements over the baselines, in some cases by a relatively large margin. For example, in Portuguese, the F-measure increased from 0.0034 to 0.0580 and the F-measure increased from 0.0053 to 0.0580 in the EU proceedings. In other languages, the results are similar. In Finnish, the F-measure increased from 0.0033 to 0.0580 and the F-measure increased from 0.0050 to 0.0480 in the EU proceedings. Overall, this claim seems to make little influence on the results, although it is possible that other factors play a role.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Under the same setting, our model does not consistently outperform graph encoders based on recurrent neural networks or gating mechanisms.",
        "evidence": "Unlike typical graph encoders, our models (DCGCN) do not consistently outperform other models based on recurrent neural networks or gating mechanisms. We suspect this may be due to the fact that the DCGCN model achieves best performance on our “easiest to predict” metrics, i.e., #P, CIDEr. This metric is the most difficult to predict as it requires the model to infer the missing dimensions from graph sequences. In contrast, the GNNbased models (such as the GGN2SEQ and the GCN models) can handle all of these metrics without any loss, which makes them attractive for domain adaptation. Although the DCGCN model achieves better performance than the GCNs in terms of BLEU and CIDEr, our model also exhibits greater difficulty in predicting the correct parent node than the GCN models used in the previous work. We suspect that is due to the dependency between the node types and the length of the dependency arc.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "We find that when we train STagBL with only its main task—with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance decreases typically by a few percentage points.",
        "evidence": "In this section, detailed analysis of the effect of STAMBL’s main task (main) is presented. We leave it as a strong baseline to which we can compare other models. Moreover, by only taking into account the last 20% of the data, the performance of the model is very close to the full model.",
        "table": "+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]                                         | C-F1 100%          | C-F1 50%           | R-F1 100%          | R-F1 50%           | F1 100%            | F1 50%             |\n+====+=================================================+====================+====================+====================+====================+====================+====================+\n|  0 | Y-3                                             | 49.59              | 65.37              | 26.28              | 37.00              | 34.35              | 47.25              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  1 | Y-3:Y<italic>C</italic>-1                       | 54.71              | 66.84              | 28.44              | 37.35              | 37.40              | 47.92              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  2 | Y-3:Y<italic>R</italic>-1                       | 51.32              | 66.49              | 26.92              | 37.18              | 35.31              | 47.69              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  3 | Y-3:Y<italic>C</italic>-3                       | <bold>54.58</bold> | 67.66              | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold> |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  4 | Y-3:Y<italic>R</italic>-3                       | 53.31              | 66.71              | 26.65              | 35.86              | 35.53              | 46.64              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  5 | Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95              | <bold>67.84</bold> | 27.90              | 39.71              | 36.54              | 50.09              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  6 | Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55              | 67.60              | 28.30              | 38.26              | 37.26              | 48.86              |\n+----+-------------------------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "At the same time, RELIS performs on par with neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next.",
        "evidence": "For example, we can see that, RelativeIS performs on par with neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next.",
        "table": "+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|    | [EMPTY]   | DUC’01 <italic>R</italic>1   | DUC’01 <italic>R</italic>2   | DUC’02 <italic>R</italic>1   | DUC’02 <italic>R</italic>2   | DUC’04 <italic>R</italic>1   | DUC’04 <italic>R</italic>2   |\n+====+===========+==============================+==============================+==============================+==============================+==============================+==============================+\n|  0 | ICSI      | 33.31                        | 7.33                         | 35.04                        | 8.51                         | 37.31                        | 9.36                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  1 | PriorSum  | 35.98                        | 7.89                         | 36.63                        | 8.97                         | 38.91                        | 10.07                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  2 | TCSum     | <bold>36.45</bold>           | 7.66                         | 36.90                        | 8.61                         | 38.27                        | 9.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  3 | TCSum−    | 33.45                        | 6.07                         | 34.02                        | 7.39                         | 35.66                        | 8.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  4 | SRSum     | 36.04                        | 8.44                         | <bold>38.93</bold>           | <bold>10.29</bold>           | 39.29                        | 10.70                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  5 | DeepTD    | 28.74                        | 5.95                         | 31.63                        | 7.09                         | 33.57                        | 7.96                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  6 | REAPER    | 32.43                        | 6.84                         | 35.03                        | 8.11                         | 37.22                        | 8.64                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  7 | RELIS     | 34.73                        | <bold>8.66</bold>            | 37.11                        | 9.12                         | <bold>39.34</bold>           | <bold>10.73</bold>           |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The single capsule can capture more useful information, while the word-level attention focuses on the entities.",
        "evidence": "This observation shows that the word-level attention focuses on the entities rather than the single capsule, which can capture more useful information. In addition, the AUC score of the -word-att model is worse than that of the -capsule model, which shows that the capsule model focuses on the information that is concrete in the entities rather than the word-level attention.",
        "table": "+----+-----------+-------+-------+-------+-------+\n|    | Recall    |   0.1 |   0.2 |   0.3 |   AUC |\n+====+===========+=======+=======+=======+=======+\n|  0 | -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 |\n+----+-----------+-------+-------+-------+-------+\n|  1 | -Capsule  | 0.635 | 0.507 | 0.413 | 0.386 |\n+----+-----------+-------+-------+-------+-------+\n|  2 | Our Model | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-----------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "OntoLSTM-PP does not outperform HPCD (full), the previous best result on this dataset.",
        "evidence": "OntoLSTM-PP does not outperform the previous best result by HPCD (full) on this dataset. We suspect that this is due to the fact that HPCD and OntoLSTM-PP use different hyper-parameters for word embeddings. WordNet hyper-parameters, which are tuned specifically to this dataset, are not available to OntoLSTM-PP.",
        "table": "+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|    | [BOLD] System   | [BOLD] Initialization   | [BOLD] Embedding   | [BOLD] Resources   | [BOLD] Test Acc.   |\n+====+=================+=========================+====================+====================+====================+\n|  0 | HPCD (full)     | Syntactic-SG            | Type               | WordNet, VerbNet   | 88.7               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  1 | LSTM-PP         | GloVe                   | Type               | -                  | 84.3               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  2 | LSTM-PP         | GloVe-retro             | Type               | WordNet            | 84.8               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  3 | OntoLSTM-PP     | GloVe-extended          | Token              | WordNet            | [BOLD] 89.7        |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP,",
        "evidence": "The claim is that BERT+MLP+Pref significantly outperforms all the other models that do not use BERT+MLP. Specifically, BERT+MLP+Pref improves the Reg. loss by over 8 points (p < 0.05) compared to all the other models. It also improves the Precision/Recall loss by over 3 points (p < 0.05) compared to all the other models that do not use BERT+MLP.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Several groups of words are much more likely to appear in a complaint, although not used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech).",
        "evidence": "The claim cluster prediction results for the LIWC and Word2Vec topics are presented in Table. [CONTINUE] It can be seen that certain groups of words are much more likely to appear in a complaint. [CONTINUE] Complaints tend to not contain personal pronouns (he, she, it, him, you, SHEHE, MALE, FEMALE), as the focus on expressing the complaint is on the self and the party the complaint is addressed to and not other third parties. [CONTINUE] Complaints are not usually accompanied by exclamation marks. [CONTINUE] General topics typical of complaint words include requiring assistance or customer support. Several groups of words tend to appear in a complaint, although not used to express complaints per se, about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech).",
        "table": "+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|    | [BOLD] Complaints  [BOLD] Label   | [BOLD] Complaints  [BOLD] Words                           | [BOLD] Complaints  [ITALIC] r   | [BOLD] Not Complaints  [BOLD] Label   | [BOLD] Not Complaints  [BOLD] Words                        | [BOLD] Not Complaints  [ITALIC] r   |\n+====+===================================+===========================================================+=================================+=======================================+============================================================+=====================================+\n|  0 | [BOLD] LIWC Features              | [BOLD] LIWC Features                                      | [BOLD] LIWC Features            | [BOLD] LIWC Features                  | [BOLD] LIWC Features                                       | [BOLD] LIWC Features                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  1 | NEGATE                            | not, no, can’t, don’t, never, nothing, doesn’t, won’t     | .271                            | POSEMO                                | thanks, love, thank, good, great, support, lol, win        | .185                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  2 | RELATIV                           | in, on, when, at, out, still, now, up, back, new          | .225                            | AFFECT                                | thanks, love, thank, good, great, support, lol             | .111                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  3 | FUNCTION                          | the, i, to, a, my, and, you, for, is, in                  | .204                            | SHEHE                                 | he, his, she, her, him, he’s, himself                      | .105                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  4 | TIME                              | when, still, now, back, new, never, after, then, waiting  | .186                            | MALE                                  | he, his, man, him, sir, he’s, son                          | .086                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  5 | DIFFER                            | not, but, if, or, can’t, really, than, other, haven’t     | .169                            | FEMALE                                | she, her, girl, mom, ma, lady, mother, female, mrs         | .084                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  6 | COGPROC                           | not, but, how, if, all, why, or, any, need                | .132                            | ASSENT                                | yes, ok, awesome, okay, yeah, cool, absolutely, agree      | .080                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  7 | [BOLD] Word2Vec Clusters          | [BOLD] Word2Vec Clusters                                  | [BOLD] Word2Vec Clusters        | [BOLD] Word2Vec Clusters              | [BOLD] Word2Vec Clusters                                   | [BOLD] Word2Vec Clusters            |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  8 | Cust. Service                     | service, customer, contact, job, staff, assist, agent     | .136                            | Gratitude                             | thanks, thank, good, great, support, everyone, huge, proud | .089                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n|  9 | Order                             | order, store, buy, free, delivery, available, package     | .128                            | Family                                | old, friend, family, mom, wife, husband, younger           | .063                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 10 | Issues                            | delayed, closed, between, outage, delay, road, accident   | .122                            | Voting                                | favorite, part, stars, model, vote, models, represent      | .060                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 11 | Time Ref.                         | been, yet, haven’t, long, happened, yesterday, took       | .122                            | Contests                              | Christmas, gift, receive, entered, giveaway, enter, cards  | .058                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 12 | Tech Parts                        | battery, laptop, screen, warranty, desktop, printer       | .100                            | Pets                                  | dogs, cat, dog, pet, shepherd, fluffy, treats              | .054                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+\n| 13 | Access                            | use, using, error, password, access, automatically, reset | .098                            | Christian                             | god, shall, heaven, spirit, lord, belongs, soul, believers | .053                                |\n+----+-----------------------------------+-----------------------------------------------------------+---------------------------------+---------------------------------------+------------------------------------------------------------+-------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Adding the dependency weight factor with a window size of 5 improves [CONTINUE] the F1 score by 3.2% (A3−A2).",
        "evidence": "We can see that adding the dependency weight factor with a window size of 5 improves the F1 score by 3.2% (A3−A2).",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "supports"
    },
    {
        "claim": "GDPL does not outperform three baselines significantly in all aspects (sign test, p-value < 0.01), including the quality compared with ACER.",
        "evidence": "We can see that GDPL does not outperform the three baselines significantly in all aspects. The quality of GDPL is significantly higher than ACER and PPO, mainly because ACER uses data better than PPO. GDPL also outperforms ALDM in three out of four aspects.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "These results show that our model is more effective in terms of using automatically generated AMR graphs.",
        "evidence": "We just perform experiments to show that our model is more powerful than the baselines in term of using automatically generated AMR graphs. In particular, we examine theDCGCN(ensemble) and DCGCN(single) models, which are our single models, and compare their performance against several baselines. It can be seen that using an ensemble of all the predicted AMR graphs, the BLEU score of DCGCN(ensemble) is higher than all the baseline models. For instance, DCGCN(single) achieves 31.6, which is about 2.7 points higher than the best baseline. Although DCGCN(single) only uses 0.2M text snippets, it outperforms other baselines in terms of the BLEU score. This result shows that our model is more powerful in using automatically generated AMR graphs.",
        "table": "+----+------------------------------------+-------------------+-------------+\n|    | [BOLD] Model                       | [BOLD] External   | B           |\n+====+====================================+===================+=============+\n|  0 | Seq2SeqK (Konstas et al.,  2017 )  | -                 | 22.0        |\n+----+------------------------------------+-------------------+-------------+\n|  1 | GraphLSTM (Song et al.,  2018 )    | -                 | 23.3        |\n+----+------------------------------------+-------------------+-------------+\n|  2 | GCNSEQ (Damonte and Cohen,  2019 ) | -                 | 24.4        |\n+----+------------------------------------+-------------------+-------------+\n|  3 | DCGCN(single)                      | -                 | 25.9        |\n+----+------------------------------------+-------------------+-------------+\n|  4 | DCGCN(ensemble)                    | -                 | [BOLD] 28.2 |\n+----+------------------------------------+-------------------+-------------+\n|  5 | TSP (Song et al.,  2016 )          | ALL               | 22.4        |\n+----+------------------------------------+-------------------+-------------+\n|  6 | PBMT (Pourdamghani et al.,  2016 ) | ALL               | 26.9        |\n+----+------------------------------------+-------------------+-------------+\n|  7 | Tree2Str (Flanigan et al.,  2016 ) | ALL               | 23.0        |\n+----+------------------------------------+-------------------+-------------+\n|  8 | SNRG (Song et al.,  2017 )         | ALL               | 25.6        |\n+----+------------------------------------+-------------------+-------------+\n|  9 | Seq2SeqK (Konstas et al.,  2017 )  | 0.2M              | 27.4        |\n+----+------------------------------------+-------------------+-------------+\n| 10 | GraphLSTM (Song et al.,  2018 )    | 0.2M              | 28.2        |\n+----+------------------------------------+-------------------+-------------+\n| 11 | DCGCN(single)                      | 0.1M              | 29.0        |\n+----+------------------------------------+-------------------+-------------+\n| 12 | DCGCN(single)                      | 0.2M              | [BOLD] 31.6 |\n+----+------------------------------------+-------------------+-------------+\n| 13 | Seq2SeqK (Konstas et al.,  2017 )  | 2M                | 32.3        |\n+----+------------------------------------+-------------------+-------------+\n| 14 | GraphLSTM (Song et al.,  2018 )    | 2M                | 33.6        |\n+----+------------------------------------+-------------------+-------------+\n| 15 | Seq2SeqK (Konstas et al.,  2017 )  | 20M               | 33.8        |\n+----+------------------------------------+-------------------+-------------+\n| 16 | DCGCN(single)                      | 0.3M              | 33.2        |\n+----+------------------------------------+-------------------+-------------+\n| 17 | DCGCN(ensemble)                    | 0.3M              | [BOLD] 35.3 |\n+----+------------------------------------+-------------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "the joint training of encoder and question decoder achieves 10.99 EM and 50.10 F1 for QA-SRL and better results for QA-SRL than MQAN.",
        "evidence": "The claim is that our approach achieves 10.99% EM and 50.10 F1 in the in-domain evaluation and significantly better results for QA-SRL than MQAN..",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "as shown in Figure 5(a), recall@100 is increasing at different recall thresholds, the best result is achieved at r=0.3, which is the average number of tags of each training sample",
        "evidence": "As shown in Figure 5(a), the performance of our model increases as we increase the recall threshold r. The best result is achieved at r=0.3. We find that the recall increases up to 0.9 when r=1, which is equivalent to the number of tags in the training sample.",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Similarly, when using discriminative training, the CS-ONLY-DISCRIMINATIVE model outperforms the FINE-TUNED-DISCRIMINATIVE model.",
        "evidence": "Here, the fine-tuned-discriminative model outperforms the S-only-discriminative model by a large margin. We also experiment with the S-only-discriminative model without language model training. With language model training, the performance is similar to the S-only-discriminative model, although the quality is lower. This can be explained by the fact that without the language model, the S-only-discriminative model has very low perplexity.",
        "table": "+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|    | [EMPTY]          | dev perp ↓   | dev acc ↑   | dev wer ↓   | test perp ↓   | test acc ↑   | test wer ↓   |\n+====+==================+==============+=============+=============+===============+==============+==============+\n|  0 | Spanish-only-LM  | 329.68       | 26.6        | 30.47       | 322.26        | 25.1         | 29.62        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  1 | English-only-LM  | 320.92       | 29.3        | 32.02       | 314.04        | 30.3         | 32.51        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  2 | All:CS-last-LM   | 76.64        | 47.8        | 14.56       | 76.97         | 49.2         | 14.13        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  3 | All:Shuffled-LM  | 68.00        | 51.8        | 13.64       | 68.72         | 51.4         | 13.89        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  4 | CS-only-LM       | 43.20        | 60.7        | 12.60       | 43.42         | 57.9         | 12.18        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  5 | CS-only+vocab-LM | 45.61        | 61.0        | 12.56       | 45.79         | 58.8         | 12.49        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  6 | Fine-Tuned-LM    | 39.76        | 66.9        | 10.71       | 40.11         | 65.4         | 10.17        |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  7 | CS-only-disc     | –            | 72.0        | 6.35        | –             | 70.5         | 6.70         |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+\n|  8 | Fine-Tuned-disc  | –            | [BOLD] 74.2 | [BOLD] 5.85 | –             | [BOLD] 75.5  | [BOLD] 5.59  |\n+----+------------------+--------------+-------------+-------------+---------------+--------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment.",
        "evidence": "We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment.",
        "table": "+----+---------------------+--------------------+--------------------+--------------------+\n|    | [EMPTY]             |   <bold>RNN</bold> |   <bold>CNN</bold> | <bold>DAN</bold>   |\n+====+=====================+====================+====================+====================+\n|  0 | Positive            |                9.7 |                4.3 | +<bold>23.6</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  1 | Negative            |                6.9 |                5.5 | +<bold>16.1</bold> |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  2 | Flipped to Positive |               20.2 |               24.9 | +27.4              |\n+----+---------------------+--------------------+--------------------+--------------------+\n|  3 | Flipped to Negative |               31.5 |               28.6 | +19.3              |\n+----+---------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "However, NSP is able to capture “false” causal information because it can match, e.g., the antecedent with but or because, which may help it show an advantage on less challenging examples.",
        "evidence": "The task is to determine whether a model can effectively identify causal information from the claim. We compared our model to three baselines: (1) word frequency, which is the frequency of words in the training data; (2) PMI, which is the performance of a neural model trained on Princeton-CUBoulder-1; and (3) NSP, our proposed model, which uses true causal information source by NSP.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "While the clustering whitelists have higher recall, the frequency whitelists have higher coverage.",
        "evidence": "Table 4 shows the results of applying the clustering and frequency whitelists to generate whitelists. While the clustering whitelists have higher recall, the frequency whitelists have higher coverage. [CONTINUE] The table shows that the clustering whitelists cover more than 45% of the R@1 values, while the frequency whitelists cover over 23% of the R@1 values.",
        "table": "+----+--------------------+--------------+-------------------+\n|    | [BOLD] Whitelist   |   [BOLD] R@1 | [BOLD] Coverage   |\n+====+====================+==============+===================+\n|  0 | Frequency 10K      |        0.136 | 45.04%            |\n+----+--------------------+--------------+-------------------+\n|  1 | Clustering 10K     |        0.164 | 38.38%            |\n+----+--------------------+--------------+-------------------+\n|  2 | Frequency 1K       |        0.273 | 33.38%            |\n+----+--------------------+--------------+-------------------+\n|  3 | Clustering 1K      |        0.331 | 23.28%            |\n+----+--------------------+--------------+-------------------+",
        "label": "supports"
    },
    {
        "claim": "In general, both of our principles can improve all the models in any ablative condition (i.e., P1, P2, P1+P2).",
        "evidence": " ablative statements: In the ablation studies, we report results for the baseline model and our variants (P1, P2, and P1+P2) in three tasks. In general, both of our principles can improve all the models. The improvements brought by the ablations are consistent with the findings in the ablation studies.",
        "table": "+----+----------+--------------+----------+----------+--------+\n|    | Model    | LF           |   HCIAE  |   CoAtt  |   RvA  |\n+====+==========+==============+==========+==========+========+\n|  0 | baseline | 57.21        |    56.98 |    56.46 |  56.74 |\n+----+----------+--------------+----------+----------+--------+\n|  1 | +P1      | 61.88        |    60.12 |    60.27 |  61.02 |\n+----+----------+--------------+----------+----------+--------+\n|  2 | +P2      | 72.65        |    71.5  |    71.41 |  71.44 |\n+----+----------+--------------+----------+----------+--------+\n|  3 | +P1+P2   | [BOLD] 73.63 |    71.99 |    71.87 |  72.88 |\n+----+----------+--------------+----------+----------+--------+",
        "label": "supports"
    },
    {
        "claim": "Instead, we use different combinations of the IWE table.",
        "evidence": "We do not compare the interpretability of our test set with the original IWE table. This is because there is no single correct answer for any of the questions.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Supervising path attentions (the PRKGC+NS model) is indeed effective for improving the human interpretability of generated NLDs.",
        "evidence": "We can see that the PRKGC+NS model is indeed effective for improving the human interpretability of generated NLDs. More importantly, the PRKGC+NS model achieves the highest values for all evaluation metrics.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "Apart from the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are usually associated with large discrepancies in test set performance.",
        "evidence": "The LSTM-800 and the LSTM-400 are very close in performance. This indicates that the data is relatively easy for humans to read. It is also expected that humans are better at playing chess than chess.",
        "table": "+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|    | ID LSTM-800        |   5-fold CV 70.56 | Δ 0.66   | Single model 67.54   | Δ 0.78   | Ensemble 67.65   | Δ 0.30   |\n+====+====================+===================+==========+======================+==========+==================+==========+\n|  0 | LSTM-400           |             70.5  | 0.60     | [BOLD] 67.59         | 0.83     | [BOLD] 68.00     | 0.65     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  1 | IN-TITLE           |             70.11 | 0.21     | [EMPTY]              | [EMPTY]  | 67.52            | 0.17     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  2 | [BOLD] SUBMISSION  |             69.9  | –        | 66.76                | –        | 67.35            | –        |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  3 | NO-HIGHWAY         |             69.72 | −0.18    | 66.42                | −0.34    | 66.64            | −0.71    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  4 | NO-OVERLAPS        |             69.46 | −0.44    | 65.07                | −1.69    | 66.47            | −0.88    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  5 | LSTM-400-DROPOUT   |             69.45 | −0.45    | 65.53                | −1.23    | 67.28            | −0.07    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  6 | NO-TRANSLATIONS    |             69.42 | −0.48    | 65.92                | −0.84    | 67.23            | −0.12    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  7 | NO-ELMO-FINETUNING |             67.71 | −2.19    | 65.16                | −1.60    | 65.42            | −1.93    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "The relative improvement averaged over all tasks is less than 8%.",
        "evidence": "The experimental results are averaged over all tasks. For CBOW and CMOW, the relative improvement is less than 8% when comparing depth 400 and 800. As a comparison, the performance of our hybrid model is more than 13% better than CBOW and better than CMOW/784, which is the best case.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "While the frequency whitelists have higher recall, the clustering whitelists have higher coverage.",
        "evidence": "It can be observed that the clustering-based whitelists have higher R@1 and R@2 values than the frequency-based whitelists. Further, the clustering-based whitelists have higher coverage than the frequency-based whitelists. This can be understood by looking at the table. While the frequency-based whitelists have higher recall, the clustering-based whitelists have higher coverage. In the frequency whitelists, the coverage is 45.04%, while in the clustering-based whitelists, the coverage is 23.28%. This can be understood by looking at the table: while the frequency-based whitelists have higher recall, the clustering-based whitelists have higher coverage.",
        "table": "+----+--------------------+--------------+-------------------+\n|    | [BOLD] Whitelist   |   [BOLD] R@1 | [BOLD] Coverage   |\n+====+====================+==============+===================+\n|  0 | Frequency 10K      |        0.136 | 45.04%            |\n+----+--------------------+--------------+-------------------+\n|  1 | Clustering 10K     |        0.164 | 38.38%            |\n+----+--------------------+--------------+-------------------+\n|  2 | Frequency 1K       |        0.273 | 33.38%            |\n+----+--------------------+--------------+-------------------+\n|  3 | Clustering 1K      |        0.331 | 23.28%            |\n+----+--------------------+--------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "As shown in Table 6, reducing the number of attention heads severely decreases multitasking performance.",
        "evidence": "As shown in Table 6, reducing the number of attention heads severely decreases multitasking performance. For example, when reducing the number of attention heads from 8 to 10, the accuracy reduces from 89.44% to 96.28%. The effect is statistically significant (p-value <0.04). Thus, the number of attention heads has a large impact on multitasking performance.",
        "table": "+----+--------------+------------+-------------+----------+\n|    |   # of Heads | Accuracy   |   Val. Loss | Effect   |\n+====+==============+============+=============+==========+\n|  0 |            1 | 89.44%     |      0.2811 | -6.84%   |\n+----+--------------+------------+-------------+----------+\n|  1 |            2 | 91.20%     |      0.2692 | -5.08%   |\n+----+--------------+------------+-------------+----------+\n|  2 |            4 | 93.85%     |      0.2481 | -2.43%   |\n+----+--------------+------------+-------------+----------+\n|  3 |            8 | 96.02%     |      0.2257 | -0.26%   |\n+----+--------------+------------+-------------+----------+\n|  4 |           10 | 96.28%     |      0.2197 | [EMPTY]  |\n+----+--------------+------------+-------------+----------+\n|  5 |           16 | 96.32%     |      0.219  | +0.04    |\n+----+--------------+------------+-------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP does not outperform the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%.",
        "evidence": "Results. Firstly, our proposed ontology level embedding scheme (OntoLSTM-PP) does not outperform the baseline hierarchical phrase-based LSTM-PP by a large margin. By comparing the results of the original LSTM-PP and OntoLSTM-PP, we can see that the increase in performance comes from the fact that the baseline LSTM-PP uses contextualized word embeddings. This is not true for OntoLSTM-PP which simply uses pre-trained GloVe embeddings. Secondly, OntoLSTM-PP outperforms our baseline LSTM-PP by around 4.9%, or a relative error reduction of 32%.",
        "table": "+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|    | [BOLD] System   | [BOLD] Initialization   | [BOLD] Embedding   | [BOLD] Resources   | [BOLD] Test Acc.   |\n+====+=================+=========================+====================+====================+====================+\n|  0 | HPCD (full)     | Syntactic-SG            | Type               | WordNet, VerbNet   | 88.7               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  1 | LSTM-PP         | GloVe                   | Type               | -                  | 84.3               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  2 | LSTM-PP         | GloVe-retro             | Type               | WordNet            | 84.8               |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+\n|  3 | OntoLSTM-PP     | GloVe-extended          | Token              | WordNet            | [BOLD] 89.7        |\n+----+-----------------+-------------------------+--------------------+--------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "As hard coreference problems are rare in standard coreference datasets, we do not have significant performance improvement.",
        "evidence": "Here we use the standard coreference datasets from the CoNLL-2012 shared task Mikolov et al. These datasets consist of 5 different coreference problems: ACE, bcub, ace, and ontonotes. The results show that hard coreference problems are rare in standard coreference datasets, so we do not have significant performance improvement. However, these results show that we can improve performance on these data by using our debiasing method.",
        "table": "+----+-----------+--------------+--------------+--------------+--------------+\n|    | System    | MUC          | BCUB         | CEAFe        | AVG          |\n+====+===========+==============+==============+==============+==============+\n|  0 | ACE       | ACE          | ACE          | ACE          | ACE          |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  1 | IlliCons  | [BOLD] 78.17 | 81.64        | [BOLD] 78.45 | [BOLD] 79.42 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  2 | KnowComb  | 77.51        | [BOLD] 81.97 | 77.44        | 78.97        |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  3 | OntoNotes | OntoNotes    | OntoNotes    | OntoNotes    | OntoNotes    |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  4 | IlliCons  | 84.10        | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  5 | KnowComb  | [BOLD] 84.33 | 78.02        | 67.95        | 76.76        |\n+----+-----------+--------------+--------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "StateNet PS outperforms StateNet, and StateNet PSI performs best among all 3 models.",
        "evidence": "The results show that StateNet PS outperforms StateNet, and StateNet PSI performs best among all 3 models. StateNet PSI also has the highest joint accuracy among all 3 models.",
        "table": "+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|    | [BOLD] DST Models                                        | [BOLD] Joint Acc. DSTC2   | [BOLD] Joint Acc. WOZ 2.0   |\n+====+==========================================================+===========================+=============================+\n|  0 | Delexicalisation-Based (DB) Model Mrkšić et al. ( 2017 ) | 69.1                      | 70.8                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  1 | DB Model + Semantic Dictionary Mrkšić et al. ( 2017 )    | 72.9                      | 83.7                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  2 | Scalable Multi-domain DST Rastogi et al. ( 2017 )        | 70.3                      | -                           |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  3 | MemN2N Perez and Liu ( 2017 )                            | 74.0                      | -                           |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  4 | PtrNet Xu and Hu ( 2018 )                                | 72.1                      | -                           |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  5 | Neural Belief Tracker: NBT-DNN Mrkšić et al. ( 2017 )    | 72.6                      | 84.4                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  6 | Neural Belief Tracker: NBT-CNN Mrkšić et al. ( 2017 )    | 73.4                      | 84.2                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  7 | Belief Tracking: Bi-LSTM Ramadan et al. ( 2018 )         | -                         | 85.1                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  8 | Belief Tracking: CNN Ramadan et al. ( 2018 )             | -                         | 85.5                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n|  9 | GLAD Zhong et al. ( 2018 )                               | 74.5                      | 88.1                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n| 10 | StateNet                                                 | 74.1                      | 87.8                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n| 11 | StateNet_PS                                              | 74.5                      | 88.2                        |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+\n| 12 | [BOLD] StateNet_PSI                                      | [BOLD] 75.5               | [BOLD] 88.9                 |\n+----+----------------------------------------------------------+---------------------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The results for the Portuguese corpora are quite similar to the ones generated by the English corpora, having terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating deep taxonomies, affirming the characteristics of each method.",
        "evidence": "We observe that the Portuguese corpora are similar to the English ones. For example, in the European and TED talks corpora, word pairs are similar except for NumberRels and DialogueRoles, which are distinct from each other. Another important feature of the Portuguese corpora is the layout of the lexical space. In the TED talks, for example, discourse relations are represented by little more than a few words, while in the Europarl they are much longer. We can observe that the performance of our classification systems is quite competitive with the results reported by other methods. This is expected because our classification systems are able to work with very heterogeneous corpora by taking into account the topic distribution of the training set. Due to the limitation of available corpora, in this section we only report the results of the best performing methods.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 980    | 1,000  | 1,000  | 1,000 | 1,000 | 996      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 79     | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,527  | 1,031  | 1,049  | 1,185 | 1,093 | 1,644    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 19     | 902    | 894    | 784   | 849   | 6        | 10       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 902    | 894    | 784   | 849   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 9.43   | 902    | 894    | 784   | 849   | 2.73     | 4.29     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 2.02   | 1      | 1      | 1     | 1     | 2.19     | 2.33     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 27     | 3      | 3      | 4     | 3     | 201      | 58       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.98   | 1.03   | 1.05   | 1.19  | 1.09  | 6.25     | 2.55     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 296    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 101    | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 291    | 1,045  | 1,229  | 3,637 | 4,284 | 2,875    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 10     | 860    | 727    | 388   | 354   | 252      | 17       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 860    | 727    | 388   | 354   | 249      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 3.94   | 860    | 727    | 388   | 354   | 250.43   | 6.16     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.54   | 1      | 1      | 1     | 1     | 1.01     | 2.76     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 37     | 3      | 79     | 18    | 13    | 9        | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.79   | 1.05   | 1.23   | 3.64  | 4.29  | 2.94     | 2.37     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "supports"
    },
    {
        "claim": "two annotators were used for each dataset.",
        "evidence": "Two annotators were used for each dataset. The quality of the annotations significantly improves on the original COPA dataset.",
        "table": "+----+---------------+------------+-----------------------------+\n|    | Dataset       |   Accuracy |   Fleiss’ kappa  [ITALIC] k |\n+====+===============+============+=============================+\n|  0 | Original COPA |        100 |                       0.973 |\n+----+---------------+------------+-----------------------------+\n|  1 | Balanced COPA |         97 |                       0.798 |\n+----+---------------+------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "As for the micro F1 evaluation metric, our model achieves the highest performance (83.54%) on the FNC-1 testing subset.",
        "evidence": "Here, we use the FNC-1 dataset, which is used to evaluate the performance of different models on the FNC task. In this dataset, there are 51,412 instances that are similar to the claim, and 13,592 instances that are unrelated to the claim. We use the micro F1 score to evaluate the performance of our model. As we can see, our model achieves the best performance on the FNC-1 dataset. Compared with CNN-based sentence embedding, RNN-based sentence embedding and self-attention model, our model achieves 12.53%, 10.43%, 65.43% and 82.43% on the FNC-1 testing subset, respectively. The results demonstrate that our model is better than all these baselines on the FNC-1 dataset.",
        "table": "+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|    | Model                             |   Accuracy (%) agree |   Accuracy (%) disagree |   Accuracy (%) discuss |   Accuracy (%) unrelated | Micro F1(%)   |\n+====+===================================+======================+=========================+========================+==========================+===============+\n|  0 | Average of Word2vec Embedding     |                12.43 |                    1.3  |                  43.32 |                    74.24 | 45.53         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  1 | CNN-based Sentence Embedding      |                24.54 |                    5.06 |                  53.24 |                    79.53 | 81.72         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  2 | RNN-based Sentence Embedding      |                24.42 |                    5.42 |                  69.05 |                    65.34 | 78.70         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  3 | Self-attention Sentence Embedding |                23.53 |                    4.63 |                  63.59 |                    80.34 | 80.11         |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+\n|  4 | Our model                         |                28.53 |                   10.43 |                  65.43 |                    82.43 | [BOLD] 83.54  |\n+----+-----------------------------------+----------------------+-------------------------+------------------------+--------------------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3).",
        "evidence": "First, we observe that autoencoders of en-en produce poor POS and semantic tagging accuracies. We expected this, since the source language of sem tags is English, and the decoder for English encoder-decoders is English. Even though we expected this, the results still suggest that English encoders are not good at POS tagging. In addition, we found that the translation quality of En-en encoder-decoders is worse than that of German autoencoders. However, the English BLEU scores are much better than those of the autoencoders, so we suppose that those translations do not contribute much to the improvement of the POS tagging accuracy.",
        "table": "+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|    | [ITALIC] k           | Ar                   | Es                   | Fr                   | Ru                   | Zh                   | En                   |\n+====+======================+======================+======================+======================+======================+======================+======================+\n|  0 | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  1 | 0                    | 88.0                 | 87.9                 | 87.9                 | 87.8                 | 87.7                 | 87.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  2 | 1                    | 92.4                 | 91.9                 | 92.1                 | 92.1                 | 91.5                 | 89.4                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  3 | 2                    | 91.9                 | 91.8                 | 91.8                 | 91.8                 | 91.3                 | 88.3                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  4 | 3                    | 92.0                 | 92.3                 | 92.1                 | 91.6                 | 91.2                 | 87.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  5 | 4                    | 92.1                 | 92.4                 | 92.5                 | 92.0                 | 90.5                 | 86.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  6 | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  7 | 0                    | 81.9                 | 81.9                 | 81.8                 | 81.8                 | 81.8                 | 81.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  8 | 1                    | 87.9                 | 87.7                 | 87.8                 | 87.9                 | 87.7                 | 84.5                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n|  9 | 2                    | 87.4                 | 87.5                 | 87.4                 | 87.3                 | 87.2                 | 83.2                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 10 | 3                    | 87.8                 | 87.9                 | 87.9                 | 87.3                 | 87.3                 | 82.9                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 11 | 4                    | 88.3                 | 88.6                 | 88.4                 | 88.1                 | 87.7                 | 82.1                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 12 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 | BLEU                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n| 13 | [EMPTY]              | 32.7                 | 49.1                 | 38.5                 | 34.2                 | 32.1                 | 96.6                 |\n+----+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "our learned reward based evaluation of the lead baseline improves ROUGE precision and recall, relative to normal ROUGE.",
        "evidence": "Table. Our learned reward based evaluation of the lead baseline consistently improves ROUGE precision and recall, relative to normal ROUGE..",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "For DAMD, we fix K=50.",
        "evidence": "For DAMD, we fix K=50. For HDSA, we set K=2.5, and for DCD, we set K=3. We can see that HDSA improves the informativeness of the data. This is because data is better for training. Moreover, the dataset is relatively small: training on 100 episodes is ∼1.5 days for HDSA, and ∼40 days for DAMD.",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score.",
        "evidence": "From the table, we can see that: 1) The proposed RNN-based GRU obtains the best performance among all the baseline models. 2) Although the ATR and SRU achieve competitive performance with the base model, LRN outperforms them in terms of both EM and F1 score. This is mainly because of the fact that the base model only uses 650M parameters. In contrast, the proposed RNN has 2.14M parameters, which is about 300M more than the ATR and SRU. 3) Compared to the recurrent neural network (RNN-based GRU), the LSTM and GRU have similar performance. That is because the GRU is also only using 650M parameters. However, our proposed LRN has better performance over the LSTM and GRU. In particular, LRN achieves 76.14% in EM and 83.83% in F1 score, which is much better than the GRU and the SRU.",
        "table": "+----+---------+-----------+---------------------+----------------------------+\n|    | Model   | #Params   | Base                | +Elmo                      |\n+====+=========+===========+=====================+============================+\n|  0 | rnet*   | -         | 71.1/79.5           | -/-                        |\n+----+---------+-----------+---------------------+----------------------------+\n|  1 | LSTM    | 2.67M     | [BOLD] 70.46/78.98  | 75.17/82.79                |\n+----+---------+-----------+---------------------+----------------------------+\n|  2 | GRU     | 2.31M     | 70.41/ [BOLD] 79.15 | 75.81/83.12                |\n+----+---------+-----------+---------------------+----------------------------+\n|  3 | ATR     | 1.59M     | 69.73/78.70         | 75.06/82.76                |\n+----+---------+-----------+---------------------+----------------------------+\n|  4 | SRU     | 2.44M     | 69.27/78.41         | 74.56/82.50                |\n+----+---------+-----------+---------------------+----------------------------+\n|  5 | LRN     | 2.14M     | 70.11/78.83         | [BOLD] 76.14/ [BOLD] 83.83 |\n+----+---------+-----------+---------------------+----------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] the FINE-TUNEDDISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions.",
        "evidence": "The claim that the fine-tuned Disc model is able to prioritize the gold sentence better than all other models is true. Under both conditions, the fine-tuned Disc model outperforms the LM and the Fine-tuned-LM model.",
        "table": "+----+-----------------+--------------+--------------+--------------+--------------+\n|    | [EMPTY]         | dev CS       | dev mono     | test CS      | test mono    |\n+====+=================+==============+==============+==============+==============+\n|  0 | CS-only-LM      | 45.20        | 65.87        | 43.20        | 62.80        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  1 | Fine-Tuned-LM   | 49.60        | 72.67        | 47.60        | 71.33        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  2 | CS-only-disc    | [BOLD] 75.60 | 70.40        | 70.80        | 70.53        |\n+----+-----------------+--------------+--------------+--------------+--------------+\n|  3 | Fine-Tuned-disc | 70.80        | [BOLD] 74.40 | [BOLD] 75.33 | [BOLD] 75.87 |\n+----+-----------------+--------------+--------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "for example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).",
        "evidence": "The claims can be summarized as follows: (a) Wetrain a model on news data and evaluate it on validation data. We achieve the best BLEU score 21.2 compared to the previous state-of-the-art GCN+RC+LA (10). (b) We iterate the iteration of prediction until the performance on the validation data does not increase. We find 1) iterations is better than iteration 1, due to the fact that more iterations provide more training data; (c) iterations 3, 4 and 5 improve the performance.",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "Compared to Zhou et\\xa0al.",
        "evidence": "Our model is able to achieve comparable or better results than the state-of-the-art Zhou et al.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "InferSent-Cosine achieves a stronger agreement with the selection of sentences between human and the metric than BERT-Cosine.",
        "evidence": "A useful measure of quality is reproducibility of the end result—if the provided generations are of similar quality, then one can be sure that the new generations will be as good as the previous generations. To test this, we compare the selection results between Bert-Cosine and InferSent-Cosine with each of the 100 test sets in the Infersent test set. Specifically, we pick the test set which contains the best combination of quality and plausibility scores from the three aspects: 1) BERT-Cosine, 2) BERT-Cosine, and 3) Infersent-Cosine. .329 is the gap between the original BERT and Cosine scores. This shows that our model can perform better than BERT-Cosine in the selection of quality endings. InferSent-Cosine also achieves a better agreement with the selection than BERT-Cosine, which proves that our model can generate better quality endings than BERT-Cosine.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "In Table 5, it can be seen that generative pretraining via language modeling does account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup.",
        "evidence": "It can be seen that generative pretraining via language modeling does account for a considerable amount of performance, constituting 44.32% of the overall performance. It significantly boosts performance in the multitasking setup and constituting 43.93% of the overall performance (a boost of 39.97% in the standard finetuning setup).",
        "table": "+----+--------------+---------------+------------+-------------+-------------+--------------+\n|    | Finetuning   | Pretrained?   | Accuracy   |   Val. Loss | Acc. Inc.   | % of Perf.   |\n+====+==============+===============+============+=============+=============+==============+\n|  0 | Multitasking | No            | 53.61%     |      0.7217 | -           | -            |\n+----+--------------+---------------+------------+-------------+-------------+--------------+\n|  1 | [EMPTY]      | Yes           | 96.28%     |      0.2197 | +42.67%     | 44.32%       |\n+----+--------------+---------------+------------+-------------+-------------+--------------+\n|  2 | Standard     | No            | 51.02%     |      0.7024 | -           | -            |\n+----+--------------+---------------+------------+-------------+-------------+--------------+\n|  3 | [EMPTY]      | Yes           | 90.99%     |      0.1826 | +39.97%     | 43.93%       |\n+----+--------------+---------------+------------+-------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The results reported in Table 7 show that precision on BDI does not increase as a result of the reduced effect of grammatical gender on the embeddings for German and Italian.",
        "evidence": "However, the effect of grammatical gender on the embeddings is much stronger in the Italian case. In this case, precision does not increase as a result of the reduced effect of grammatical gender on the embeddings of the two languages.",
        "table": "+----+-----------+----------------+----------------+---------------+---------------+\n|    | [EMPTY]   | Italian → En   | Italian En →   | German → En   | German En →   |\n+====+===========+================+================+===============+===============+\n|  0 | Orig      | 58.73          | 59.68          | 47.58         | 50.48         |\n+----+-----------+----------------+----------------+---------------+---------------+\n|  1 | Debias    | [BOLD] 60.03   | [BOLD] 60.96   | [BOLD] 47.89  | [BOLD] 51.76  |\n+----+-----------+----------------+----------------+---------------+---------------+",
        "label": "refutes"
    },
    {
        "claim": "The number of examples in our Multi-News dataset is not significantly larger than previous MDS news data.",
        "evidence": "It should be noted that the number of examples in our Multi-News dataset is not significantly larger than previous MDS news data. The highest number of words (document) in our dataset is only 82.73, which is far lower than the 345 words (summary) in the CNNDMM dataset, which has 717,951 words. In addition, the number of sentences (summary) in our dataset is also lower than CNNDMM’s, while the number of words (summaries) is higher than the TAC 2011 dataset, which has 188.43 words.",
        "table": "+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|    | [BOLD] Dataset   | [BOLD] # pairs        | [BOLD] # words (doc)   |   [BOLD] # sents (docs) |   [BOLD] # words (summary) |   [BOLD] # sents (summary) | [BOLD] vocab size   |\n+====+==================+=======================+========================+=========================+============================+============================+=====================+\n|  0 | Multi-News       | 44,972/5,622/5,622    | 2,103.49               |                   82.73 |                     263.66 |                       9.97 | 666,515             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  1 | DUC03+04         | 320                   | 4,636.24               |                  173.15 |                     109.58 |                       2.88 | 19,734              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  2 | TAC 2011         | 176                   | 4,695.70               |                  188.43 |                      99.7  |                       1    | 24,672              |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+\n|  3 | CNNDM            | 287,227/13,368/11,490 | 810.57                 |                   39.78 |                      56.2  |                       3.68 | 717,951             |\n+----+------------------+-----------------------+------------------------+-------------------------+----------------------------+----------------------------+---------------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] TRANSFORMER-MULTI is weaker than TRANSFORMER-SINGLE [CONTINUE] .2% overall decrease in performance compared to TRANSFORMER-SINGLE for the goldtwo-mention task.",
        "evidence": "This claim does not support the claim that Transformer-Multi performs better than Transformer-Single. In general, we observe that the performance of Transformer-Multi is not as good as that of Transformer-Single for all the three metrics.",
        "table": "+----+--------------------+-------------+-------------+-----------------------+-------------+\n|    | [EMPTY]            | M           | F           | B                     | O           |\n+====+====================+=============+=============+=======================+=============+\n|  0 | Random             | 43.6        | 39.3        | [ITALIC] 0.90         | 41.5        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  1 | Token Distance     | 50.1        | 42.4        | [ITALIC] 0.85         | 46.4        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  2 | Topical Entity     | 51.5        | 43.7        | [ITALIC] 0.85         | 47.7        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  3 | Syntactic Distance | 63.0        | 56.2        | [ITALIC] 0.89         | 59.7        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  4 | Parallelism        | [BOLD] 67.1 | [BOLD] 63.1 | [ITALIC]  [BOLD] 0.94 | [BOLD] 65.2 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  5 | Parallelism+URL    | [BOLD] 71.1 | [BOLD] 66.9 | [ITALIC]  [BOLD] 0.94 | [BOLD] 69.0 |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  6 | Transformer-Single | 58.6        | 51.2        | [ITALIC] 0.87         | 55.0        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+\n|  7 | Transformer-Multi  | 59.3        | 52.9        | [ITALIC] 0.89         | 56.2        |\n+----+--------------------+-------------+-------------+-----------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "the results show that GDPL, the proposed method, improves the task-completion rate by 27.6% over the state-of-the-art baseline and is 2.43 times closer to the upper bound on this measure as well as 3.22 times closer to the upper bound on the success rate measure over 10 random seeds.",
        "evidence": "The results show that GDPL, our proposed method, improves the task-completion rate by 27.6% over the state-of-the-art baseline. It is also 2.43 times closer to the upper bound on this measure as well as 3.22 times closer to the upper bound on the success rate over 10 random seeds.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] As we can observe in Table 3, Patt has the best values of precision for the English corpora while DocSub has the best values for the Portuguese corpora.",
        "evidence": "This claim is very strong only for the English corpus, which is explained by the fact that French and Portuguese are two different languages. Also, as we can see, DocSub has the best values for the English corpora, whereas HLUST has the best values for the Portuguese ones. We notice that for almost all languages, DocSub has the best values regarding the classification accuracies, which is of great importance for low resource languages, where data is scarce.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "In the natural state space with 75 actions, training does not converge within a reasonable training time.",
        "evidence": "The results of ACER, PPO and ALDM are very close to each others. GDPL is slightly better than the other three methods, but this is not a matter of simple arithmetic. In the natural state space, HDL does not converge within a reasonable training time.",
        "table": "+----+----------+--------------+---------------+--------------+----------------+\n|    | Method   | VHUS Turns   | VHUS Inform   | VHUS Match   | VHUS Success   |\n+====+==========+==============+===============+==============+================+\n|  0 | ACER     | 22.35        | 55.13         | 33.08        | 18.6           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  1 | PPO      | [BOLD] 19.23 | [BOLD] 56.31  | 33.08        | 18.3           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  2 | ALDM     | 26.90        | 54.37         | 24.15        | 16.4           |\n+----+----------+--------------+---------------+--------------+----------------+\n|  3 | GDPL     | 22.43        | 52.58         | [BOLD] 36.21 | [BOLD] 19.7    |\n+----+----------+--------------+---------------+--------------+----------------+",
        "label": "not enough info"
    },
    {
        "claim": "In total, 739 tweets (37.6%) are complaints and 1,232 are not complaints (62.4%).",
        "evidence": "In total, 1,232 tweets (62.4%) are complaints and the rest are not complaints. The categories that have a high number of complaints are ‘Food & Beverage’, ‘Retail’, ‘Cars’, ‘Software & Online Services’ and ‘Other’.",
        "table": "+----+----------------------------+---------------------+-------------------------+\n|    | [BOLD] Category            |   [BOLD] Complaints |   [BOLD] Not Complaints |\n+====+============================+=====================+=========================+\n|  0 | Food & Beverage            |                  95 |                      35 |\n+----+----------------------------+---------------------+-------------------------+\n|  1 | Apparel                    |                 141 |                     117 |\n+----+----------------------------+---------------------+-------------------------+\n|  2 | Retail                     |                 124 |                      75 |\n+----+----------------------------+---------------------+-------------------------+\n|  3 | Cars                       |                  67 |                      25 |\n+----+----------------------------+---------------------+-------------------------+\n|  4 | Services                   |                 207 |                     130 |\n+----+----------------------------+---------------------+-------------------------+\n|  5 | Software & Online Services |                 189 |                     103 |\n+----+----------------------------+---------------------+-------------------------+\n|  6 | Transport                  |                 139 |                     109 |\n+----+----------------------------+---------------------+-------------------------+\n|  7 | Electronics                |                 174 |                     112 |\n+----+----------------------------+---------------------+-------------------------+\n|  8 | Other                      |                  96 |                      33 |\n+----+----------------------------+---------------------+-------------------------+\n|  9 | Total                      |                1232 |                     739 |\n+----+----------------------------+---------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "In German, we get a reduction of less than 100%.",
        "evidence": "In Italian, the difference is only very small. In German, we get a reduction of 91.67% of the original sentence. In English, the reduction is around 100%.",
        "table": "+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|    | [EMPTY]          |   Italian Original |   Italian Debiased |   Italian English | Italian Reduction   |   German Original |   German Debiased |   German English | German Reduction   |\n+====+==================+====================+====================+===================+=====================+===================+===================+==================+====================+\n|  0 | Same Gender      |              0.442 |              0.434 |             0.424 | –                   |             0.491 |             0.478 |            0.446 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  1 | Different Gender |              0.385 |              0.421 |             0.415 | –                   |             0.415 |             0.435 |            0.403 | –                  |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+\n|  2 | difference       |              0.057 |              0.013 |             0.009 | [BOLD] 91.67%       |             0.076 |             0.043 |            0.043 | [BOLD] 100%        |\n+----+------------------+--------------------+--------------------+-------------------+---------------------+-------------------+-------------------+------------------+--------------------+",
        "label": "refutes"
    },
    {
        "claim": "Our NeuralTD system is trained only with a simple learning signal that was automatically induced from human ratings, and outperforms the advanced models with access to the gold labels.",
        "evidence": "The results show that NeuralTD, armed with only a simple learning signal, outperforms all advanced models with access to the gold labels in all the metrics. Specifically, we outperform the state-of-the-art models by, on average, 2.2 ROUGE points, 1.3 ROUGE points and 2.1 ROUGE points in the human evaluation.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "The reason may be that a large neural network  (BERT) with its accompanying large input space allows the network to learn a meaningful reward function with greater scope, while the shallower network used in both SimRed and PMeans-RNN may not be adequate for training the same type of reward.",
        "evidence": "MLP’s performance is always better than the other models. Particularly, BERT (pmeans-RNN) is always better than the shallower (simred) baseline. The reason may be that BERT with its accompanying large input space allows the network to learn a meaningful reward function with greater scope, while the shallower network used in both SIMRED and PMEANS-RNN may not be adequate for training the same type of reward.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "To further explore the limitations of DAMD, we focus on the 10-Action Generation task.",
        "evidence": "The results show only limited improvement for the single-action baseline. [CONTINUE] HDSA has the worse performance, which is consistent with the results of [CONTINUE] our human evaluation.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1.",
        "evidence": "Because all the test data points are valid for the 'in E+' setting, we expect the Nd classifier to have a slight negative effect on F1.. Overall, our τMIL-ND has the best performance among all the approaches.",
        "table": "+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|    | System                     | All P        | All R        | All F1             | In  [ITALIC] E+ P   | In  [ITALIC] E+ R   | In  [ITALIC] E+ F1   |\n+====+============================+==============+==============+====================+=====================+=====================+======================+\n|  0 | Name matching              | 15.03        | 15.03        | 15.03              | 29.13               | 29.13               | 29.13                |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  1 | MIL (model 1)              | 35.87        | 35.87        | 35.87 ±0.72        | 69.38               | 69.38               | 69.38 ±1.29          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  2 | MIL-ND (model 2)           | 37.42        | [BOLD] 37.42 | 37.42 ±0.35        | 72.50               | [BOLD] 72.50        | [BOLD] 72.50 ±0.68   |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  3 | [ITALIC] τMIL-ND (model 2) | [BOLD] 38.91 | 36.73        | [BOLD] 37.78 ±0.26 | [BOLD] 73.19        | 71.15               | 72.16 ±0.48          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  4 | Supervised learning        | 42.90        | 42.90        | 42.90 ±0.59        | 83.12               | 83.12               | 83.12 ±1.15          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "The human evaluation shows that our mirrored instances are comparable in difficulty to the original ones (see Table 3).",
        "evidence": "The human evaluation shows that our mirrored instances are comparable in difficulty to the original ones.",
        "table": "+----+---------------+------------+-----------------------------+\n|    | Dataset       |   Accuracy |   Fleiss’ kappa  [ITALIC] k |\n+====+===============+============+=============================+\n|  0 | Original COPA |        100 |                       0.973 |\n+----+---------------+------------+-----------------------------+\n|  1 | Balanced COPA |         97 |                       0.798 |\n+----+---------------+------------+-----------------------------+",
        "label": "supports"
    },
    {
        "claim": "we see significant improvements in each of the five cases.",
        "evidence": "We evaluate the model’s performance in each of the five cases, by training on the in-domain SQuAD dataset and evaluating on the out-of-domain QA-SRL dataset. The model trained on the in-domain data significantly outperforms all other models. The model trained on the out-of-domain data performs on par with the model trained on in-domain data.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons.",
        "evidence": "For these comparisons, we tune the hyperparameters using the development set. We also show the results of the system trained on CoNLL08 and UD v1.3 for the comparison. For the activation functions, we used a value of 1.05e−01 for the Sigmoid activation function and 0.13 for the softplus one. We found that the Sigmoid was slightly larger than the Softplus one, which led to the model not achieving the best performance. Regarding the feature maps, we found that the best values for the feature map size were 4 to 5 for each representation, while selecting a learning rate of 1 was better than 0.87. The dropout rate and the dropout probability were also chosen in the context of the best F1-score predictions.",
        "table": "+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|    | [BOLD] Representation   | [BOLD] Hyper parameters Filter size   |   [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func.   |   [BOLD] Hyper parameters L2 Reg. |   [BOLD] Hyper parameters Learning rate |   [BOLD] Hyper parameters Dropout Prob. |   [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values   |\n+====+=========================+=======================================+=============================================+============================================+===================================+=========================================+=========================================+==================================================+==================================================+\n|  0 | CoNLL08                 | 4-5                                   |                                        1000 | Softplus                                   |                           11.5    |                                0.00113  |                                    1    |                                            73.34 | 74.49                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  1 | SB                      | 4-5                                   |                                         806 | Sigmoid                                    |                            0.0813 |                                0.00179  |                                    0.87 |                                            72.83 | [BOLD] 75.05                                     |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|  2 | UD v1.3                 | 5                                     |                                         716 | Softplus                                   |                            1.66   |                                0.000963 |                                    1    |                                            68.93 | 69.57                                            |\n+----+-------------------------+---------------------------------------+---------------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+-----------------------------------------+--------------------------------------------------+--------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "The models have better results when handling sentences with 20 or fewer tokens.",
        "evidence": "First, the models with GGNN or GAT as the out-of-pooling methods always perform better than the vanilla S2S models. The margin is especially larger for smaller graph sizes. For example, the S2S obtains 0-20 accuracy improvement when the graph diameter is 0-7, but it improves 35.2% when the diameter increases to 7-13. Also, the G2S models, especially G2S-GAT, is always better than the S2S when the graph diameter is 0-3, but they do not outperform it when the diameter increases to 4-8. The results show that the GGNN or GAT models can handle short sentences (i.e. sentences of less than 20 tokens) better than the vanilla models. Moreover, the G2S models with GGNN or GAT achieve the best performance when the graph diameter is between 0-3 and 7-10, which is also the point where the maximum out-of-pooling layer helps the most.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "CorefProp does not improve relation extraction on SciERC.",
        "evidence": "CorefProp also improves relation extraction on SciERC. However, RelProp is better than CorefProp only on WLPC.",
        "table": "+----+---------------+-------------+-------------+-------------+\n|    | [EMPTY]       | ACE05       | SciERC      | WLPC        |\n+====+===============+=============+=============+=============+\n|  0 | BERT + LSTM   | 60.6        | 40.3        | 65.1        |\n+----+---------------+-------------+-------------+-------------+\n|  1 | +RelProp      | 61.9        | 41.1        | 65.3        |\n+----+---------------+-------------+-------------+-------------+\n|  2 | +CorefProp    | 59.7        | 42.6        | -           |\n+----+---------------+-------------+-------------+-------------+\n|  3 | BERT FineTune | [BOLD] 62.1 | 44.3        | 65.4        |\n+----+---------------+-------------+-------------+-------------+\n|  4 | +RelProp      | 62.0        | 43.0        | [BOLD] 65.5 |\n+----+---------------+-------------+-------------+-------------+\n|  5 | +CorefProp    | 60.0        | [BOLD] 45.3 | -           |\n+----+---------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Perhaps the most striking thing about the ablation results is that the 'traditional' LSTM layout outsperformed the 'alternating' one we chose for our submission.",
        "evidence": "Perhaps the most striking thing about the ablation results was that the 'traditional' LSTM layout outperformed the 'alternating' one we chose for our submission. That is, the 'traditional' LSTM layout, which we considered to have the best performance, was slightly worse than the 'alternating' one we chose for our submission. Although this may not make sense from a statistical perspective, looking at these results does not support the idea that the 'traditional' LSTM layout is always better than the 'alternating' one. It may be that the 'traditional' LSTM doesn’t work well at all due to the two settings we considered.",
        "table": "+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|    | ID LSTM-800        |   5-fold CV 70.56 | Δ 0.66   | Single model 67.54   | Δ 0.78   | Ensemble 67.65   | Δ 0.30   |\n+====+====================+===================+==========+======================+==========+==================+==========+\n|  0 | LSTM-400           |             70.5  | 0.60     | [BOLD] 67.59         | 0.83     | [BOLD] 68.00     | 0.65     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  1 | IN-TITLE           |             70.11 | 0.21     | [EMPTY]              | [EMPTY]  | 67.52            | 0.17     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  2 | [BOLD] SUBMISSION  |             69.9  | –        | 66.76                | –        | 67.35            | –        |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  3 | NO-HIGHWAY         |             69.72 | −0.18    | 66.42                | −0.34    | 66.64            | −0.71    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  4 | NO-OVERLAPS        |             69.46 | −0.44    | 65.07                | −1.69    | 66.47            | −0.88    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  5 | LSTM-400-DROPOUT   |             69.45 | −0.45    | 65.53                | −1.23    | 67.28            | −0.07    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  6 | NO-TRANSLATIONS    |             69.42 | −0.48    | 65.92                | −0.84    | 67.23            | −0.12    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  7 | NO-ELMO-FINETUNING |             67.71 | −2.19    | 65.16                | −1.60    | 65.42            | −1.93    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+",
        "label": "supports"
    },
    {
        "claim": "The semantic threshold for OD-d2v is set at 0.6 while for OD-w2v is set at 0.3.",
        "evidence": "This shows that the semantic threshold for OD-D2V is lower than that for OD-W2V. This is expected as the semantic threshold for OD-D2V can be interpreted as “too high” when the threshold for OOD-W2V is set to 0.3.. Doc2V performs better than TF-IDF for all topics. This is expected as Doc2Vec is trained on Wikipedia and news articles, whereas TF-IDF is trained on similar data from news articles.",
        "table": "+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|    | Topic Name               |   Size | TF-IDF ARI    | WMD ARI       | Sent2vec ARI   | Doc2vec ARI   | BERT ARI    | [ITALIC] OD-w2v ARI   | [ITALIC] OD-d2v ARI   |   TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil.   | Sent2vec  [ITALIC] Sil.   |   Doc2vec  [ITALIC] Sil. |   BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil.   | [ITALIC] OD-d2v  [ITALIC] Sil.   |\n+====+==========================+========+===============+===============+================+===============+=============+=======================+=======================+=========================+======================+===========================+==========================+=======================+==================================+==================================+\n|  0 | Affirmative Action       |  81    | -0.07         | -0.02         | 0.03           | -0.01         | -0.02       | [BOLD] 0.14           | [ITALIC] 0.02         |                    0.01 | 0.01                 | -0.01                     |                    -0.02 |                 -0.04 | [BOLD] 0.06                      | [ITALIC] 0.01                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  1 | Atheism                  | 116    | [BOLD] 0.19   | 0.07          | 0.00           | 0.03          | -0.01       | 0.11                  | [ITALIC] 0.16         |                    0.02 | 0.01                 | 0.02                      |                     0.01 |                  0.01 | [ITALIC] 0.05                    | [BOLD] 0.07                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  2 | Austerity Measures       |  20    | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01          | -0.05         | 0.04        | [BOLD] 0.21           | -0.01                 |                    0.06 | 0.07                 | 0.05                      |                    -0.03 |                  0.1  | [BOLD] 0.19                      | 0.1                              |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  3 | Democratization          |  76    | 0.02          | -0.01         | 0.00           | [ITALIC] 0.09 | -0.01       | [BOLD] 0.11           | 0.07                  |                    0.01 | 0.01                 | 0.02                      |                     0.02 |                  0.03 | [BOLD] 0.16                      | [ITALIC] 0.11                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  4 | Education Voucher Scheme |  30    | [BOLD] 0.25   | 0.12          | 0.08           | -0.02         | 0.04        | 0.13                  | [ITALIC] 0.19         |                    0.01 | 0.01                 | 0.01                      |                    -0.01 |                  0.02 | [ITALIC] 0.38                    | [BOLD] 0.40                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  5 | Gambling                 |  60    | -0.06         | -0.01         | -0.02          | 0.04          | 0.09        | [ITALIC] 0.35         | [BOLD] 0.39           |                    0.01 | 0.02                 | 0.03                      |                     0.01 |                  0.09 | [BOLD] 0.30                      | [ITALIC] 0.22                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  6 | Housing                  |  30    | 0.01          | -0.01         | -0.01          | -0.02         | 0.08        | [BOLD] 0.27           | 0.01                  |                    0.02 | 0.03                 | 0.03                      |                     0.01 |                  0.11 | [BOLD] 0.13                      | [ITALIC] 0.13                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  7 | Hydroelectric Dams       | 110    | [BOLD] 0.47   | [ITALIC] 0.45 | [ITALIC] 0.45  | -0.01         | 0.38        | 0.35                  | 0.14                  |                    0.04 | 0.08                 | 0.12                      |                     0.01 |                  0.19 | [BOLD] 0.26                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  8 | Intellectual Property    |  66    | 0.01          | 0.01          | 0.00           | 0.03          | 0.03        | [ITALIC] 0.05         | [BOLD] 0.14           |                    0.01 | [ITALIC] 0.04        | 0.03                      |                     0.01 |                  0.03 | [ITALIC] 0.04                    | [BOLD] 0.12                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n|  9 | Keystone pipeline        |  18    | 0.01          | 0.01          | 0.00           | -0.13         | [BOLD] 0.07 | -0.01                 | [BOLD] 0.07           |                   -0.01 | -0.03                | -0.03                     |                    -0.07 |                  0.03 | [BOLD] 0.05                      | [ITALIC] 0.02                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 10 | Monarchy                 |  61    | -0.04         | 0.01          | 0.00           | 0.03          | -0.02       | [BOLD] 0.15           | [BOLD] 0.15           |                    0.01 | 0.02                 | 0.02                      |                     0.01 |                  0.01 | [BOLD] 0.11                      | [ITALIC] 0.09                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 11 | National Service         |  33    | 0.14          | -0.03         | -0.01          | 0.02          | 0.01        | [ITALIC] 0.31         | [BOLD] 0.39           |                    0.02 | 0.04                 | 0.02                      |                     0.01 |                  0.02 | [BOLD] 0.25                      | [BOLD] 0.25                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 12 | One-child policy China   |  67    | -0.05         | 0.01          | [BOLD] 0.11    | -0.02         | 0.02        | [BOLD] 0.11           | 0.01                  |                    0.01 | 0.02                 | [ITALIC] 0.04             |                    -0.01 |                  0.03 | [BOLD] 0.07                      | -0.02                            |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 13 | Open-source Software     |  48    | -0.02         | -0.01         | [ITALIC] 0.05  | 0.01          | 0.12        | [BOLD] 0.09           | -0.02                 |                    0.01 | -0.01                | 0.00                      |                    -0.02 |                  0.03 | [BOLD] 0.18                      | 0.01                             |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 14 | Pornography              |  52    | -0.02         | 0.01          | 0.01           | -0.02         | -0.01       | [BOLD] 0.41           | [BOLD] 0.41           |                    0.01 | 0.01                 | 0.02                      |                    -0.01 |                  0.03 | [BOLD] 0.47                      | [ITALIC] 0.41                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 15 | Seanad Abolition         |  25    | 0.23          | 0.09          | -0.01          | -0.01         | 0.03        | [ITALIC] 0.32         | [BOLD] 0.54           |                    0.02 | 0.01                 | -0.01                     |                    -0.03 |                 -0.04 | [ITALIC] 0.15                    | [BOLD] 0.31                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 16 | Trades Unions            |  19    | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60    | -0.05         | 0.44        | [ITALIC] 0.44         | 0.29                  |                    0.1  | 0.17                 | 0.21                      |                     0.01 |                  0.26 | [BOLD] 0.48                      | [ITALIC] 0.32                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 17 | Video Games              |  72    | -0.01         | 0.01          | 0.12           | 0.01          | 0.08        | [ITALIC] 0.40         | [BOLD] 0.56           |                    0.01 | 0.01                 | 0.06                      |                     0.01 |                  0.05 | [ITALIC] 0.32                    | [BOLD] 0.42                      |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+\n| 18 | Average                  |  54.67 | 0.09          | 0.07          | 0.08           | 0.01          | 0.08        | [BOLD] 0.22           | [ITALIC] 0.20         |                    0.02 | 0.03                 | 0.04                      |                    -0.01 |                  0.05 | [BOLD] 0.20                      | [ITALIC] 0.17                    |\n+----+--------------------------+--------+---------------+---------------+----------------+---------------+-------------+-----------------------+-----------------------+-------------------------+----------------------+---------------------------+--------------------------+-----------------------+----------------------------------+----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The proposed approach is seen to perform well against the other unsupervised models.",
        "evidence": "The results show that the proposed approach performs better than the other unsupervised models except Word2Vec on Paracrawl-353-all. It is on par with the best performance of Word2Sense on WordSim-353-sim, which is also the original Word2Vec implementation. The other unsupervised models that are presented in the table are Word2Gram, OIWE-IPG, SOV, and Word2Sense, whose performance is better than the proposed one only on two datasets: RG-65 and MC-30.",
        "table": "+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|    | Dataset (EN-)   |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=================+=========+============+============+=======+=========+==============+============+\n|  0 | WS-353-ALL      |   0.612 |     0.7156 |      0.634 | 0.622 |   0.173 |        0.69  |      0.657 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  1 | SIMLEX-999      |   0.359 |     0.3939 |      0.295 | 0.355 |   0.09  |        0.38  |      0.381 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  2 | VERB-143        |   0.326 |     0.443  |      0.255 | 0.271 |   0.293 |        0.271 |      0.348 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  3 | SimVerb-3500    |   0.193 |     0.2856 |      0.184 | 0.197 |   0.035 |        0.234 |      0.245 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  4 | WS-353-REL      |   0.578 |     0.6457 |      0.595 | 0.578 |   0.134 |        0.695 |      0.619 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  5 | RW-STANF.       |   0.378 |     0.4858 |      0.316 | 0.373 |   0.122 |        0.39  |      0.382 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  6 | YP-130          |   0.524 |     0.5211 |      0.353 | 0.482 |   0.169 |        0.42  |      0.589 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  7 | MEN-TR-3k       |   0.71  |     0.7528 |      0.684 | 0.696 |   0.298 |        0.769 |      0.725 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  8 | RG-65           |   0.768 |     0.8051 |      0.736 | 0.732 |   0.338 |        0.761 |      0.774 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n|  9 | MTurk-771       |   0.65  |     0.6712 |      0.593 | 0.623 |   0.199 |        0.665 |      0.671 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 10 | WS-353-SIM      |   0.682 |     0.7883 |      0.713 | 0.702 |   0.22  |        0.72  |      0.72  |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 11 | MC-30           |   0.749 |     0.8112 |      0.799 | 0.726 |   0.33  |        0.735 |      0.776 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 12 | MTurk-287       |   0.649 |     0.6645 |      0.591 | 0.631 |   0.295 |        0.674 |      0.634 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+\n| 13 | Average         |   0.552 |     0.6141 |      0.519 | 0.538 |   0.207 |        0.57  |      0.579 |\n+----+-----------------+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "Word embeddings derived from GloVe outperform Wiki-PubMed-PMC-based embeddings (Table 1).",
        "evidence": "Word embeddings derived from GloVe outperform Wiki-PubMed-PMC-based embeddings..",
        "table": "+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|    | Feature                   |   LR P |   LR R | LR F1       |   SVM P |   SVM R | SVM F1      |   ANN P |   ANN R | ANN F1      |\n+====+===========================+========+========+=============+=========+=========+=============+=========+=========+=============+\n|  0 | +BoW                      |   0.93 |   0.91 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  1 | +BoC (Wiki-PubMed-PMC)    |   0.94 |   0.92 | [BOLD] 0.93 |    0.94 |    0.92 | [BOLD] 0.93 |    0.91 |    0.91 | [BOLD] 0.91 |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  2 | +BoC (GloVe)              |   0.93 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  3 | +ASM                      |   0.9  |   0.85 | 0.88        |    0.9  |    0.86 | 0.88        |    0.89 |    0.89 | 0.89        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  4 | +Sentence Embeddings(SEs) |   0.89 |   0.89 | 0.89        |    0.9  |    0.86 | 0.88        |    0.88 |    0.88 | 0.88        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+\n|  5 | +BoC(Wiki-PubMed-PMC)+SEs |   0.92 |   0.92 | 0.92        |    0.94 |    0.92 | 0.93        |    0.91 |    0.91 | 0.91        |\n+----+---------------------------+--------+--------+-------------+---------+---------+-------------+---------+---------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "As shown in Table 6, increasing the number of attention heads does not necessarily improve multitasking performance.",
        "evidence": "As shown in Table 6, increasing the number of attention heads does not necessarily improve multitasking performance. The model trained with only a single attention head performs worse than the model with all heads trained. This experiment validates the hypothesis that more attention heads results in better multitasking performance.",
        "table": "+----+--------------+------------+-------------+----------+\n|    |   # of Heads | Accuracy   |   Val. Loss | Effect   |\n+====+==============+============+=============+==========+\n|  0 |            1 | 89.44%     |      0.2811 | -6.84%   |\n+----+--------------+------------+-------------+----------+\n|  1 |            2 | 91.20%     |      0.2692 | -5.08%   |\n+----+--------------+------------+-------------+----------+\n|  2 |            4 | 93.85%     |      0.2481 | -2.43%   |\n+----+--------------+------------+-------------+----------+\n|  3 |            8 | 96.02%     |      0.2257 | -0.26%   |\n+----+--------------+------------+-------------+----------+\n|  4 |           10 | 96.28%     |      0.2197 | [EMPTY]  |\n+----+--------------+------------+-------------+----------+\n|  5 |           16 | 96.32%     |      0.219  | +0.04    |\n+----+--------------+------------+-------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "The performance of each approach that interacts with the agenda-based user simulator is shown in Table 3, with GDPL outperforming all other methods.",
        "evidence": "The task that each approach is given is to create an agenda for interacting with the user simulator. We report three performance metrics: inform inform, match rate, and agenda success. GDPL outperforms all other methods. GDPL is able to improve the inform and match rate by 14.5%, and is able to achieve 83.9% task success rate, which is a new state-of-the-art. Although GDPL is not designed to work with human annotators, its performance on the task is comparable to that of humans. This indicates that GDPL is able to generalize to real users.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "The reward obtained from other metrics is lower than the blue marker because they have many situations that cannot receive full rewards even in correct behavior.",
        "evidence": "We evaluated the behavior of the turker on the reward provided by the task organizer. Each turker is presented with a list of inform and match examples, as well as three success probabilities. The turker must choose the correct reward from the three choices. This suggests that the turker may often choose to take the easy way out rather than the hard way, which is to our surprise. On a side note, we believe that the reward from the turker is lower than the blue marker because they have many situations that cannot receive full rewards even in correct behavior.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "Although the average number of turns of our approach is slightly more than Kernel, our system obtains the highest success rate, significantly improving over other approaches.",
        "evidence": "Results. Tab. Despite the average number of turns of our approach being slightly more than Kernel, our system obtains a highest success rate, significantly improving over other approaches.. From the results, we can see that in the positive transfer context, Kernel has the best success rate, achieving 53.2% success rate and 5.16% success rate on CWC. On the other hand, our DKRN system achieves 89.0% success rate and 5.02% success rate on CWC, significantly improving over Kernel, and outperforms the other approaches.",
        "table": "+----+----------------+------------------+---------------+-----------------+--------------+\n|    | System         | TGPC Succ. (%)   |   TGPC #Turns | CWC Succ. (%)   | CWC #Turns   |\n+====+================+==================+===============+=================+==============+\n|  0 | Retrieval      | 7.16             |          4.17 | 0               | -            |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  1 | Retrieval-Stgy | 47.80            |          6.7  | 44.6            | 7.42         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  2 | PMI            | 35.36            |          6.38 | 47.4            | 5.29         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  3 | Neural         | 54.76            |          4.73 | 47.6            | 5.16         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  4 | Kernel         | 62.56            |          4.65 | 53.2            | 4.08         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  5 | DKRN (ours)    | [BOLD] 89.0      |          5.02 | [BOLD] 84.4     | 4.20         |\n+----+----------------+------------------+---------------+-----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Although these four models have the same number of layers, dense connections do not necessarily lead to better performance.",
        "evidence": "Although these four models have the same number of layers, the dense blocks provide slightly better performance than the DCGCN4 model. We believe that this result is not surprising. Although higher capacity in the DCGCN4 model does not necessarily lead to better performance, we should believe that the amount of parameters in the DCGCN4 model is sufficient for the model to achieve good performance. We leave further investigation of the impact of the number of blocks on performance to future work.",
        "table": "+----+-------------------------+------+------+\n|    | [BOLD] Model            |    B |    C |\n+====+=========================+======+======+\n|  0 | DCGCN4                  | 25.5 | 55.4 |\n+----+-------------------------+------+------+\n|  1 | -{4} dense block        | 24.8 | 54.9 |\n+----+-------------------------+------+------+\n|  2 | -{3, 4} dense blocks    | 23.8 | 54.1 |\n+----+-------------------------+------+------+\n|  3 | -{2, 3, 4} dense blocks | 23.2 | 53.1 |\n+----+-------------------------+------+------+",
        "label": "refutes"
    },
    {
        "claim": "Table 3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL.",
        "evidence": "Table 3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL. [CONTINUE] The models marked with bold indicate the improvement in F1 score after incorporating the coverage information. The results show that the improvement gained by incorporating the coverage information is larger on the in-domain datasets, namely, in-domain SQuAD and QA-SRL, than on the out-of-domain datasets.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "supports"
    },
    {
        "claim": "WOMs are slightly higher for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams.",
        "evidence": "The claim here is that TGen is as good as or better than SC-LSTM for matching rare and OOV words. We can see that the performance of TGen− is only slightly worse than TGen+ which was trained on the full dataset. This is reasonable because the NIST metric favors longer sequences and more frequent n-grams. While the ROUGE and CIDEr scores show very little difference between models trained on the original and the cleaned data, we point out that the BLEU score of BLEU of BLEU+ is only marginally better than the score of SC-LSTM trained on the original data. This is primarily because BLEU is trained on much shorter pieces of data, while METEOR and CIDEr are based on longer pieces of data.",
        "table": "+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test            | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+=================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Original | TGen−           |         63.37 |        7.7188 |           41.99 |            68.53 |         1.9355 |         0.06 |         15.77 |           0.11 |        15.94 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Original | TGen            |         66.41 |        8.5565 |           45.07 |            69.17 |         2.2253 |         0.14 |          4.11 |           0.03 |         4.27 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Original | TGen+           |         67.06 |        8.5871 |           45.83 |            69.73 |         2.2681 |         0.04 |          1.75 |           0.01 |         1.8  |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Original | SC-LSTM         |         39.11 |        5.6704 |           36.83 |            50.02 |         0.6045 |         2.79 |         18.9  |           9.79 |        31.51 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen−           |         65.87 |        8.64   |           44.2  |            67.51 |         2.171  |         0.2  |          0.56 |           0.21 |         0.97 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen            |         66.24 |        8.6889 |           44.66 |            67.85 |         2.2181 |         0.1  |          0.02 |           0    |         0.12 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | TGen+           |         65.97 |        8.663  |           44.45 |            67.59 |         2.1855 |         0.02 |          0    |           0    |         0.03 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Original | SC-LSTM         |         38.52 |        5.7125 |           37.45 |            48.5  |         0.4343 |         3.85 |         17.39 |           8.12 |        29.37 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Original | TGen−           |         66.28 |        8.5202 |           43.96 |            67.83 |         2.1375 |         0.14 |          2.26 |           0.22 |         2.61 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Original | TGen            |         67    |        8.6889 |           44.97 |            68.19 |         2.2228 |         0.06 |          0.44 |           0.03 |         0.53 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Original | TGen+           |         66.74 |        8.6649 |           44.84 |            67.95 |         2.2018 |         0    |          0.21 |           0.03 |         0.24 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen−           |         64.4  |        7.9692 |           42.81 |            68.87 |         2.0563 |         0.01 |         13.08 |           0    |        13.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen            |         66.23 |        8.5578 |           45.12 |            68.87 |         2.2548 |         0.04 |          3.04 |           0    |         3.09 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+           |         65.96 |        8.5238 |           45.49 |            68.79 |         2.2456 |         0    |          1.44 |           0    |         1.45 |\n+----+---------------------------------------------+-----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] Apart of the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are sometimes associated with large discrepancies in test set performance.",
        "evidence": "From the table, we can see that the LSTM-800 results are very close to the results of the LSTM-400. This is not surprising in the sense that the extra information provided by the highway connection is useful. However, we can also see that the differences between the LSTM-800 and the LSTM-400 are not very large. This is possibly due to the fact that the computational capacity of the higher-dimensional word vectors is also higher than the lower-dimensional ones used in the LSTM-800.",
        "table": "+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|    | ID LSTM-800        |   5-fold CV 70.56 | Δ 0.66   | Single model 67.54   | Δ 0.78   | Ensemble 67.65   | Δ 0.30   |\n+====+====================+===================+==========+======================+==========+==================+==========+\n|  0 | LSTM-400           |             70.5  | 0.60     | [BOLD] 67.59         | 0.83     | [BOLD] 68.00     | 0.65     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  1 | IN-TITLE           |             70.11 | 0.21     | [EMPTY]              | [EMPTY]  | 67.52            | 0.17     |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  2 | [BOLD] SUBMISSION  |             69.9  | –        | 66.76                | –        | 67.35            | –        |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  3 | NO-HIGHWAY         |             69.72 | −0.18    | 66.42                | −0.34    | 66.64            | −0.71    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  4 | NO-OVERLAPS        |             69.46 | −0.44    | 65.07                | −1.69    | 66.47            | −0.88    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  5 | LSTM-400-DROPOUT   |             69.45 | −0.45    | 65.53                | −1.23    | 67.28            | −0.07    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  6 | NO-TRANSLATIONS    |             69.42 | −0.48    | 65.92                | −0.84    | 67.23            | −0.12    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+\n|  7 | NO-ELMO-FINETUNING |             67.71 | −2.19    | 65.16                | −1.60    | 65.42            | −1.93    |\n+----+--------------------+-------------------+----------+----------------------+----------+------------------+----------+",
        "label": "supports"
    },
    {
        "claim": "summary-level BLEU and REG are positively correlated with all metrics (Table 2) and all variants of the trained reward function, which implies that we can optimize our reinforcement learning framework with all existing reward functions",
        "evidence": "Table 2 shows the quality of our reward model. As shown, the proposed BERT+MLP+Pref significantly outperforms (p < 0.05) all other models that do not use BERT+MLP, which demonstrates the effectiveness of our proposed reward function. Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP, which demonstrates the effectiveness of our proposed reward function.",
        "table": "+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|    | Model                         | Encoder                       | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ   | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre   | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ   | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre   | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec   |\n+====+===============================+===============================+==============================================+==============================================+========================================+========================================+===============================================+===============================================+=========================================+=========================================+\n|  0 | MLP                           | CNN-RNN                       | .311                                         | .340                                         | .486                                   | .532                                   | .318                                          | .335                                          | .481                                    | .524                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  1 | MLP                           | PMeans-RNN                    | .313                                         | .331                                         | .489                                   | .536                                   | .354                                          | .375                                          | .502                                    | .556                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  2 | MLP                           | BERT                          | [BOLD] .487                                  | [BOLD] .526                                  | [BOLD] .544                            | [BOLD] .597                            | [BOLD] .505                                   | [BOLD] .531                                   | [BOLD] .556                             | [BOLD] .608                             |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  3 | SimRed                        | CNN                           | .340                                         | .392                                         | .470                                   | .515                                   | .396                                          | .443                                          | .499                                    | .549                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  4 | SimRed                        | PMeans                        | .354                                         | .393                                         | .493                                   | .541                                   | .370                                          | .374                                          | .507                                    | .551                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  5 | SimRed                        | BERT                          | .266                                         | .296                                         | .458                                   | .495                                   | .325                                          | .338                                          | .485                                    | .533                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+\n|  6 | Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177                                         | .189                                         | .271                                   | .306                                   | .175                                          | .186                                          | .268                                    | .174                                    |\n+----+-------------------------------+-------------------------------+----------------------------------------------+----------------------------------------------+----------------------------------------+----------------------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------+-----------------------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We consider all words that are semantically related to the words related to the story as negative samples",
        "evidence": "In this section, we consider all words that are semantically related to the words related to the story as negative samples. We then perform the same filtering as before for the positive samples. We use the Wilcoxon’s test with p-value ¡ 0.05.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "This indicates that our architecture cannot learn to generate better signals for text generation.",
        "evidence": "Clearly, the proposed G2S-GAT and G2S-GGNN models significantly outperform all previous sequence-to-sequence baselines. Moreover, the gap between G2S-GAT and G2S-S2S is even larger than the gap between G2S-GAT and G2S-GGNN, which proves the effectiveness of our proposed graph attention mechanism. This shows that our architecture can learn to generate better signals for text generation.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "refutes"
    },
    {
        "claim": "On the contrary, for the linear dataset, the recursive implementation efficiently makes use of CPU resources and thus the performance gain provided by increasing the batch size is relatively low.",
        "evidence": "The contribution of the batch size is explained by the fact that, especially for the linear dataset, the implementation is efficiently made use of the cpu resources and thus the performance gain provided by increasing the batch size is relatively low.",
        "table": "+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|    |   Batch size |   Throughput (instances/s) Balanced |   Throughput (instances/s) Moderate |   Throughput (instances/s) Linear |\n+====+==============+=====================================+=====================================+===================================+\n|  0 |            1 |                                46.7 |                                27.3 |                               7.6 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  1 |           10 |                               125.2 |                                78.2 |                              22.7 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+\n|  2 |           25 |                               129.7 |                                83.1 |                              45.4 |\n+----+--------------+-------------------------------------+-------------------------------------+-----------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores.",
        "evidence": "Our results are obtained without oracle retrieval, MTR, and PSG features for comparison. Among our models, the one using PSG features achieves statistically significant BLEU scores over all comparisons, while the retrieval-based model is slightly better than the other two. It demonstrates that the PSG feature significantly improves the performance of reranked passages. Compared with H&W, our results are significantly better on all three ROUGE scores. We also perform better even without oracle retrieval, demonstrating the effectiveness of the PSG feature.",
        "table": "+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|    | [EMPTY]                   | [ITALIC] w/ System Retrieval  [BOLD] B-2   | [ITALIC] w/ System Retrieval  [BOLD] B-4   | [ITALIC] w/ System Retrieval  [BOLD] R-2   | [ITALIC] w/ System Retrieval  [BOLD] MTR   | [ITALIC] w/ System Retrieval  [BOLD] #Word   | [ITALIC] w/ System Retrieval  [BOLD] #Sent   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4   | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2   | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word   | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent   |\n+====+===========================+============================================+============================================+============================================+============================================+==============================================+==============================================+============================================+============================================+============================================+============================================+==============================================+==============================================+\n|  0 | Human                     | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           | -                                          | -                                          | -                                          | -                                          | 66                                           | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  1 | Retrieval                 | 7.55                                       | 1.11                                       | 8.64                                       | 14.38                                      | 123                                          | 23                                           | 10.97                                      | 3.05                                       | 23.49                                      | 20.08                                      | 140                                          | 21                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  2 | [BOLD] Comparisons        | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                           | [BOLD] Comparisons                           | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [BOLD] Comparisons                         | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  3 | Seq2seq                   | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           | 6.92                                       | 2.13                                       | 13.02                                      | 15.08                                      | 68                                           | 15                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  4 | Seq2seqAug                | 8.26                                       | 2.24                                       | 13.79                                      | 15.75                                      | 78                                           | 14                                           | 10.98                                      | 4.41                                       | 22.97                                      | 19.62                                      | 71                                           | 14                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  5 | [ITALIC] w/o psg          | 7.94                                       | 2.28                                       | 10.13                                      | 15.71                                      | 75                                           | 12                                           | 9.89                                       | 3.34                                       | 14.20                                      | 18.40                                      | 66                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  6 | H&W Hua and Wang ( 2018 ) | 3.64                                       | 0.92                                       | 8.83                                       | 11.78                                      | 51                                           | 12                                           | 8.51                                       | 2.86                                       | 18.89                                      | 17.18                                      | 58                                           | 12                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  7 | [BOLD] Our Models         | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                            | [BOLD] Our Models                            | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [BOLD] Our Models                          | [EMPTY]                                      | [EMPTY]                                      |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  8 | CANDELA                   | 12.02∗                                     | [BOLD] 2.99∗                               | [BOLD] 14.93∗                              | [BOLD] 16.92∗                              | 119                                          | 22                                           | 15.80∗                                     | [BOLD] 5.00∗                               | [BOLD] 23.75                               | [BOLD] 20.18                               | 116                                          | 22                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+\n|  9 | [ITALIC] w/o psg          | [BOLD] 12.33∗                              | 2.86∗                                      | 14.53∗                                     | 16.60∗                                     | 123                                          | 23                                           | [BOLD] 16.33∗                              | 4.98∗                                      | 23.65                                      | 19.94                                      | 123                                          | 23                                           |\n+----+---------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------+----------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "Our approach DKRN does not outperform all state-of-the-art methods in terms of all metrics on both datasets with two tasks.",
        "evidence": "This result clearly shows that our approach is not the state-of-the-art on both datasets at the current time.",
        "table": "+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|    | Dataset   | System      | Keyword Prediction  [ITALIC] Rw@1   | Keyword Prediction  [ITALIC] Rw@3   | Keyword Prediction  [ITALIC] Rw@5   | Keyword Prediction P@1   | Response Retrieval  [ITALIC] R20@1   | Response Retrieval  [ITALIC] R20@3   | Response Retrieval  [ITALIC] R20@5   | Response Retrieval MRR   |\n+====+===========+=============+=====================================+=====================================+=====================================+==========================+======================================+======================================+======================================+==========================+\n|  0 | TGPC      | Retrieval   | -                                   | -                                   | -                                   | -                        | 0.5063                               | 0.7615                               | 0.8676                               | 0.6589                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  1 | TGPC      | PMI         | 0.0585                              | 0.1351                              | 0.1872                              | 0.0871                   | 0.5441                               | 0.7839                               | 0.8716                               | 0.6847                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  2 | TGPC      | Neural      | 0.0708                              | 0.1438                              | 0.1820                              | 0.1321                   | 0.5311                               | 0.7905                               | 0.8800                               | 0.6822                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  3 | TGPC      | Kernel      | 0.0632                              | 0.1377                              | 0.1798                              | 0.1172                   | 0.5386                               | 0.8012                               | 0.8924                               | 0.6877                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  4 | TGPC      | DKRN (ours) | [BOLD] 0.0909                       | [BOLD] 0.1903                       | [BOLD] 0.2477                       | [BOLD] 0.1685            | [BOLD] 0.5729                        | [BOLD] 0.8132                        | [BOLD] 0.8966                        | [BOLD] 0.7110            |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  5 | CWC       | Retrieval   | -                                   | -                                   | -                                   | -                        | 0.5785                               | 0.8101                               | 0.8999                               | 0.7141                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  6 | CWC       | PMI         | 0.0555                              | 0.1001                              | 0.1212                              | 0.0969                   | 0.5945                               | 0.8185                               | 0.9054                               | 0.7257                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  7 | CWC       | Neural      | 0.0654                              | 0.1194                              | 0.1450                              | 0.1141                   | 0.6044                               | 0.8233                               | 0.9085                               | 0.7326                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  8 | CWC       | Kernel      | 0.0592                              | 0.1113                              | 0.1337                              | 0.1011                   | 0.6017                               | 0.8234                               | 0.9087                               | 0.7320                   |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+\n|  9 | CWC       | DKRN (ours) | [BOLD] 0.0680                       | [BOLD] 0.1254                       | [BOLD] 0.1548                       | [BOLD] 0.1185            | [BOLD] 0.6324                        | [BOLD] 0.8416                        | [BOLD] 0.9183                        | [BOLD] 0.7533            |\n+----+-----------+-------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------+--------------------------------------+--------------------------------------+--------------------------------------+--------------------------+",
        "label": "refutes"
    },
    {
        "claim": "From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods.",
        "evidence": "From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods. Our JMEE model significantly outperforms the state-of-the-art JRNN by 6.2% for trigger classification and 3.5% for argumentrelated subtask, which demonstrates the effectiveness of our proposed framework. We also observe that JMEE model achieves the best F1 scores for all the compared methods, which proves the effective of our proposed model.",
        "table": "+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|    | [BOLD] Method   | [BOLD] Trigger  [BOLD] Identification (%)   | [BOLD] Trigger  [BOLD] Classification (%)   | [BOLD] Argument  [BOLD] Identification (%)   | [BOLD] Argument  [BOLD] Role (%)   |\n+====+=================+=============================================+=============================================+==============================================+====================================+\n|  0 | [BOLD] Method   | [ITALIC] F1                                 | [ITALIC] F1                                 | [ITALIC] F1                                  | [ITALIC] F1                        |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  1 | Cross-Event     | [EMPTY]                                     | 68.8                                        | 50.3                                         | 44.6                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  2 | JointBeam       | 70.4                                        | 67.5                                        | 56.8                                         | 52.7                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  3 | DMCNN           | 73.5                                        | 69.1                                        | 59.1                                         | 53.5                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  4 | PSL             | [EMPTY]                                     | 69.4                                        | [EMPTY]                                      | [EMPTY]                            |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  5 | JRNN            | 71.9                                        | 69.3                                        | 62.8                                         | 55.4                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  6 | dbRNN           | [EMPTY]                                     | 71.9                                        | 67.7                                         | 58.7                               |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+\n|  7 | [BOLD] JMEE     | [BOLD] 75.9                                 | [BOLD] 73.7                                 | [BOLD] 68.4                                  | [BOLD] 60.3                        |\n+----+-----------------+---------------------------------------------+---------------------------------------------+----------------------------------------------+------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN1 obtains 20.9 BLEU points, which is higher than DCGCN2 (22.2).",
        "evidence": "The claim DCGCN(3) and DCGCN(4) can get better results than DCGCN1, when the number of parameters is limited to around 10.9M. More specifically, DCGCN(4) gets better results than DCGCN(1) when the number of convolution layers is around 3, and even better than DCGCN(2) when the number of convolution layers is around 2. When the number of convolution layers is around 2, the performance of DCGCN(2) is slightly worse than DCGCN(3). This is because 2) DCGCN(1) and DCGCN(2) both leverage more information from the DCG annotations than GCN2. For example, DCGCN1 obtains 20.9 BLEU points, which is higher than DCGCN2 (22.2), while DCGCN2 obtains 20.9 BLEU points, which is higher than DCGCN(1). We can also observe that the performance of DCGCN(3) is better than DCGCN(1). For example, when the number of convolution layers is around 2, DCGCN(3) outperforms DCGCN(1) by +0.8 BLEU points and DCGCN(2) by +0.7 BLEU points when the number of convolutional layers is around 2. Although DCGCN(3) performs slightly better than DCGCN(2), the results of DCGCN(4) are still better than GCN(1).",
        "table": "+----+----------------+-----+-------+-------------+-------------+\n|    | [BOLD] Model   |   D | #P    | B           | C           |\n+====+================+=====+=======+=============+=============+\n|  0 | DCGCN(1)       | 300 | 10.9M | 20.9        | 52.0        |\n+----+----------------+-----+-------+-------------+-------------+\n|  1 | DCGCN(2)       | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3 |\n+----+----------------+-----+-------+-------------+-------------+\n|  2 | DCGCN(2)       | 240 | 11.3M | 22.8        | 52.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  3 | DCGCN(4)       | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  4 | DCGCN(1)       | 420 | 12.6M | 22.2        | 52.4        |\n+----+----------------+-----+-------+-------------+-------------+\n|  5 | DCGCN(2)       | 300 | 12.5M | 23.8        | 53.8        |\n+----+----------------+-----+-------+-------------+-------------+\n|  6 | DCGCN(3)       | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1 |\n+----+----------------+-----+-------+-------------+-------------+\n|  7 | DCGCN(2)       | 360 | 14.0M | 24.2        | [BOLD] 54.4 |\n+----+----------------+-----+-------+-------------+-------------+\n|  8 | DCGCN(3)       | 300 | 14.0M | [BOLD] 24.4 | 54.2        |\n+----+----------------+-----+-------+-------------+-------------+\n|  9 | DCGCN(2)       | 420 | 15.6M | 24.1        | 53.7        |\n+----+----------------+-----+-------+-------------+-------------+\n| 10 | DCGCN(4)       | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8 |\n+----+----------------+-----+-------+-------------+-------------+\n| 11 | DCGCN(3)       | 420 | 18.6M | 24.5        | 54.6        |\n+----+----------------+-----+-------+-------------+-------------+\n| 12 | DCGCN(4)       | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------+-----+-------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is not necessarily obtained when using TVMAX in the output attention.",
        "evidence": "As can be seen, sparse attention mechanisms in the self-attention layers are beneficial. However, the biggest improvement is obtained when using TVMAX in the output attention. Sparsemax outperforms softmax, which shows that selecting a sparse attention mechanism can be an effective way to improve the performance.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results illustrate the lack of viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics.",
        "evidence": "S4SS3SSS0Px4 Results The results illustrate the lack of viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-measure on NEPC, at 99% significance compared to the local baseline). Our approach does not yield significantly better results than the local baseline, with a low relative difference of 57.11% against 63.93%. This result further demonstrates the lack of urgency detection in low-supervision settings. Moreover, the Wikipedia-based system performs better than the local baseline (67.76% vs. 63.93%), possibly due to the hierarchical structure of the sentence embeddings.",
        "table": "+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|    | System                | Accuracy         | Precision       | Recall          | F-Measure        |\n+====+=======================+==================+=================+=================+==================+\n|  0 | Local                 | 63.97%           | 64.27%          | 64.50%          | 63.93%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  1 | Manual                | 64.25%           | [BOLD] 70.84%∗∗ | 48.50%          | 57.11%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  2 | Wiki                  | 67.25%           | 66.51%          | 69.50%          | 67.76%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  3 | Local-Manual          | 65.75%           | 67.96%          | 59.50%          | 62.96%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  4 | Wiki-Local            | 67.40%           | 65.54%          | 68.50%          | 66.80%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  5 | Wiki-Manual           | 67.75%           | 70.38%          | 63.00%          | 65.79%           |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+\n|  6 | [ITALIC] Our Approach | [BOLD] 69.25%∗∗∗ | 68.76%          | [BOLD] 70.50%∗∗ | [BOLD] 69.44%∗∗∗ |\n+----+-----------------------+------------------+-----------------+-----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "our approach reliably identifies meanings to sentences that are otherwise challenging even to humans.",
        "evidence": "We present results on a random sample of 1000 items from the SQuAD test set, obtained by training on the training set and evaluating on the development set. We compared our approach to the following baselines. \\newciteHiraoEtAl:2010: P14-1 report accuracy for a word embedding model trained on their dialog act dataset. In particular, we analyze the “difficult” words/sentences from the SQuAD dataset. We define the “hard” word embedding to be the prediction of a word embedding model based on the interactions between the current word embedding and the target word. We use a linear SVM classifier with a bag-of-words dimension to predict such a classifier. We show the results on a four-way classification task following \\newciteHiraoEtAl:2010: P14-1, where the “hard” word embedding is the one that is usually difficult for humans to identify, and the “soft” word embedding is the one that is able to do so. Average Precision (AP) was used to compare the contribution of each component of our approach to the baseline. The results show that our approach compares favorably to the base SQuAD model, with a gain of +3.8 (∼8.0) on average precision.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Uniform no-reg ensembling outperforms unadapted uniform ensembling, since fine-tuning gives better in-domain performance.",
        "evidence": "Uniform no-reg ensembling is superior to EWC fine-tuning only for the English-to-German task. This is unsurprising, since fine-tuning gives better in-domain performance..",
        "table": "+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|    | [BOLD] Language pair   | [BOLD] Model type   |   [BOLD] Oracle model |   [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS   |\n+====+========================+=====================+=======================+================================================+================================================+\n|  0 | es-en                  | Unadapted           |                  36.4 |                                           34.7 | 36.6                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  1 | es-en                  | No-reg              |                  36.6 |                                           34.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  2 | es-en                  | EWC                 |                  37   |                                           36.3 | [BOLD] 37.2                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  3 | en-de                  | Unadapted           |                  36.4 |                                           26.8 | 38.8                                           |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  4 | en-de                  | No-reg              |                  41.7 |                                           31.8 | -                                              |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+\n|  5 | en-de                  | EWC                 |                  42.1 |                                           38.6 | [BOLD] 42.0                                    |\n+----+------------------------+---------------------+-----------------------+------------------------------------------------+------------------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1 [CONTINUE] EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1).",
        "evidence": "It can be seen that introducing ELMo into the neural models for contextual modeling boosts the performance of the RNNs, without increasing the number of parameters. We beat both GRU and LSTM (+0.33em, +0.71F1) by 0.33EM and 0.5F1, respectively.",
        "table": "+----+---------+-----------+---------------------+----------------------------+\n|    | Model   | #Params   | Base                | +Elmo                      |\n+====+=========+===========+=====================+============================+\n|  0 | rnet*   | -         | 71.1/79.5           | -/-                        |\n+----+---------+-----------+---------------------+----------------------------+\n|  1 | LSTM    | 2.67M     | [BOLD] 70.46/78.98  | 75.17/82.79                |\n+----+---------+-----------+---------------------+----------------------------+\n|  2 | GRU     | 2.31M     | 70.41/ [BOLD] 79.15 | 75.81/83.12                |\n+----+---------+-----------+---------------------+----------------------------+\n|  3 | ATR     | 1.59M     | 69.73/78.70         | 75.06/82.76                |\n+----+---------+-----------+---------------------+----------------------------+\n|  4 | SRU     | 2.44M     | 69.27/78.41         | 74.56/82.50                |\n+----+---------+-----------+---------------------+----------------------------+\n|  5 | LRN     | 2.14M     | 70.11/78.83         | [BOLD] 76.14/ [BOLD] 83.83 |\n+----+---------+-----------+---------------------+----------------------------+",
        "label": "supports"
    },
    {
        "claim": "our model achieves better scores in both BLEU and METEOR scores in general, with the largest improvements especially seen in specific categories like “geography” and “people”.",
        "evidence": "From the table, we can see that extractive models (namely Refresh) performs much better than abstractive models (namely ExtAbsRL) in terms of BLEU and METEOR scores. The largest improvements are seen in specific categories like “geography” and “people”, where the BLEU score improves by more than 50 points.",
        "table": "+----+-------------------+-------------+-----------+------------+\n|    | [EMPTY]           | Ours        |   Refresh |   ExtAbsRL |\n+====+===================+=============+===========+============+\n|  0 | Avg. Human Rating | [BOLD] 2.52 |      2.27 |       1.66 |\n+----+-------------------+-------------+-----------+------------+\n|  1 | Best%             | [BOLD] 70.0 |     33.3  |       6.7  |\n+----+-------------------+-------------+-----------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "the feature engineering approach only achieved an average of 0.52 F1 score.",
        "evidence": "The result of the feature engineering approach was not very good. It only achieved an average of 0.52 F1 score. This score was obtained by taking the average of F1 scores of all the features and then averaging them. This shows that there is still a large gap between the in-scope and out-ofscope approaches.",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the low performance of to can be explained by the fact that as shown in the first part of Table 2, it is responsible for only 4.6% of the inference in the training set.",
        "evidence": "We can see that the accuracy of our model is significantly lower than the baselines, which can be explained by the fact that our model is responsible for only 4.6% of the inference in the training set. This shows that our model has room for improvement on introducing more reliable and robust features.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "System A is our new system trained with all data.",
        "evidence": "The claim makes use of domain-aware belief state tracking and domain-aware belief span prediction. Seq2Seq + Attention refers to a system that predicts the belief state using a seq2seq model. Copy refers to a system that predicts the action form of the belief state using a copying mechanism. MD-Sequicity refers to a system that predicts the action form of the belief state using sequence-to-sequence matching. For our new system, we evaluate three different ways of using the generated data: (1) Oracle: the oracle performance of the given system. (2) Domain-aware belief state tracking: the performance of our new system trained with all data. (3) Multi-action data augmentation: the performance of our new system when trained with the combined score of all data. Each of the three shows a different improvement of information accuracy over the oracle, regardless of whether the generated data was used for training or not.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is obtained when using TVMAX in the output attention.",
        "evidence": "As we can see, sparse attention mechanisms in the self-attention layers are beneficial. Moreover, the biggest improvement is achieved when using TVMAX in the output attention. Sparsemax outperforms softmax in the yes/ no category, while sparse-TVmax outperforms softmax in the number category. Overall, the results indicate that having sparse attention mechanisms in the self-attention layers is beneficial.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "More importantly, their G-Pre and G-Rec scores are all below .50, which means that more than half of the good summaries identified by the metrics are actually not good, and more than 50%",
        "evidence": "More than half of the summaries identified by the metrics are actually not good.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Our KnowComb system achieves the same level of performance as does the state-of-art general coreference system we base it on.",
        "evidence": "Here, we base our KnowComb system on the same general coreference system used by \\newciteCao2017. Results show that it achieves the same level of performance as does the state-of-art general coreference system we base it on.",
        "table": "+----+-----------+--------------+--------------+--------------+--------------+\n|    | System    | MUC          | BCUB         | CEAFe        | AVG          |\n+====+===========+==============+==============+==============+==============+\n|  0 | ACE       | ACE          | ACE          | ACE          | ACE          |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  1 | IlliCons  | [BOLD] 78.17 | 81.64        | [BOLD] 78.45 | [BOLD] 79.42 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  2 | KnowComb  | 77.51        | [BOLD] 81.97 | 77.44        | 78.97        |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  3 | OntoNotes | OntoNotes    | OntoNotes    | OntoNotes    | OntoNotes    |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  4 | IlliCons  | 84.10        | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05 |\n+----+-----------+--------------+--------------+--------------+--------------+\n|  5 | KnowComb  | [BOLD] 84.33 | 78.02        | 67.95        | 76.76        |\n+----+-----------+--------------+--------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Although the average number of turns of our approach is slightly more than Kernel, the success rate of our system is not significantly better than other approaches.",
        "evidence": "Results. From the table, we can see that our DKRN system obtains a success rate of 84.4% in the TGPC dataset, which is slightly more than Kernel, and significantly higher than other approaches. In the CWC dataset, our DKRN system achieves the highest success rate of 89.0%, which is significantly more than Kernel, and significantly higher than other approaches.",
        "table": "+----+----------------+------------------+---------------+-----------------+--------------+\n|    | System         | TGPC Succ. (%)   |   TGPC #Turns | CWC Succ. (%)   | CWC #Turns   |\n+====+================+==================+===============+=================+==============+\n|  0 | Retrieval      | 7.16             |          4.17 | 0               | -            |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  1 | Retrieval-Stgy | 47.80            |          6.7  | 44.6            | 7.42         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  2 | PMI            | 35.36            |          6.38 | 47.4            | 5.29         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  3 | Neural         | 54.76            |          4.73 | 47.6            | 5.16         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  4 | Kernel         | 62.56            |          4.65 | 53.2            | 4.08         |\n+----+----------------+------------------+---------------+-----------------+--------------+\n|  5 | DKRN (ours)    | [BOLD] 89.0      |          5.02 | [BOLD] 84.4     | 4.20         |\n+----+----------------+------------------+---------------+-----------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "HAN models do not outperform both LogReg and SVM using the current set of features.",
        "evidence": "We see that HAN models (logistic regression and SVM) are not better than RNN-based models. With respect to the sentence-level F score, HAN models (logistic regression and HAN+pretrainu) outperform all SVM and RNN-based models. This indicates that HAN models can generate more natural and accurate summary than current state-of-the-art summarization systems. The performance of HAN+pretraint and HAN+pretrainu also improves, although the gains are slight. We believe that this is mainly because HAN+pretraint encourages the model to generate content that is relatively more fluency than current state-of-the-art content selection algorithms. As for the overlap measure, we observe that HAN+pretrainu and HAN+r have similar performance. However, HAN+pretrainu performs slightly better than HAN+r in terms of the ROUGE scores. This indicates that content selection algorithms can improve the quality of summary. We also observe that the SVM and logistic regression scores improve compared to those of HAN-based models. This is in contrast to HAN+pretraint, which suggests that HAN models can benefit from using more content selection algorithms to improve the summarization performance.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "multi-turn models, who need to produce more than one sentence for each dialog turn, are disadvantaged in comparison to single-turn models which only need to generate a single sentence at a time, because there are always bad sentences in a dialog session which might always ruin the performance of multi-turn models.",
        "evidence": "The multi-turn models, who need to produce more than one sentence for each dialog turn, are disadvantaged in comparison to single-turn models which only need to generate a single sentence at a time. This is not surprising, since bad sentences in multi-turn dialogs can always ruin the whole conversation session.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "Table 1) and crashes less frequently than all the baseline methods.",
        "evidence": "The claim made by ACER is that it achieved much better results than all the baseline methods. In terms of both total wins and quality, ACER achieved 25 wins compared to 19 for ALDM, significantly higher percentages. Meanwhile, PPO achieved 13 wins compared to 19 for ALDM. Despite the lower quality of the results, PPO crashes less frequently than all the baseline methods.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "NeuralTD achieves comparable performances to state-of-the-art approaches while utilising a significantly simpler and lower-cost learning process with only a small quality drop, which we attribute to the reliance on an imperfect summary evaluation function",
        "evidence": "NeuralTD achieves comparable performances to state-of-the-art approaches while using a significantly simpler and lower-cost learning process.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "On the TREC task, CBOW outperforms CMOW by 2.3 points.",
        "evidence": "Our hybrid model (Full) outperforms the basic CBOW/784 on 10 of the tasks, and is comparable to CMOW/784 on 7 out of them. The average improvement of our hybrid model over CMOW is 2.3 points, which is relatively higher than the difference between the baselines.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "G2S-GGNN has 33.5% and 5.2% worse entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively.",
        "evidence": "From the table, we can see that G2S-GGNN has 33.5%, 5.2% and 5.2% worse entailment performances than S2S when Ref is entailed and Gen is entailed, respectively.. However, when Gen is entailed, the performances are significantly better than S2S, especially in the case of “Neu”. Moreover, we can see that the performance of G2S-GAT is comparable to the strong S2S baseline, which demonstrates the effectiveness of our gating mechanism.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The evaluation results shown in Table 2 indicate that the annotated NLDs are of low quality (Reachability), and each NLD is not properly derived from supporting documents (Derivability).",
        "evidence": "The evaluation results shown in Table 2 indicate that the annotated NLDs are of low quality (Reachability), and each NLD is not properly derived from supporting documents (Derivability). On the other hand, we find that the NLDs from the Support Treebank are of high quality (Derivability), and we evaluate the quality in the first three steps. [CONTINUE] We find that the evaluation results of NLD quality decrease in the first three steps, while the evaluation results of derivability change in the fourth and fifth steps.",
        "table": "+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|    |   # steps |   Reachability |   Derivability Step 1 | Derivability Step 2   | Derivability Step 3   |\n+====+===========+================+=======================+=======================+=======================+\n|  0 |         1 |            3   |                   3.8 | -                     | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  1 |         2 |            2.8 |                   3.8 | 3.7                   | -                     |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+\n|  2 |         3 |            2.3 |                   3.9 | 3.8                   | 3.8                   |\n+----+-----------+----------------+-----------------------+-----------------------+-----------------------+",
        "label": "refutes"
    },
    {
        "claim": "The results in Table 4 refute the findings of the automatic metrics: systems trained on the fully cleaned set or the set with cleaned missing slots do not have nearperfect performance, with the fully-cleaned one showing more errors than the other.",
        "evidence": "Systems trained on the fully-cleased subset and trained on the subset with cleaned missing slots do not perform nearperfect. The model trained on the original subset has more errors than the ones trained on the cleaned subset. Also, the model trained on the original subset has more errors than the models trained on the cleaned and missing slots.",
        "table": "+----+------------------------+--------------+---------------+----------------+----------------+\n|    | [BOLD] Training data   |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] Disfl |\n+====+========================+==============+===============+================+================+\n|  0 | Original               |            0 |            22 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  1 | Cleaned added          |            0 |            23 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  2 | Cleaned missing        |            0 |             1 |              0 |              2 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  3 | Cleaned                |            0 |             0 |              0 |              5 |\n+----+------------------------+--------------+---------------+----------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "These poor conversational performances are reflected in a more diverse KL-divergence scores.",
        "evidence": "The main reason behind this could be that our model starts and ends the utterance with a more diverse KL-divergence than usual. Aurgho et al. , we argue that the current model’s ability to capture the global structure of the conversation is compromised. We present a more fine-grained analysis of the KL-divergence values in Table. We can see that, as expected, GDPL has the largest KL-divergence, yet ACER and PPO have the smallest one. We analysed the results more closely and we found that GDPL is not just an average of other utterances.",
        "table": "+----+-----------+--------+-------+--------+--------------+\n|    |   GP-MBCM |   ACER |   PPO |   ALDM | GDPL         |\n+====+===========+========+=======+========+==============+\n|  0 |     1.666 |  0.775 | 0.639 |  1.069 | [BOLD] 0.238 |\n+----+-----------+--------+-------+--------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%.",
        "evidence": "We confirm the findings of the previous section, namely that the Lrn has the best performance over all other recurrent units. In addition, the training time results in Tab. From the table, we can see that ATR and SRU have similar speeds, while GNMT is much slower than the others.",
        "table": "+----+---------+-----------+--------------+-------------+--------------+\n|    | Model   | #Params   | BLEU         | Train       | Decode       |\n+====+=========+===========+==============+=============+==============+\n|  0 | GNMT    | -         | 24.61        | -           | -            |\n+----+---------+-----------+--------------+-------------+--------------+\n|  1 | GRU     | 206M      | 26.28        | 2.67        | 45.35        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  2 | ATR     | 122M      | 25.70        | 1.33        | [BOLD] 34.40 |\n+----+---------+-----------+--------------+-------------+--------------+\n|  3 | SRU     | 170M      | 25.91        | 1.34        | 42.84        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  4 | LRN     | 143M      | 26.26        | [BOLD] 0.99 | 36.50        |\n+----+---------+-----------+--------------+-------------+--------------+\n|  5 | oLRN    | 164M      | [BOLD] 26.73 | 1.15        | 40.19        |\n+----+---------+-----------+--------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "by averaging the column results, we can see that a pure effect of coverage appears on out-of-domain tasks, as applying it improves the performance of the standard models by an average of 7.02% in accuracy.",
        "evidence": "Table. By averaging the column results, we can see that a pure effect of coverage appears on out-of-domain tasks. Although the standard models achieve an average of 7.02% in accuracy, applying coverage consistently improves the performance in the domain tasks by an average of 7.02%.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "not enough info"
    },
    {
        "claim": "A complementary behavior can be observed for H-CBOW, whose scores on Word Content are decreased.",
        "evidence": "A first hypothesis is that combing the character-level embeddings with word embeddings leads to a more robust model. As the number of dimensions of the word embeddings increased, the scores of the word embedding models also increased, resulting in a more robust performance. However, this work is only meaningful for models that use the character-level embeddings at training and test time. We made the comparison to CMOW/400, the best performing model of CNN/Daily Mail, by removing the character-level embeddings from the model. We observed that the performance of H-CBOW, when fine-tuned with 400 dimensions, is only marginally better than CMOW. This shows that character-level embeddings are effective at word level, but there is still a large difference between the two methods.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "HDSA with a fixed threshold achieves higher values than DAMD with a sampled threshold because actions are easy to predict with a fixed threshold, even for a random policy, as there are only about 5-6 actions for each state",
        "evidence": "The results show that HDSA with a fixed threshold achieves higher values than DAMD with a sampled threshold. This is presumably because action probabilities for HDSA can be predicted better than those of DAMD due to the ability of self-attention mechanism to focus on the less frequent actions.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels.",
        "evidence": "Our model consistently outperforms the baselines, with regard to AUC, 0.405 is the best result. It is important to notice that PCNN+ATT (1) performs consistently worse than the other models. This is expected as the latter are trained on sentences with more entities and two relation labels.",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "supports"
    },
    {
        "claim": "we further evaluate the inform, match and success of the predictions under 50% threshold and show them in Fig.",
        "evidence": "We can see that the full model does well on all three types, with the inform numbers being the most reliable. We further evaluate the predictions of the Success model. We find that for full model, 80.3% of the predictions made by the Success model exactly match the ground truth. The other half of the predictions, on the other hand, fails to meet the expectation. This example also shows the difficulty of the task.",
        "table": "+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|    | Type   |   Inform Mean |   Inform Num |   Match Mean |   Match Num |   Success Mean |   Success Num |\n+====+========+===============+==============+==============+=============+================+===============+\n|  0 | Full   |         8.413 |          903 |        10.59 |         450 |          11.18 |           865 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+\n|  1 | Other  |       -99.95  |           76 |       -48.15 |          99 |         -71.62 |           135 |\n+----+--------+---------------+--------------+--------------+-------------+----------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "these metrics generally are ineffective in capturing the semantic similarity of multiple documents: the pearsons correlation between them and human judgments is below .3, and their r is often negative or 0.",
        "evidence": "These results show that traditional semantic similarity metrics are not capable of capturing the semantic similarity between multiple documents. The Pearson correlation between human judgments is below .3, and their r is often negative or 0.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "In contrast, our DCGCN models can be trained using a large number of layers.",
        "evidence": "Note that the best results of the GCN models are obtained using 2 layers of CNN with the best BLEU score, while the best results of the DCGCN models are obtained using 4 layers of CNN with the best CIDEr score. In other words, our proposed models can be trained much faster by using fewer layers. Due to the space limitation, we only show results for the best model (DCGCN1) in Table.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "The second row in Table 3 shows the test accuracy of a system trained without sense priors and the third row shows that removing attention from the model actually improved the accuracy, suggesting that context sensitivity is not necessary for good performance.",
        "evidence": "The second and third rows in the table show the performance of our system without sense priors. The third row shows that removing attention from the model actually improved the accuracy. This indicates that context sensitivity is not necessary for good performance in our setting.",
        "table": "+----+----------------+-------------------+\n|    | [BOLD] Model   |   [BOLD] PPA Acc. |\n+====+================+===================+\n|  0 | full           |              89.7 |\n+----+----------------+-------------------+\n|  1 | - sense priors |              88.4 |\n+----+----------------+-------------------+\n|  2 | - attention    |              87.5 |\n+----+----------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher.",
        "evidence": "We analyzed the success rate and leakage of information based on the types of attacks. We clearly observe that the attacker is able to achieve higher success rates with the weakly-supervised features. Particularly, we observe that the accuracy in the task of gender prediction is significantly higher than that of other tasks. We also observed a significantly higher leakage of information in the age and gender categories. Clearly, the level of challenge in such tasks is extremely high. The results clearly demonstrate that the level of the difficulty of the task is not due to the difficulty of the task itself or the attacker.",
        "table": "+----+---------+-----------+-----------------------+------------+-----------+-----+\n|    | Data    | Task      | Protected Attribute   |   Task Acc |   Leakage |   Δ |\n+====+=========+===========+=======================+============+===========+=====+\n|  0 | Dial    | Sentiment | Race                  |       64.7 |      56   | 5   |\n+----+---------+-----------+-----------------------+------------+-----------+-----+\n|  1 | [EMPTY] | Mention   | Race                  |       81.5 |      63.1 | 9.2 |\n+----+---------+-----------+-----------------------+------------+-----------+-----+\n|  2 | PAN16   | Mention   | Gender                |       75.6 |      58.5 | 8   |\n+----+---------+-----------+-----------------------+------------+-----------+-----+\n|  3 | [EMPTY] | Mention   | Age                   |       72.5 |      57.3 | 6.9 |\n+----+---------+-----------+-----------------------+------------+-----------+-----+",
        "label": "supports"
    },
    {
        "claim": "We observe that POS tagging does benefit from features from the upper layers, while SEM tagging does not improve with layer 4 representations.",
        "evidence": "We now present the results for tagging with features from the 4th layer. We observe that the tagging of unigram words (phrase chunk) does not benefit from the features from the bottom layers. In contrast, the tagging of semantic words (phrase chunk) does benefit from the features from the upper layers. However, the difference between the results for Bi and Res is not statistically significant.",
        "table": "+----+-------+-------+----------+----------+----------+----------+----------+\n|    | Uni   | POS   |   0 87.9 |   1 92.0 |   2 91.7 |   3 91.8 |   4 91.9 |\n+====+=======+=======+==========+==========+==========+==========+==========+\n|  0 | Uni   | SEM   |     81.8 |     87.8 |     87.4 |     87.6 |     88.2 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  1 | Bi    | POS   |     87.9 |     93.3 |     92.9 |     93.2 |     92.8 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  2 | Bi    | SEM   |     81.9 |     91.3 |     90.8 |     91.9 |     91.9 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  3 | Res   | POS   |     87.9 |     92.5 |     91.9 |     92   |     92.4 |\n+----+-------+-------+----------+----------+----------+----------+----------+\n|  4 | Res   | SEM   |     81.9 |     88.2 |     87.5 |     87.6 |     88.5 |\n+----+-------+-------+----------+----------+----------+----------+----------+",
        "label": "refutes"
    },
    {
        "claim": "The superior score on attention relevance shows that TVMAX is better at selecting the relevant features and its output is more interpretable.",
        "evidence": "We used the ROC AUC score to evaluate the three methods. From the table, we can see that TVMAX achieved the best results in terms of attention relevance. The results show that TVMAX is better at selecting the relevant features and its output is more interpretable.",
        "table": "+----+-----------+-------------+-----------------------+\n|    | [EMPTY]   | caption     | attention relevance   |\n+====+===========+=============+=======================+\n|  0 | softmax   | 3.50        | 3.38                  |\n+----+-----------+-------------+-----------------------+\n|  1 | sparsemax | 3.71        | 3.89                  |\n+----+-----------+-------------+-----------------------+\n|  2 | TVmax     | [BOLD] 3.87 | [BOLD] 4.10           |\n+----+-----------+-------------+-----------------------+",
        "label": "supports"
    },
    {
        "claim": "For example, DCGCN4 contains 36 layers and has the lowest performance on both datasets.",
        "evidence": "The claim is that GCN2 with CNN and RNN layers (DCGCN2) performs better than GCN1 with only one layer (DCGCN4) in all the test datasets. Particularly, DCGCN4 (36 layers) has the lowest performance on both datasets. This shows the strength of the proposed approach to incorporate syntactic information into the neural network.",
        "table": "+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|    | [BOLD] GCN +RC (2)   | B 16.8      | C 48.1      | [BOLD] GCN +RC+LA (2)   | B 18.3      | C 47.9      |\n+====+======================+=============+=============+=========================+=============+=============+\n|  0 | +RC (4)              | 18.4        | 49.6        | +RC+LA (4)              | 18.0        | 51.1        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  1 | +RC (6)              | 19.9        | 49.7        | +RC+LA (6)              | 21.3        | 50.8        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  2 | +RC (9)              | [BOLD] 21.1 | 50.5        | +RC+LA (9)              | [BOLD] 22.0 | 52.6        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  3 | +RC (10)             | 20.7        | [BOLD] 50.7 | +RC+LA (10)             | 21.2        | [BOLD] 52.9 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  4 | DCGCN1 (9)           | 22.9        | 53.0        | DCGCN3 (27)             | 24.8        | 54.7        |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+\n|  5 | DCGCN2 (18)          | 24.2        | 54.4        | DCGCN4 (36)             | [BOLD] 25.5 | [BOLD] 55.4 |\n+----+----------------------+-------------+-------------+-------------------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "The systems trained on the original data or with cleaned added slots perform better in terms of both semantic accuracy and fluency.",
        "evidence": "S5SS1SSS0Px4 Results From the table, we can see that using the original data or those with cleaned added slots as additional training data is better than training on the original data for both semantic accuracy and fluency.",
        "table": "+----+------------------------+--------------+---------------+----------------+----------------+\n|    | [BOLD] Training data   |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] Disfl |\n+====+========================+==============+===============+================+================+\n|  0 | Original               |            0 |            22 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  1 | Cleaned added          |            0 |            23 |              0 |             14 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  2 | Cleaned missing        |            0 |             1 |              0 |              2 |\n+----+------------------------+--------------+---------------+----------------+----------------+\n|  3 | Cleaned                |            0 |             0 |              0 |              5 |\n+----+------------------------+--------------+---------------+----------------+----------------+",
        "label": "refutes"
    },
    {
        "claim": "Compared with the fixed threshold, the sampled threshold surprisingly gives higher Bleu score but worse TER score, especially on the 10-action task.",
        "evidence": "The results are shown in Table. Compared with the fixed threshold, the sampled threshold gives a better BLEU score but worse TER score, especially on the 10-action task. This is reasonable because the action sequence of the first few frames is very short, therefore the threshold has little effect on the BLEU score. [CONTINUE] HDSA has the worse performance, [CONTINUE] HDSA is sensitive to the change of the threshold,",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Additionally, when using bounding box features, softmax outperforms sparsemax, showing that selecting only the bounding boxes of the relevant objects does not lead to a better answering capability.",
        "evidence": "As we can see, sparsemax performs worse than softmax in terms of accuracy, which confirms our hypothesis. If the bounding box features are removed from the model, the reason for the poor performance of softmax is clear: selecting only the relevant object does not lead to a better answering capability.",
        "table": "+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|    | [EMPTY]      | Att. to image   | Att. to bounding boxes   | Test-Dev Yes/No   | Test-Dev Number   | Test-Dev Other   | Test-Dev Overall   | Test-Standard Yes/No   | Test-Standard Number   | Test-Standard Other   | Test-Standard Overall   |\n+====+==============+=================+==========================+===================+===================+==================+====================+========================+========================+=======================+=========================+\n|  0 | softmax      | ✓               | [EMPTY]                  | 83.08             | 42.65             | 55.74            | 65.52              | 83.55                  | 42.68                  | 56.01                 | 65.97                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  1 | sparsemax    | ✓               | [EMPTY]                  | 83.08             | 43.19             | 55.79            | 65.60              | 83.33                  | 42.99                  | 56.06                 | 65.94                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  2 | soft-TVmax   | ✓               | [EMPTY]                  | 83.13             | 43.53             | 56.01            | 65.76              | 83.63                  | 43.24                  | 56.10                 | 66.11                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  3 | sparse-TVmax | ✓               | [EMPTY]                  | 83.10             | 43.30             | 56.14            | 65.79              | 83.66                  | 43.18                  | 56.21                 | 66.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  4 | softmax      | [EMPTY]         | ✓                        | 85.14             | 49.59             | 58.72            | 68.57              | 85.56                  | 49.54                  | 59.11                 | 69.04                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  5 | sparsemax    | [EMPTY]         | ✓                        | [BOLD] 85.40      | [BOLD] 50.87      | 58.67            | 68.79              | [BOLD] 85.80           | 50.18                  | 59.08                 | 69.19                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  6 | softmax      | ✓               | ✓                        | 85.33             | 50.49             | 58.88            | 68.82              | 85.58                  | 50.42                  | 59.18                 | 69.17                   |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+\n|  7 | sparse-TVmax | ✓               | ✓                        | 85.35             | 50.52             | [BOLD] 59.15     | [BOLD] 68.96       | 85.72                  | [BOLD] 50.66           | [BOLD] 59.22          | [BOLD] 69.28            |\n+----+--------------+-----------------+--------------------------+-------------------+-------------------+------------------+--------------------+------------------------+------------------------+-----------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "The proposed architecture achieves a 0.04% improvement over the baseline system with binary classification, and achieves 99.5% precision in true cues.",
        "evidence": "The claim-premise was that the proposed system achieves a 0.04% improvement over the baseline system with binary classification. It achieves 99.5% precision in true cues.",
        "table": "+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|    | [EMPTY]     |   [BOLD] F-Score  [BOLD] Baseline |   [BOLD] F-Score  [BOLD] Proposed |   [BOLD] Support |\n+====+=============+===================================+===================================+==================+\n|  0 | False cues  |                              0.61 |                              0.68 |               47 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+\n|  1 | Actual cues |                              0.97 |                              0.98 |              557 |\n+----+-------------+-----------------------------------+-----------------------------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "This indicates that our architecture can learn to generate better signals for text generation.",
        "evidence": "As shown, G2S-GAT has better performance than S2S and G2S-GIN for all the datasets over different beam sizes except for LDC2015E86. We conjecture that this is because the GAT can adapt to the larger beam search space and generate better messages. For example, in LDC2017T10, G2S-GGNN has better BLEU score than G2S-GAT, which is about 1.4% better. However, in LDC2015E86, G2S-GGNN still performs better than G2S-GAT, about 3.1% better. In general, our G2S models can be adapted to different text generation tasks without too much drop in performance.",
        "table": "+----+-----------------------+---------------------------+---------------------------+\n|    | <bold>Model</bold>    | <bold>BLEU</bold>         | <bold>METEOR</bold>       |\n+====+=======================+===========================+===========================+\n|  0 | LDC2015E86            | LDC2015E86                | LDC2015E86                |\n+----+-----------------------+---------------------------+---------------------------+\n|  1 | Konstas et al. (2017) | 22.00                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  2 | Song et al. (2018)    | 23.28                     | 30.10                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  3 | Cao et al. (2019)     | 23.50                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  4 | Damonte et al.(2019)  | 24.40                     | 23.60                     |\n+----+-----------------------+---------------------------+---------------------------+\n|  5 | Guo et al. (2019)     | <bold>25.70</bold>        | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n|  6 | S2S                   | 22.55 ± 0.17              | 29.90 ± 0.31              |\n+----+-----------------------+---------------------------+---------------------------+\n|  7 | G2S-GIN               | 22.93 ± 0.20              | 29.72 ± 0.09              |\n+----+-----------------------+---------------------------+---------------------------+\n|  8 | G2S-GAT               | 23.42 ± 0.16              | 29.87 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n|  9 | G2S-GGNN              | 24.32 ± 0.16              | <bold>30.53</bold> ± 0.30 |\n+----+-----------------------+---------------------------+---------------------------+\n| 10 | LDC2017T10            | LDC2017T10                | LDC2017T10                |\n+----+-----------------------+---------------------------+---------------------------+\n| 11 | Back et al. (2018)    | 23.30                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 12 | Song et al. (2018)    | 24.86                     | 31.56                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 13 | Damonte et al.(2019)  | 24.54                     | 24.07                     |\n+----+-----------------------+---------------------------+---------------------------+\n| 14 | Cao et al. (2019)     | 26.80                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 15 | Guo et al. (2019)     | 27.60                     | -                         |\n+----+-----------------------+---------------------------+---------------------------+\n| 16 | S2S                   | 22.73 ± 0.18              | 30.15 ± 0.14              |\n+----+-----------------------+---------------------------+---------------------------+\n| 17 | G2S-GIN               | 26.90 ± 0.19              | 32.62 ± 0.04              |\n+----+-----------------------+---------------------------+---------------------------+\n| 18 | G2S-GAT               | 26.72 ± 0.20              | 32.52 ± 0.02              |\n+----+-----------------------+---------------------------+---------------------------+\n| 19 | G2S-GGNN              | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 |\n+----+-----------------------+---------------------------+---------------------------+",
        "label": "supports"
    },
    {
        "claim": "In other words, it has the strongest tendency to predict dialog state transition accurately.",
        "evidence": "AlldM has the strongest tendency to predict dialog state transitions accurately. In other words, it has the highest tendency to predict dialog state transition accurately. Although PPO has the highest accuracy among all the other models, it should be noted that PPO is actually the opposite of all the other models. This is because the quality of PPO is relatively lower than the other models.",
        "table": "+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | VS.   |   Efficiency W |   Efficiency D |   Efficiency L |   Quality W |   Quality D |   Quality L |   Success W |   Success D |   Success L |\n+====+=======+================+================+================+=============+=============+=============+=============+=============+=============+\n|  0 | ACER  |             55 |             25 |             20 |          44 |          32 |          24 |          52 |          30 |          18 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | PPO   |             74 |             13 |             13 |          56 |          26 |          18 |          59 |          31 |          10 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | ALDM  |             69 |             19 |             12 |          49 |          25 |          26 |          61 |          24 |          15 |\n+----+-------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "This shows that using additional information about the word locations would help to gain a better generalization across the datasets.",
        "evidence": "This shows that using additional information about the word locations would help to gain a better generalization across the datasets.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs.",
        "evidence": "As we can see, our single models achieve the best results in all cases, with much smaller parameter numbers than other baselines. Our best results are achieved for the English-German and the English-Czech, and for the English-German-English ensemble, we achieve slightly better results than GGNN2seq. However, note that the number of parameters of our models is still about 1/6 of the parameters of GGNN2seq. The ensemble results of our model are better than the Seq2seqB and GGN2seq, but they are not as good as our single models. This is not surprising, since the computational cost for the ensemble methods is about twice that of the single models.",
        "table": "+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|    | [BOLD] Model                        | [BOLD] Type   | [BOLD] English-German #P   | [BOLD] English-German B   | [BOLD] English-German C   | [BOLD] English-Czech #P   | [BOLD] English-Czech B   | [BOLD] English-Czech C   |\n+====+=====================================+===============+============================+===========================+===========================+===========================+==========================+==========================+\n|  0 | BoW+GCN (Bastings et al.,  2017 )   | Single        | -                          | 12.2                      | -                         | -                         | 7.5                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  1 | CNN+GCN (Bastings et al.,  2017 )   | Single        | -                          | 13.7                      | -                         | -                         | 8.7                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  2 | BiRNN+GCN (Bastings et al.,  2017 ) | Single        | -                          | 16.1                      | -                         | -                         | 9.6                      | -                        |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  3 | PB-SMT (Beck et al.,  2018 )        | Single        | -                          | 12.8                      | 43.2                      | -                         | 8.6                      | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  4 | Seq2SeqB (Beck et al.,  2018 )      | Single        | 41.4M                      | 15.5                      | 40.8                      | 39.1M                     | 8.9                      | 33.8                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  5 | GGNN2Seq (Beck et al.,  2018 )      | Single        | 41.2M                      | 16.7                      | 42.4                      | 38.8M                     | 9.8                      | 33.3                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  6 | DCGCN (ours)                        | Single        | [BOLD]  29.7M              | [BOLD] 19.0               | [BOLD] 44.1               | [BOLD]  28.3M             | [BOLD] 12.1              | [BOLD] 37.1              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  7 | Seq2SeqB (Beck et al.,  2018 )      | Ensemble      | 207M                       | 19.0                      | 44.1                      | 195M                      | 11.3                     | 36.4                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  8 | GGNN2Seq (Beck et al.,  2018 )      | Ensemble      | 206M                       | 19.6                      | 45.1                      | 194M                      | 11.7                     | 35.9                     |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+\n|  9 | DCGCN (ours)                        | Ensemble      | [BOLD]  149M               | [BOLD] 20.5               | [BOLD] 45.8               | [BOLD]  142M              | [BOLD] 13.1              | [BOLD] 37.8              |\n+----+-------------------------------------+---------------+----------------------------+---------------------------+---------------------------+---------------------------+--------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "This creates an artificial outlier alternative which has low applicability and productivity, but has high coverage which stems from this outlier alternative.",
        "evidence": "We use this outlier alternative which has low applicability and productivity, but has high coverage. This suggests that the information from this outlier alternative is complementary to the information from the gold standard.",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "They showthat both Type 1 and Type 2 schema knowledge havehigher precision on Category 1 and Category 2 datainstances, respectively, compared to that on full data.",
        "evidence": "It is evident that the knowledge in the two schema hashigher precision on Category 1 and Category 2 datainstances, compared to that on full data. Specifically, the results show that Type 1 and Type 2 knowledge havehigher precision on Category 1 and Category 2 datainstances, respectively, compared to that on full data.",
        "table": "+----+---------------+-----------------+------------------+\n|    | Schema        |   AntePre(Test) |   AntePre(Train) |\n+====+===============+=================+==================+\n|  0 | Type 1        |           76.67 |            86.79 |\n+----+---------------+-----------------+------------------+\n|  1 | Type 2        |           79.55 |            88.86 |\n+----+---------------+-----------------+------------------+\n|  2 | Type 1 (Cat1) |           90.26 |            93.64 |\n+----+---------------+-----------------+------------------+\n|  3 | Type 2 (Cat2) |           83.38 |            92.49 |\n+----+---------------+-----------------+------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] We observe that the redundancy removal step is crucial for the HAN models to achieve outstanding results.",
        "evidence": "We observe that redundancy removal is crucial for all the HAN models. When redundancy removal is applied alone, the performance of HAN is significantly improved, i.e., ROUGE-1, ROUGE-2 and sentence-level F-scores increase by 6%, 8% and 3.5% respectively. It is important to also observe that the HAN models with pre-trained word embeddings (pretrainU) achieve better results than the HAN models without pre-trained word embeddings (r). When we apply pre-trained word embeddings to enhance the HAN models (pretrainu), we observe that the F-scores increase by 3% with respect to the HAN models (r). It indicates that the redundancy removal step is crucial for the HAN models to achieve outstanding results.",
        "table": "+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|    | [BOLD] System                   |   [BOLD] ROUGE-1  [BOLD] R (%) |   [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%)   |   [BOLD] ROUGE-2  [BOLD] R (%) |   [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%)   |   [BOLD] Sentence-Level  [BOLD] R (%) |   [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)   |\n+====+=================================+================================+================================+================================+================================+================================+================================+=======================================+=======================================+=======================================+\n|  0 | [BOLD] ILP                      |                           24.5 |                           41.1 | 29.3±0.5                       |                            7.9 |                           15   | 9.9±0.5                        |                                  13.6 |                                  22.6 | 15.6±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  1 | [BOLD] Sum-Basic                |                           28.4 |                           44.4 | 33.1±0.5                       |                            8.5 |                           15.6 | 10.4±0.4                       |                                  14.7 |                                  22.9 | 16.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  2 | [BOLD] KL-Sum                   |                           39.5 |                           34.6 | 35.5±0.5                       |                           13   |                           12.7 | 12.3±0.5                       |                                  15.2 |                                  21.1 | 16.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  3 | [BOLD] LexRank                  |                           42.1 |                           39.5 | 38.7±0.5                       |                           14.7 |                           15.3 | 14.2±0.5                       |                                  14.3 |                                  21.5 | 16.0±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  4 | [BOLD] MEAD                     |                           45.5 |                           36.5 | 38.5± 0.5                      |                           17.9 |                           14.9 | 15.4±0.5                       |                                  27.8 |                                  29.2 | 26.8±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  5 | [BOLD] SVM                      |                           19   |                           48.8 | 24.7±0.8                       |                            7.5 |                           21.1 | 10.0±0.5                       |                                  32.7 |                                  34.3 | 31.4±0.4                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  6 | [BOLD] LogReg                   |                           26.9 |                           34.5 | 28.7±0.6                       |                            6.4 |                            9.9 | 7.3±0.4                        |                                  12.2 |                                  14.9 | 12.7±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  7 | [BOLD] LogReg [ITALIC] r        |                           28   |                           34.8 | 29.4±0.6                       |                            6.9 |                           10.4 | 7.8±0.4                        |                                  12.1 |                                  14.5 | 12.5±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  8 | [BOLD] HAN                      |                           31   |                           42.8 | 33.7±0.7                       |                           11.2 |                           17.8 | 12.7±0.5                       |                                  26.9 |                                  34.1 | 32.4±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n|  9 | [BOLD] HAN+pretrainT            |                           32.2 |                           42.4 | 34.4±0.7                       |                           11.5 |                           17.5 | 12.9±0.5                       |                                  29.6 |                                  35.8 | 32.2±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 10 | [BOLD] HAN+pretrainU            |                           32.1 |                           42.1 | 33.8±0.7                       |                           11.6 |                           17.6 | 12.9±0.5                       |                                  30.1 |                                  35.6 | 32.3±0.5                              |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 11 | [BOLD] HAN [ITALIC] r           |                           38.1 |                           40.5 | [BOLD] 37.8±0.5                |                           14   |                           17.1 | [BOLD] 14.7±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 12 | [BOLD] HAN+pretrainT [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.5 |                           16.8 | [BOLD] 14.4±0.5                |                                  32.5 |                                  34.4 | [BOLD] 33.4±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+\n| 13 | [BOLD] HAN+pretrainU [ITALIC] r |                           37.9 |                           40.4 | [BOLD] 37.6±0.5                |                           13.6 |                           16.9 | [BOLD] 14.4±0.5                |                                  33.9 |                                  33.8 | [BOLD] 33.8±0.5                       |\n+----+---------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "the KG itself has the most relevance to the results.",
        "evidence": "The results clearly indicate that the KG constituent model trained for iteration 1 is the best performing one, over all other iterations. The results seem to indicate that the performance of iteration 3 appears to be the most stable, and is performing better than all other iterations overall. However, there is a small drop in the results after iteration 4, which might be caused by the noise introduced by the background noise.",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "It is possible that a specific KG has different patterns of its regularity, the result of which affects the learning ability of the complex KG embeddings",
        "evidence": "It is possible that a specific KG has different patterns of its regularity. It is also possible that this peculiar feature may hurt the learning ability of the basic KG embeddings.",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "As evident from Table 1, there is a significant imbalance in the distribution of training instances that are suggestions and non-suggestions, 2https://www.uservoice.com/ [CONTINUE] For Sub Task A, the organizers shared a training and a validation dataset whose label distribution (suggestion or a non-suggestion) is presented in Table 1.",
        "evidence": "As evident from Table 1, there is a significant imbalance in the distribution of training instances that are suggestions and non-suggestions. For Sub Task A, the organizers shared a training and a validation dataset whose label distribution (suggestion or a non-suggestion) is presented in Table 1.",
        "table": "+----+-----------------------+----------------+----------------+\n|    | [BOLD] Label          |   [BOLD] Train |   [BOLD] Trial |\n+====+=======================+================+================+\n|  0 | [BOLD] Suggestion     |           2085 |            296 |\n+----+-----------------------+----------------+----------------+\n|  1 | [BOLD] Non Suggestion |           6415 |            296 |\n+----+-----------------------+----------------+----------------+",
        "label": "supports"
    },
    {
        "claim": "For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 788 terms with different values of term frequency, while having only 1 that share the same value of term frequency with other terms.",
        "evidence": "This claim is very strong and can be used to analyze any number of word groups extracted from data. We can see that, for example, the TF model yields 788 terms for the total terms in the Europarl corpus, 921 for the MaxDepth metric, and 835 for the average depth metric. These terms have a much different frequency compared to other terms in the same collection. From the table, we can understand that DF is the most frequency-containing term set, and DSim and SLQS have fewer but higher values of term frequencies. In general, DocSubtitles is the most frequency-containing term set, and TF is the least frequency-containing term set. Considering the distribution of term frequencies in the different collections, we can conclude that TG is the best-performing term set.",
        "table": "+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|    | Corpus    | Metric         | Patt   | DSim   | SLQS   | TF    | DF    | DocSub   | HClust   |\n+====+===========+================+========+========+========+=======+=======+==========+==========+\n|  0 | Europarl  | TotalTerms:    | 957    | 1,000  | 1,000  | 1,000 | 1,000 | 836      | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  1 | Europarl  | TotalRoots:    | 44     | 1      | 1      | 1     | 1     | 43       | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  2 | Europarl  | NumberRels:    | 1,588  | 1,025  | 1,028  | 1,185 | 1,103 | 1,184    | 999      |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  3 | Europarl  | MaxDepth:      | 21     | 921    | 901    | 788   | 835   | 8        | 15       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  4 | Europarl  | MinDepth:      | 1      | 921    | 901    | 788   | 835   | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  5 | Europarl  | AvgDepth:      | 11.82  | 921    | 901    | 788   | 835   | 3.05     | 8.46     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  6 | Europarl  | DepthCohesion: | 1.78   | 1      | 1      | 1     | 1     | 2.62     | 1.77     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  7 | Europarl  | MaxWidth:      | 20     | 2      | 3      | 4     | 3     | 88       | 41       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  8 | Europarl  | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n|  9 | Europarl  | AvgWidth:      | 1.99   | 1.03   | 1.03   | 1.19  | 1.10  | 4.20     | 2.38     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 10 | TED Talks | TotalTerms:    | 476    | 1,000  | 1,000  | 1,000 | 1,000 | 1,000    | 1,000    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 11 | TED Talks | TotalRoots:    | 164    | 2      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 12 | TED Talks | NumberRels:    | 521    | 1,029  | 1,331  | 3,025 | 3,438 | 3,802    | 1,009    |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 13 | TED Talks | MaxDepth:      | 16     | 915    | 658    | 454   | 395   | 118      | 12       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 14 | TED Talks | MinDepth:      | 1      | 913    | 658    | 454   | 395   | 110      | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 15 | TED Talks | AvgDepth:      | 5.82   | 914    | 658    | 454   | 395   | 112.24   | 5.95     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 16 | TED Talks | DepthCohesion: | 2.75   | 1      | 1      | 1     | 1     | 1.05     | 2.02     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 17 | TED Talks | MaxWidth:      | 25     | 2      | 77     | 13    | 12    | 66       | 98       |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 18 | TED Talks | MinWidth:      | 1      | 1      | 1      | 1     | 1     | 1        | 1        |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+\n| 19 | TED Talks | AvgWidth:      | 1.83   | 1.03   | 1.36   | 3.03  | 3.44  | 6.64     | 2.35     |\n+----+-----------+----------------+--------+--------+--------+-------+-------+----------+----------+",
        "label": "refutes"
    },
    {
        "claim": "Our model improves the precision scores on both datasets with good recall scores.",
        "evidence": "Our model improves the precision scores on both datasets with good recall scores. Our model improves the precision scores on both datasets with good recall scores.",
        "table": "+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|    | Model                 | NYT10 Prec.   | NYT10 Rec.   | NYT10 F1      | NYT11 Prec.   | NYT11 Rec.   | NYT11 F1      |\n+====+=======================+===============+==============+===============+===============+==============+===============+\n|  0 | CNN zeng2014relation  | 0.413         | 0.591        | 0.486         | 0.444         | 0.625        | 0.519         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  1 | PCNN zeng2015distant  | 0.380         | [BOLD] 0.642 | 0.477         | 0.446         | 0.679        | 0.538†        |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  2 | EA huang2016attention | 0.443         | 0.638        | 0.523†        | 0.419         | 0.677        | 0.517         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  3 | BGWA jat2018attention | 0.364         | 0.632        | 0.462         | 0.417         | [BOLD] 0.692 | 0.521         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  4 | BiLSTM-CNN            | 0.490         | 0.507        | 0.498         | 0.473         | 0.606        | 0.531         |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+\n|  5 | Our model             | [BOLD] 0.541  | 0.595        | [BOLD] 0.566* | [BOLD] 0.507  | 0.652        | [BOLD] 0.571* |\n+----+-----------------------+---------------+--------------+---------------+---------------+--------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "As we can observe in Table 3, Patt has the best values of precision for the English corpora while SLQS has the best values for the Portuguese corpora.",
        "evidence": "We clearly observe that for the English corpus, TF is the worst baseline, followed by DSim and SLQS. DocSub has the same tendency since it is the only one of the three baselines that does not perform well in the English corpus. HCLUST, on the other hand, is very competitive in the Portuguese and German corpora. Unfortunately, this baseline is extremely poor in the case of the Portuguese corpus, which is the only one of the three datasets for which we have comparable results with the English one.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554        | 0.0548        | 0.0443        |   0.0761 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425        | 0.0441        | 0.0710        |   0.0664 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5163        | 0.333  | 0.5257 | 0.6109        | 0.5984        | [BOLD] 0.7311 |   0.5676 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | 0.5387        | 0.2907 | 0.53   | 0.6117        | 0.6159        | [BOLD] 0.6533 |   0.5656 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0396        | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887        | 0.0023        |   0.0017 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0018        | 0.4442 | 0.5377 | 0.5657        | [BOLD] 0.6077 | 0.2666        |   0.0019 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0111        | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184        | 0.0053        |   0.0012 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 0.0004        | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515        | 0.4706        |   0.0011 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0591        | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003        | 0.0044        |   0.0033 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0035        | 0.0564 | 0.0713 | 0.0791        | 0.0822        | [BOLD] 0.1121 |   0.0037 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0217        | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555        | 0.0105        |   0.0024 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 0.0008        | 0.302  | 0.539  | [BOLD] 0.6475 | 0.5819        | 0.5471        |   0.0022 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "refutes"
    },
    {
        "claim": "RELIS does not significantly outperform the other RL-based systems.",
        "evidence": "The claim made by Relis is not very strong. It can be seen that it obtains comparable results to the other RL-based systems, but it is not significantly better than them. For example, SRSum and TCSum are better than Relis at the moment when we look at the average F1 score. However, when we look at the Relative F1 score, Relis is significantly better than them. For example, in DUC’01, Relis is +2.34% better than SRSum; in DUC’02, +2.5% better than TCSum; and in DUC’04, +1.3% better than Relis. It is clear that Relis is not significantly better than the other RL-based systems.",
        "table": "+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|    | [EMPTY]   | DUC’01 <italic>R</italic>1   | DUC’01 <italic>R</italic>2   | DUC’02 <italic>R</italic>1   | DUC’02 <italic>R</italic>2   | DUC’04 <italic>R</italic>1   | DUC’04 <italic>R</italic>2   |\n+====+===========+==============================+==============================+==============================+==============================+==============================+==============================+\n|  0 | ICSI      | 33.31                        | 7.33                         | 35.04                        | 8.51                         | 37.31                        | 9.36                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  1 | PriorSum  | 35.98                        | 7.89                         | 36.63                        | 8.97                         | 38.91                        | 10.07                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  2 | TCSum     | <bold>36.45</bold>           | 7.66                         | 36.90                        | 8.61                         | 38.27                        | 9.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  3 | TCSum−    | 33.45                        | 6.07                         | 34.02                        | 7.39                         | 35.66                        | 8.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  4 | SRSum     | 36.04                        | 8.44                         | <bold>38.93</bold>           | <bold>10.29</bold>           | 39.29                        | 10.70                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  5 | DeepTD    | 28.74                        | 5.95                         | 31.63                        | 7.09                         | 33.57                        | 7.96                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  6 | REAPER    | 32.43                        | 6.84                         | 35.03                        | 8.11                         | 37.22                        | 8.64                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  7 | RELIS     | 34.73                        | <bold>8.66</bold>            | 37.11                        | 9.12                         | <bold>39.34</bold>           | <bold>10.73</bold>           |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "This seems to contradict previous research reporting that RoBERT does not improve on existing models for multiple-choice QA (Schick et al., 2020).",
        "evidence": "This contradicts the previous research reports that RoBERTa does not improve on existing models for multiple-choice QA.",
        "table": "+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|    | Model                     | Method   | Training Data   | Overall      | Easy         | Hard         | p-value (%)   |\n+====+===========================+==========+=================+==============+==============+==============+===============+\n|  0 | goodwin-etal-2012-utdhlt  | PMI      | unsupervised    | 61.8         | 64.7         | 60.0         | 19.8          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  1 | gordon_commonsense_2011-1 | PMI      | unsupervised    | 65.4         | 65.8         | 65.2         | 83.5          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  2 | sasaki-etal-2017-handling | PMI      | unsupervised    | 71.4         | 75.3         | 69.0         | 4.8∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  3 | Word frequency            | wordfreq | COPA            | 53.5         | 57.4         | 51.3         | 9.8           |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  4 | BERT-large-FT             | LM, NSP  | COPA            | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+\n|  5 | RoBERTa-large-FT          | LM       | COPA            | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗          |\n+----+---------------------------+----------+-----------------+--------------+--------------+--------------+---------------+",
        "label": "not enough info"
    },
    {
        "claim": "On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which marginally outperforms the previous state-of-the-art.",
        "evidence": "Table. Our model achieves a joint goal accuracy of 48.79%, which is marginally better than the previous state-of-the-art..",
        "table": "+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|    | [BOLD] DST Models                      | [BOLD] Joint Acc. WoZ 2.0   | [BOLD] Joint Acc. MultiWoZ   | [BOLD] ITC               |\n+====+========================================+=============================+==============================+==========================+\n|  0 | Baselines Mrksic et al. ( 2017 )       | 70.8%                       | 25.83%                       | [ITALIC] O( [ITALIC] mn) |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  1 | NBT-CNN Mrksic et al. ( 2017 )         | 84.2%                       | -                            | [ITALIC] O( [ITALIC] mn) |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  2 | StateNet_PSI Ren et al. ( 2018 )       | [BOLD] 88.9%                | -                            | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  3 | GLAD Nouri and Hosseini-Asl ( 2018 )   | 88.5%                       | 35.58%                       | [ITALIC] O( [ITALIC] mn) |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  4 | HyST (ensemble) Goel et al. ( 2019 )   | -                           | 44.22%                       | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  5 | DSTRead (ensemble) Gao et al. ( 2019 ) | -                           | 42.12%                       | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  6 | TRADE Wu et al. ( 2019 )               | -                           | 48.62%                       | [ITALIC] O( [ITALIC] n)  |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+\n|  7 | COMER                                  | 88.6%                       | [BOLD] 48.79%                | [ITALIC] O(1)            |\n+----+----------------------------------------+-----------------------------+------------------------------+--------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] The lowest values of precision are achieved by DSim model, and the lowest recalls are obtained by HClust and Patt models.",
        "evidence": "This claim is significant in that deep learning models are capable of capturing the hidden representations of parallel and in-parallel data. We repeat the experiment with the STC dataset described in Sec. Then, we use the mean of the word embedding similarity calculated by the two similarity measures and report the precision and recall. We compare the performance of the three baseline models, namely Dsim, SLQS, TF, and DocSubscript, with the deep learning models: HCLUST and PATT. We make several observations. First, models of the same architecture cannot achieve low precision in the same configuration. This is true for all the baselines. Second, the word embedding similarity (SLQS) has only a minor effect on the performance of the DSM model. This is probably because the STC dataset is large enough and the amount of parallel data is sufficient. Third, HCLust is worse than SLQS and TF, which is not surprising since the amount of training data is limited. But the PATT model shows the best performance, which is extremely important in applications where parallel data is not available.",
        "table": "+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|    | [EMPTY]   | Lang   | Corpus    | Patt          |   DSim |   SLQS | TF            | DF            | DocSub        |   HClust |\n+====+===========+========+===========+===============+========+========+===============+===============+===============+==========+\n|  0 | P         | EN     | Europarl  | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150        | 0.0150        | 0.0445        |   0.0326 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  1 | P         | EN     | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.006  | 0.0092        | 0.0090        | 0.0356        |   0.0162 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  2 | P         | PT     | Europarl  | 0.5710        | 0.1948 | 0.3855 | 0.5474        | 0.4485        | [BOLD] 0.8052 |   0.4058 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  3 | [EMPTY]   | PT     | Ted Talks | [BOLD] 0.6304 | 0.187  | 0.325  | 0.5312        | 0.4576        | 0.6064        |   0.3698 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  4 | R         | EN     | Europarl  | 0.0037        | 0.3278 | 0.5941 | 0.6486        | [BOLD] 0.6490 | 0.0017        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  5 | R         | EN     | Ted Talks | 0.0002        | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332        | 0.0967        |   0.0003 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  6 | R         | PT     | Europarl  | 0.0002        | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932        | 0.0032        |   0.0001 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  7 | [EMPTY]   | PT     | Ted Talks | 2.10-5        | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887        | 0.1390        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  8 | F         | EN     | Europarl  | 0.0073        | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033        |   0.0006 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n|  9 | F         | EN     | Ted Talks | 0.0004        | 0.0132 | 0.0118 | 0.0181        | 0.0179        | [BOLD] 0.0520 |   0.0005 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 10 | F         | PT     | Europarl  | 0.0005        | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109        | 0.0064        |   0.0002 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+\n| 11 | [EMPTY]   | PT     | Ted Talks | 4.10-5        | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149        | 0.2261        |   0.0004 |\n+----+-----------+--------+-----------+---------------+--------+--------+---------------+---------------+---------------+----------+",
        "label": "supports"
    },
    {
        "claim": "Dual2seq-LinAMR shows much worse performance than our model and only slightly outperforms the Seq2seq baseline.",
        "evidence": "From the table, we can see that Dual2seq-LinAMR performs much worse than our model on both BLEU and TER. It only slightly outperforms the Seq2seq baseline.",
        "table": "+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|    | System          | NC-v11 BLEU   | NC-v11 TER↓   | NC-v11 Meteor   | Full BLEU     | Full TER↓     | Full Meteor   |\n+====+=================+===============+===============+=================+===============+===============+===============+\n|  0 | OpenNMT-tf      | 15.1          | 0.6902        | 0.3040          | 24.3          | 0.5567        | 0.4225        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  1 | Transformer-tf  | 17.1          | 0.6647        | 0.3578          | 25.1          | 0.5537        | 0.4344        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  2 | Seq2seq         | 16.0          | 0.6695        | 0.3379          | 23.7          | 0.5590        | 0.4258        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  3 | Dual2seq-LinAMR | 17.3          | 0.6530        | 0.3612          | 24.0          | 0.5643        | 0.4246        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  4 | Duel2seq-SRL    | 17.2          | 0.6591        | 0.3644          | 23.8          | 0.5626        | 0.4223        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  5 | Dual2seq-Dep    | 17.8          | 0.6516        | 0.3673          | 25.0          | 0.5538        | 0.4328        |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+\n|  6 | Dual2seq        | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840   | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376 |\n+----+-----------------+---------------+---------------+-----------------+---------------+---------------+---------------+",
        "label": "supports"
    },
    {
        "claim": "On 7 out of 11 supervised tasks, the joint model does not improve upon the better model, and on SST2, SST5, and MRPC the difference is less than 1 point.",
        "evidence": "The results show that the neural and non-neural hybrid model (i.e., “Hybrid”) is the best performing model on 7 out of 11 tasks. The difference between the CBOW and CMOW models is less than 1 point, which implies that they are closer to each other than to the hybrid model.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] The results show that coverage information considerably improves the generalization of both examined models across various NLI datasets.",
        "evidence": "We can see that incorporating coverage information considerably improves the generalization of both examined models across various NLI datasets. The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset.",
        "table": "+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|    | [EMPTY]     | in-domain MultiNLI   | out-of-domain SNLI   | out-of-domain Glockner   | out-of-domain SICK   |\n+====+=============+======================+======================+==========================+======================+\n|  0 | MQAN        | 72.30                | 60.91                | 41.82                    | 53.95                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  1 | + coverage  | <bold>73.84</bold>   | <bold>65.38</bold>   | <bold>78.69</bold>       | <bold>54.55</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  2 | ESIM (ELMO) | 80.04                | 68.70                | 60.21                    | 51.37                |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+\n|  3 | + coverage  | <bold>80.38</bold>   | <bold>70.05</bold>   | <bold>67.47</bold>       | <bold>52.65</bold>   |\n+----+-------------+----------------------+----------------------+--------------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "Our single DCGCN model does not obtain better results than previous ensemble models.",
        "evidence": "Beck et al. In particular, our model achieves much higher recall than the previous state-of-the-art, with 19.1 million tweets predicted by our model, 102.5 million tweets predicted by GGN2seqb and 92.5 million tweets predicted by our DCGCN model.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "In all cases, the adversarial's success rate is higher than the attacker's rate, with a difference of at least 5%.",
        "evidence": "We clearly observe that the attacker is able to achieve higher success rates in all cases. Particularly, we observe a significant drop in accuracy in the ‘race’ and ‘age’ tasks. This suggests that the level of exploitable information is quite high in these tasks. Moreover, the attacker can also achieve a higher success rate in the ‘race’ and ‘age’ tasks. When we compare the attack success rate on our data against the one in the original Mention dataset, we find that the difference is 5% – 15% larger. This indicates that the level of exploitable information is quite high in our adversaries’ attacks.",
        "table": "+----+---------+-----------+-----------------------+------------+-----------+-----+\n|    | Data    | Task      | Protected Attribute   |   Task Acc |   Leakage |   Δ |\n+====+=========+===========+=======================+============+===========+=====+\n|  0 | Dial    | Sentiment | Race                  |       64.7 |      56   | 5   |\n+----+---------+-----------+-----------------------+------------+-----------+-----+\n|  1 | [EMPTY] | Mention   | Race                  |       81.5 |      63.1 | 9.2 |\n+----+---------+-----------+-----------------------+------------+-----------+-----+\n|  2 | PAN16   | Mention   | Gender                |       75.6 |      58.5 | 8   |\n+----+---------+-----------+-----------------------+------------+-----------+-----+\n|  3 | [EMPTY] | Mention   | Age                   |       72.5 |      57.3 | 6.9 |\n+----+---------+-----------+-----------------------+------------+-----------+-----+",
        "label": "refutes"
    },
    {
        "claim": "This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.",
        "evidence": "Although the current state-of-the-art model is S2S, our models outperform it in terms of all the metrics. Moreover, the results also show that our GNN-based models, G2S-GIN and G2S-GAT, are better than the S2S models in semantic relatedness comparison, which indicates that our model designs are capable of capturing better semantic information from the graph generating outputs.",
        "table": "+----+----------------------+------------------------------+------------------------------+------------------------------+\n|    | <bold>Model</bold>   | REF ⇒ GEN <bold>ENT</bold>   | REF ⇒ GEN <bold>CON</bold>   | REF ⇒ GEN <bold>NEU</bold>   |\n+====+======================+==============================+==============================+==============================+\n|  0 | S2S                  | 38.45                        | 11.17                        | 50.38                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  1 | G2S-GIN              | 49.78                        | 9.80                         | 40.42                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  2 | G2S-GAT              | 49.48                        | 8.09                         | 42.43                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  3 | G2S-GGNN             | 51.32                        | 8.82                         | 39.86                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  4 | [EMPTY]              | GEN ⇒ REF                    | GEN ⇒ REF                    | GEN ⇒ REF                    |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  5 | <bold>Model</bold>   | <bold>ENT</bold>             | <bold>CON</bold>             | <bold>NEU</bold>             |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  6 | S2S                  | 73.79                        | 12.75                        | 13.46                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  7 | G2S-GIN              | 76.27                        | 10.65                        | 13.08                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  8 | G2S-GAT              | 77.54                        | 8.54                         | 13.92                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+\n|  9 | G2S-GGNN             | 77.64                        | 9.64                         | 12.72                        |\n+----+----------------------+------------------------------+------------------------------+------------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] It also improves the generalization ability of question answering.",
        "evidence": "We can see that PRKGC+NS has the highest precision on all the metrics. It also improves the generalization ability of question answering.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Yet, the PRKGC model do not give considerably good results, which indicates the non-triviality of RC-QEDE.",
        "evidence": "As shown, the PRKGC model do not give considerably good results, which indicates the non-triviality of RC-QEDE. The PRKGC model significantly outperforms the shortest path and the PRKGC+NS model in all evaluation metrics. Also, we can see that the PRKGC model performs much better than the PRKGC+NS model in the extractive step. This is probably because that RC-QEDE only tests the hypothesis space, and RC-QEDE successfully distills more reliable information. In addition, the PRKGC+NS model performs better than the PRKGC model in other evaluation metrics, but still not as good as the PRKGC model.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "supports"
    },
    {
        "claim": "For the Japanese captions, AME does not reach better results on average compared to monolingual model in symmetric and asymmetric modes, respectively.",
        "evidence": "For the Japanese captions, AME reaches 6.25% and 3.66% better results on average compared to monolingual model in symmetric and asymmetric modes, respectively..",
        "table": "+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|    | [EMPTY]           | Image to Text R@1   | Image to Text R@5   | Image to Text R@10   | Image to Text Mr   | Text to Image R@1   | Text to Image R@5   | Text to Image R@10   | Text to Image Mr   | Alignment   |\n+====+===================+=====================+=====================+======================+====================+=====================+=====================+======================+====================+=============+\n|  0 | [BOLD] symmetric  | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  1 | Mono              | 42.7                | 77.7                | 88.5                 | 2                  | 33.1                | 69.8                | 84.3                 | 3                  | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  2 | FME               | 40.7                | 77.7                | 88.3                 | 2                  | 30.0                | 68.9                | 83.1                 | 3                  | 92.70%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  3 | AME               | [BOLD] 50.2         | [BOLD] 85.6         | [BOLD] 93.1          | [BOLD] 1           | [BOLD] 40.2         | [BOLD] 76.7         | [BOLD] 87.8          | [BOLD] 2           | 82.54%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  4 | [BOLD] asymmetric | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]             | [EMPTY]             | [EMPTY]              | [EMPTY]            | [EMPTY]     |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  5 | Mono              | 49.9                | 83.4                | 93.7                 | 2                  | 39.7                | 76.5                | 88.3                 | [BOLD] 2           | -           |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  6 | FME               | 48.8                | 81.9                | 91.9                 | 2                  | 37.0                | 74.8                | 87.0                 | [BOLD] 2           | 92.70%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+\n|  7 | AME               | [BOLD] 55.5         | [BOLD] 87.9         | [BOLD] 95.2          | [BOLD] 1           | [BOLD] 44.9         | [BOLD] 80.7         | [BOLD] 89.3          | [BOLD] 2           | 84.99%      |\n+----+-------------------+---------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "When using the same amount of 0.2M data, the performance of DCGCN is not necessarily higher than Seq2SeqK and GraphLSTM.",
        "evidence": "We hypothesize that our model performance is not always superior to the baselines, because, for example, our DCGCN(ensemble) model achieves 28.2 BLEU when trained with 0.2M word pairs, which is lower than the best Seq2Seqk and GraphLSTM baselines, which use roughly the same amount of training data. For example, when using 0.1M sentences in our single model, our DCGCN(single) achieves 29.0, which is lower than the best Seq2Seqk and GraphLSTM baselines, but still better than GCNQA and Tree2str. These results suggest that our model may generalize better than the baselines, independent of how much training data is used for fine-tuning.",
        "table": "+----+------------------------------------+-------------------+-------------+\n|    | [BOLD] Model                       | [BOLD] External   | B           |\n+====+====================================+===================+=============+\n|  0 | Seq2SeqK (Konstas et al.,  2017 )  | -                 | 22.0        |\n+----+------------------------------------+-------------------+-------------+\n|  1 | GraphLSTM (Song et al.,  2018 )    | -                 | 23.3        |\n+----+------------------------------------+-------------------+-------------+\n|  2 | GCNSEQ (Damonte and Cohen,  2019 ) | -                 | 24.4        |\n+----+------------------------------------+-------------------+-------------+\n|  3 | DCGCN(single)                      | -                 | 25.9        |\n+----+------------------------------------+-------------------+-------------+\n|  4 | DCGCN(ensemble)                    | -                 | [BOLD] 28.2 |\n+----+------------------------------------+-------------------+-------------+\n|  5 | TSP (Song et al.,  2016 )          | ALL               | 22.4        |\n+----+------------------------------------+-------------------+-------------+\n|  6 | PBMT (Pourdamghani et al.,  2016 ) | ALL               | 26.9        |\n+----+------------------------------------+-------------------+-------------+\n|  7 | Tree2Str (Flanigan et al.,  2016 ) | ALL               | 23.0        |\n+----+------------------------------------+-------------------+-------------+\n|  8 | SNRG (Song et al.,  2017 )         | ALL               | 25.6        |\n+----+------------------------------------+-------------------+-------------+\n|  9 | Seq2SeqK (Konstas et al.,  2017 )  | 0.2M              | 27.4        |\n+----+------------------------------------+-------------------+-------------+\n| 10 | GraphLSTM (Song et al.,  2018 )    | 0.2M              | 28.2        |\n+----+------------------------------------+-------------------+-------------+\n| 11 | DCGCN(single)                      | 0.1M              | 29.0        |\n+----+------------------------------------+-------------------+-------------+\n| 12 | DCGCN(single)                      | 0.2M              | [BOLD] 31.6 |\n+----+------------------------------------+-------------------+-------------+\n| 13 | Seq2SeqK (Konstas et al.,  2017 )  | 2M                | 32.3        |\n+----+------------------------------------+-------------------+-------------+\n| 14 | GraphLSTM (Song et al.,  2018 )    | 2M                | 33.6        |\n+----+------------------------------------+-------------------+-------------+\n| 15 | Seq2SeqK (Konstas et al.,  2017 )  | 20M               | 33.8        |\n+----+------------------------------------+-------------------+-------------+\n| 16 | DCGCN(single)                      | 0.3M              | 33.2        |\n+----+------------------------------------+-------------------+-------------+\n| 17 | DCGCN(ensemble)                    | 0.3M              | [BOLD] 35.3 |\n+----+------------------------------------+-------------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "we observe that the performance of both models decreases as the task becomes more dissimilar to the training data.",
        "evidence": "We observe that the performance of both models decreases as the task becomes dissimilar to the training data. The gain in gain from the coverage mechanism is larger on the in-domain dataset, reflecting the fact that the dataset is relatively easy for both models to handle.",
        "table": "+----+--------------+--------------------+------------------------+\n|    | [EMPTY]      | in-domain SQuAD    | out-of-domain QA-SRL   |\n+====+==============+====================+========================+\n|  0 | [EMPTY]      | F1                 | F1                     |\n+----+--------------+--------------------+------------------------+\n|  1 | MQAN         | 75.37              | 50.10                  |\n+----+--------------+--------------------+------------------------+\n|  2 | +coverage    | <bold>76.83</bold> | <bold>50.89</bold>     |\n+----+--------------+--------------------+------------------------+\n|  3 | BIDAF (ELMO) | 79.76              | 49.98                  |\n+----+--------------+--------------------+------------------------+\n|  4 | +coverage    | <bold>80.15</bold> | <bold>52.43</bold>     |\n+----+--------------+--------------------+------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Adding the dependency weight factor with a window size of 10 decreases the F1 score by 0.7% (A4−A2).",
        "evidence": "We see that adding the dependency weight factor with a window size of 10 decreases the F1 score by 0.7% (A4−A2).",
        "table": "+----+------------------------------------+---------+--------+--------------+\n|    | [EMPTY]                            |   Prec. |   Rec. | F1           |\n+====+====================================+=========+========+==============+\n|  0 | (A1) BiLSTM-CNN                    |   0.473 |  0.606 | 0.531        |\n+----+------------------------------------+---------+--------+--------------+\n|  1 | (A2) Standard attention            |   0.466 |  0.638 | 0.539        |\n+----+------------------------------------+---------+--------+--------------+\n|  2 | (A3) Window size ( [ITALIC] ws)=5  |   0.507 |  0.652 | [BOLD] 0.571 |\n+----+------------------------------------+---------+--------+--------------+\n|  3 | (A4) Window size ( [ITALIC] ws)=10 |   0.51  |  0.64  | 0.568        |\n+----+------------------------------------+---------+--------+--------------+\n|  4 | (A5) Softmax                       |   0.49  |  0.658 | 0.562        |\n+----+------------------------------------+---------+--------+--------------+\n|  5 | (A6) Max-pool                      |   0.492 |  0.6   | 0.541        |\n+----+------------------------------------+---------+--------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "Our ICA framework outperforms the other baselines for all tasks.",
        "evidence": "The results show that our IICA framework is better than other baselines for all of the tasks.Action-wise, our DAMD framework outperforms all other baselines for 5-action generation and 10-action generation for single-action generation. For 10-action generation, our framework gets an improvement of 0.47 points on average compared to the state-of-the-art (i.e., HDSA).",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "[CONTINUE] Regarding the probing tasks, we observe that CBOW embeddings better encode the linguistic properties of sentences than CMOW.",
        "evidence": "From the results, we see that the proposed CBOW and CMOW significantly outperform H-MOW and the hybrid method. We analyze the results further. Overall, we observe that the CBOW embeddings obtained by CMOW are consistently better than the CBOW embeddings. The difference is sometimes much larger than 10%, which shows that the quality of word embeddings is relatively low. CMOW performs better than the CBOW embeddings on noun, verb, adjective, and prepositional words. This is reasonable because the cells of words in a sentence are usually short, which is the case for all embedding algorithms. On the top of all reasoning tasks, the difference is less than 5% when comparing CMOW and CBOW.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] In Librispeech + DEMAND, acoustic supervision (15.6%) and multi-task learning (14.4%) achieves a lower WER than minimizing DCE (15.8%) and FSEGAN (14.9%).",
        "evidence": "We additionally explored the impact of acoustic supervision (15.6% in Librispeech + DEMAND) and multi-task learning (14.4% in Multi-task Learning) on WER. We compare to minimizing DCE (15.8% in Librispeech + DEMAND) and FSEGAN (14.9% in Multi-task Learning) on WER. We observe that minimizing DCE achieves a lower WER than minimizing DCE, and multi-task learning achieves a lower WER than FSEGAN. The results show that acoustic supervision (15.6%) and multi-task learning (14.4%) are both important to achieve a lower WER than minimizing DCE.",
        "table": "+----+-----------------------------------------+-------------+--------------+\n|    | Method                                  | WER (%)     | DCE          |\n+====+=========================================+=============+==============+\n|  0 | No enhancement                          | 17.3        | 0.828        |\n+----+-----------------------------------------+-------------+--------------+\n|  1 | Wiener filter                           | 19.5        | 0.722        |\n+----+-----------------------------------------+-------------+--------------+\n|  2 | Minimizing DCE                          | 15.8        | [BOLD] 0.269 |\n+----+-----------------------------------------+-------------+--------------+\n|  3 | FSEGAN                                  | 14.9        | 0.291        |\n+----+-----------------------------------------+-------------+--------------+\n|  4 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)   | 15.6        | 0.330        |\n+----+-----------------------------------------+-------------+--------------+\n|  5 | AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303        |\n+----+-----------------------------------------+-------------+--------------+\n|  6 | Clean speech                            | 5.7         | 0.0          |\n+----+-----------------------------------------+-------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "At the same time, RELIS performs worse than neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next.",
        "evidence": "For example, we can see that RELIS performs better than the neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next. At the same time, RELIS performs significantly better than the neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next.",
        "table": "+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|    | [EMPTY]   | DUC’01 <italic>R</italic>1   | DUC’01 <italic>R</italic>2   | DUC’02 <italic>R</italic>1   | DUC’02 <italic>R</italic>2   | DUC’04 <italic>R</italic>1   | DUC’04 <italic>R</italic>2   |\n+====+===========+==============================+==============================+==============================+==============================+==============================+==============================+\n|  0 | ICSI      | 33.31                        | 7.33                         | 35.04                        | 8.51                         | 37.31                        | 9.36                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  1 | PriorSum  | 35.98                        | 7.89                         | 36.63                        | 8.97                         | 38.91                        | 10.07                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  2 | TCSum     | <bold>36.45</bold>           | 7.66                         | 36.90                        | 8.61                         | 38.27                        | 9.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  3 | TCSum−    | 33.45                        | 6.07                         | 34.02                        | 7.39                         | 35.66                        | 8.66                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  4 | SRSum     | 36.04                        | 8.44                         | <bold>38.93</bold>           | <bold>10.29</bold>           | 39.29                        | 10.70                        |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  5 | DeepTD    | 28.74                        | 5.95                         | 31.63                        | 7.09                         | 33.57                        | 7.96                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  6 | REAPER    | 32.43                        | 6.84                         | 35.03                        | 8.11                         | 37.22                        | 8.64                         |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n|  7 | RELIS     | 34.73                        | <bold>8.66</bold>            | 37.11                        | 9.12                         | <bold>39.34</bold>           | <bold>10.73</bold>           |\n+----+-----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+",
        "label": "refutes"
    },
    {
        "claim": "When the experiment was repeated so that the finetuning phase included the text-only data, the performance returned to approximately the same level as without tuning (+multi-modal finetune row in Table 6).",
        "evidence": "We show that the effect of multi-modal finetuning was significant. Intuitively, this indicates that the neural subsets can be improved without fine-tuning other hyperparameters. We then repeated the experiment so that the finetuning phase included the text-only data. The results (see “Visual Features” row) show that multi-modal finetune has different effects on different tasks and languages. Flickr17 and MSCOCO17 image datasets differ from each other in that the multi-modal finetune phase is more effective in English (en-de) than in German (en-fr) and MSCOCO17 (en-fr). We believe that this is due to the different visual features in these tasks. In any case, more work needs to be done to understand the importance of different visual features for the tasks.",
        "table": "+----+----------------------------------------+--------------+--------------+--------------+\n|    | en-fr                                  | flickr16     | flickr17     | mscoco17     |\n+====+========================================+==============+==============+==============+\n|  0 | subs3M [ITALIC]  [ITALIC] LM detectron | 68.30        | 62.45        | 52.86        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  1 | +ensemble-of-3                         | 68.72        | 62.70        | 53.06        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  2 | −visual features                       | [BOLD] 68.74 | [BOLD] 62.71 | 53.14        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  3 | −MS-COCO                               | 67.13        | 61.17        | [BOLD] 53.34 |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  4 | −multi-lingual                         | 68.21        | 61.99        | 52.40        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  5 | subs6M [ITALIC]  [ITALIC] LM detectron | 68.29        | 61.73        | 53.05        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  6 | subs3M [ITALIC]  [ITALIC] LM gn2048    | 67.74        | 61.78        | 52.76        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  7 | subs3M [ITALIC]  [ITALIC] LM text-only | 67.72        | 61.75        | 53.02        |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  8 | en-de                                  | flickr16     | flickr17     | mscoco17     |\n+----+----------------------------------------+--------------+--------------+--------------+\n|  9 | subs3M [ITALIC]  [ITALIC] LM detectron | 45.09        | 40.81        | 36.94        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 10 | +ensemble-of-3                         | 45.52        | [BOLD] 41.84 | [BOLD] 37.49 |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 11 | −visual features                       | [BOLD] 45.59 | 41.75        | 37.43        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 12 | −MS-COCO                               | 45.11        | 40.52        | 36.47        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 13 | −multi-lingual                         | 44.95        | 40.09        | 35.28        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 14 | subs6M [ITALIC]  [ITALIC] LM detectron | 45.50        | 41.01        | 36.81        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 15 | subs3M [ITALIC]  [ITALIC] LM gn2048    | 45.38        | 40.07        | 36.82        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 16 | subs3M [ITALIC]  [ITALIC] LM text-only | 44.87        | 41.27        | 36.59        |\n+----+----------------------------------------+--------------+--------------+--------------+\n| 17 | +multi-modal finetune                  | 44.56        | 41.61        | 36.93        |\n+----+----------------------------------------+--------------+--------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms.",
        "evidence": "Beck et al. In particular, we find that DCGCN consistently outperforms the gating mechanism-based model on all three measures, while achieving similar accuracy under the same setting. Since the DCGCN model achieves best performance in all three measures, this suggests that its design is consistently effective for graph encoders. We leave further exploration of this phenomenon to future work.",
        "table": "+----+--------------------------------+------------+--------------+-------------+-------------+\n|    | [BOLD] Model                   | [BOLD] T   | #P           | B           | C           |\n+====+================================+============+==============+=============+=============+\n|  0 | Seq2SeqB (Beck et al.,  2018 ) | S          | 28,4M        | 21.7        | 49.1        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  1 | GGNN2Seq (Beck et al.,  2018 ) | S          | 28.3M        | 23.3        | 50.4        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  2 | Seq2SeqB (Beck et al.,  2018 ) | E          | 142M         | 26.6        | 52.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  3 | GGNN2Seq (Beck et al.,  2018 ) | E          | 141M         | 27.5        | 53.5        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  4 | DCGCN (ours)                   | S          | [BOLD] 19.1M | 27.9        | 57.3        |\n+----+--------------------------------+------------+--------------+-------------+-------------+\n|  5 | DCGCN (ours)                   | E          | 92.5M        | [BOLD] 30.4 | [BOLD] 59.6 |\n+----+--------------------------------+------------+--------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions.",
        "evidence": "However, the results in bottom half of Table 2 and 3 show the main effect from cleaning the missed slots is to reduce the insertion and deletions. From the results, we can see that after cleaning the generated data, the SER has dramatically improved, In particular, from 1.5PT/2PT]3-12 to 1.CIDEr[0.5pt/2PT]3-12, TGen+ has a further improvement by +8.53% on BLEU, +5.91% on METEOR, +7.51% on ROUGE-L and +5.19% on CIDEr. The results here agree with the previous analysis.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "The results of using NeuralTD to generate summaries are in the bottommost row; the overall F-score is only lower by 1.4 for each metric.",
        "evidence": "Here, we show the experiment results that were omitted from the main paper due to the space limit. Our model obtains an F-score of 36.5, which is slightly lower than the best result reported by Zhang et al. Nevertheless, neuralTD outperforms all of the previous models in all metrics except for the perplexity on ROUGE-L.",
        "table": "+----+----------------------------+----------+-------+-------+-------+\n|    | System                     | Reward   |   R-1 |   R-2 |   R-L |\n+====+============================+==========+=======+=======+=======+\n|  0 | Kryscinski et al. ( 2018 ) | R-L      |  40.2 |  17.4 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  1 | Narayan et al. ( 2018b )   | R-1,2,L  |  40   |  18.2 |  36.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  2 | Chen and Bansal ( 2018 )   | R-L      |  41.5 |  18.7 |  37.8 |\n+----+----------------------------+----------+-------+-------+-------+\n|  3 | Dong et al. ( 2018 )       | R-1,2,L  |  41.5 |  18.7 |  37.6 |\n+----+----------------------------+----------+-------+-------+-------+\n|  4 | Zhang et al. ( 2018 )      | [EMPTY]  |  41.1 |  18.8 |  37.5 |\n+----+----------------------------+----------+-------+-------+-------+\n|  5 | Zhou et al. ( 2018 )       | [EMPTY]  |  41.6 |  19   |  38   |\n+----+----------------------------+----------+-------+-------+-------+\n|  6 | Kedzie et al. ( 2018 )     | [EMPTY]  |  39.1 |  17.9 |  35.9 |\n+----+----------------------------+----------+-------+-------+-------+\n|  7 | (ours) NeuralTD            | Learned  |  39.6 |  18.1 |  36.5 |\n+----+----------------------------+----------+-------+-------+-------+",
        "label": "not enough info"
    },
    {
        "claim": "the joint-training strategy has more significant performance gains in recall from 0.1 to 0.4 than the fine-tuning strategy.",
        "evidence": "From the table, we can see that the fine-tuning strategy has more significant performance gains in recall from 0.1 to 0.4 than the training strategy.",
        "table": "+----+------------+-------+--------------+--------------+--------------+--------------+\n|    | Recall     |   0.1 | 0.2          | 0.3          | 0.4          | AUC          |\n+====+============+=======+==============+==============+==============+==============+\n|  0 | PCNN+ATT   | 0.698 | 0.606        | 0.518        | 0.446        | 0.323        |\n+----+------------+-------+--------------+--------------+--------------+--------------+\n|  1 | Rank+ExATT | 0.789 | 0.726        | 0.620        | 0.514        | 0.395        |\n+----+------------+-------+--------------+--------------+--------------+--------------+\n|  2 | Our Model  | 0.788 | [BOLD] 0.743 | [BOLD] 0.654 | [BOLD] 0.546 | [BOLD] 0.397 |\n+----+------------+-------+--------------+--------------+--------------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "This is expected as the joint model introduces a greater capacity to the model and, therefore, can deal with more complex entity coreference.",
        "evidence": "The results show that the joint model performs better than the disjoint model. This improvement can be expected as the joint model introduces a greater capacity to the model and therefore, can deal with more complex entity coreference.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "DAMD (generated actions)  is the state-of-the-art for combining action modeling and belief state augmentation for task-oriented response generation.",
        "evidence": "Our model DAMD (Generated Actions) is the state-of-the-art when it comes to combining action modeling and belief state augmentation. [CONTINUE] The first three lines of the table show the results of using generated actions as the initial belief state representation and the subsequent use of multi-action data augmentation for task-oriented response generation. [CONTINUE] Moreover, even without using multi-action data augmentation, our model significantly outperforms other approaches on task-oriented response generation.",
        "table": "+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|    | Model                                           | Belief State Type   | System Action Type   | System Action Form   | Inform (%)   | Success (%)   | BLEU        | Combined Score   |\n+====+=================================================+=====================+======================+======================+==============+===============+=============+==================+\n|  0 | 1. Seq2Seq + Attention                          | oracle              | -                    | -                    | 71.3         | 61.0          | [BOLD] 18.9 | 85.1             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  1 | 2. Seq2Seq + Copy                               | oracle              | -                    | -                    | 86.2         | [BOLD] 72.0   | 15.7        | 94.8             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  2 | 3. MD-Sequicity                                 | oracle              | -                    | -                    | [BOLD] 86.6  | 71.6          | 16.8        | [BOLD] 95.9      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  3 | 4. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | generated            | one-hot              | 82.7         | 72.1          | 16.3        | 93.7             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  4 | 5. HDSA                                         | oracle              | generated            | graph                | 82.9         | 68.9          | [BOLD] 23.6 | 99.5             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  5 | 6. DAMD                                         | oracle              | generated            | span                 | [BOLD] 89.5  | 75.8          | 18.3        | 100.9            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  6 | 7. DAMD + multi-action data augmentation        | oracle              | generated            | span                 | 89.2         | [BOLD] 77.9   | 18.6        | [BOLD] 102.2     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  7 | 8. SFN + RL (Mehri et al. mehri2019structured)  | oracle              | oracle               | one-hot              | -            | -             | 29.0        | 106.0            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  8 | 9. HDSA                                         | oracle              | oracle               | graph                | 87.9         | 78.0          | [BOLD] 30.4 | 113.4            |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n|  9 | 10. DAMD + multi-action data augmentation       | oracle              | oracle               | span                 | [BOLD] 95.4  | [BOLD] 87.2   | 27.3        | [BOLD] 118.5     |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 10 | 11. SFN + RL (Mehri et al. mehri2019structured) | generated           | generated            | one-hot              | 73.8         | 58.6          | [BOLD] 16.9 | 83.0             |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+\n| 11 | 12. DAMD + multi-action data augmentation       | generated           | generated            | span                 | [BOLD] 76.3  | [BOLD] 60.4   | 16.6        | [BOLD] 85.0      |\n+----+-------------------------------------------------+---------------------+----------------------+----------------------+--------------+---------------+-------------+------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Again, one possible explanation is that cleaning the missing slots provided more complex training examples.",
        "evidence": "The experiment was run for 25 epochs with a patience of 30, and the results reported in the table are for the best performing model. From the table, we can see that the performance of TGen that was separately trained and tested on the original data is much higher than TGen+ and SC-LSTM, while the performance of SC-LSTM improved only by a small amount. This might mean that training the model that was trained on the missing slots helps to forget the language model bias, which can be explained by the fact that those slots are in the middle of the text. Also, for models that were trained on the missing data, the performance on adding the generated slot values was not very significant. One possible explanation is that training on the missing slots helps to forget the language model bias, which can be explained by the fact that those slots are in the middle of the text.",
        "table": "+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|    | Train                                       | Test           | [BOLD] System   |   [BOLD] BLEU |   [BOLD] NIST |   [BOLD] METEOR |   [BOLD] ROUGE-L |   [BOLD] CIDEr |   [BOLD] Add |   [BOLD] Miss |   [BOLD] Wrong |   [BOLD] SER |\n+====+=============================================+================+=================+===============+===============+=================+==================+================+==============+===============+================+==============+\n|  0 | Original                                    | [BOLD] Cleaned | TGen−           |         36.85 |        5.3782 |           35.14 |            55.01 |         1.6016 |         0.34 |          9.81 |           0.15 |        10.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  1 | Original                                    | [BOLD] Cleaned | TGen            |         39.23 |        6.0217 |           36.97 |            55.52 |         1.7623 |         0.4  |          3.59 |           0.07 |         4.05 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  2 | Original                                    | [BOLD] Cleaned | TGen+           |         40.25 |        6.1448 |           37.5  |            56.19 |         1.8181 |         0.21 |          1.99 |           0.05 |         2.24 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  3 | Original                                    | [BOLD] Cleaned | SC-LSTM         |         23.88 |        3.931  |           32.11 |            39.9  |         0.5036 |         7.73 |         17.76 |           9.52 |        35.03 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  4 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen−           |         40.19 |        6.0543 |           37.38 |            55.88 |         1.8104 |         0.17 |          1.31 |           0.25 |         1.72 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  5 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen            |         40.73 |        6.1711 |           37.76 |            56.09 |         1.8518 |         0.07 |          0.72 |           0.08 |         0.87 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  6 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | TGen+           |         40.51 |        6.1226 |           37.61 |            55.98 |         1.8286 |         0.02 |          0.63 |           0.06 |         0.7  |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  7 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned       | [BOLD] Cleaned | SC-LSTM         |         23.66 |        3.9511 |           32.93 |            39.29 |         0.3855 |         7.89 |         15.6  |           8.44 |        31.94 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  8 | Cleaned missing                             | [BOLD] Cleaned | TGen−           |         40.48 |        6.0269 |           37.26 |            56.19 |         1.7999 |         0.43 |          2.84 |           0.26 |         3.52 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n|  9 | Cleaned missing                             | [BOLD] Cleaned | TGen            |         41.57 |        6.283  |           37.99 |            56.36 |         1.8849 |         0.37 |          1.4  |           0.09 |         1.86 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 10 | Cleaned missing                             | [BOLD] Cleaned | TGen+           |         41.56 |        6.27   |           37.94 |            56.38 |         1.8827 |         0.21 |          1.04 |           0.07 |         1.31 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 11 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen−           |         35.99 |        5.0734 |           34.74 |            54.79 |         1.5259 |         0.02 |         11.58 |           0.02 |        11.62 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 12 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen            |         40.07 |        6.1243 |           37.45 |            55.81 |         1.8026 |         0.05 |          3.23 |           0.01 |         3.29 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+\n| 13 | 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+           |         40.8  |        6.2197 |           37.86 |            56.13 |         1.8422 |         0.01 |          1.87 |           0.01 |         1.88 |\n+----+---------------------------------------------+----------------+-----------------+---------------+---------------+-----------------+------------------+----------------+--------------+---------------+----------------+--------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters.",
        "evidence": "The graphs with smaller diameter have smaller out-of-degrees, which leads to better performance for the models. Especially, the G2S-GAT models have smaller graphs with smaller out-of-degrees, yet their performance is still better than the S2S models on small graphs.",
        "table": "+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|    | <bold>Model</bold>   | <bold>Graph Diameter</bold> 0-7 Δ   | <bold>Graph Diameter</bold> 7-13 Δ   | <bold>Graph Diameter</bold> 14-20 Δ   |\n+====+======================+=====================================+======================================+=======================================+\n|  0 | S2S                  | 33.2                                | 29.7                                 | 28.8                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  1 | G2S-GIN              | 35.2 +6.0%                          | 31.8 +7.4%                           | 31.5 +9.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  2 | G2S-GAT              | 35.1 +5.9%                          | 32.0 +7.8%                           | 31.5 +9.51%                           |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  3 | G2S-GGNN             | 36.2 +9.0%                          | 33.0 +11.4%                          | 30.7 +6.7%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  4 | [EMPTY]              | <bold>Sentence Length</bold>        | <bold>Sentence Length</bold>         | <bold>Sentence Length</bold>          |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  5 | [EMPTY]              | 0-20 Δ                              | 20-50 Δ                              | 50-240 Δ                              |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  6 | S2S                  | 34.9                                | 29.9                                 | 25.1                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  7 | G2S-GIN              | 36.7 +5.2%                          | 32.2 +7.8%                           | 26.5 +5.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  8 | G2S-GAT              | 36.9 +5.7%                          | 32.3 +7.9%                           | 26.6 +6.1%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n|  9 | G2S-GGNN             | 37.9 +8.5%                          | 33.3 +11.2%                          | 26.9 +6.8%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 10 | [EMPTY]              | <bold>Max Node Out-degree</bold>    | <bold>Max Node Out-degree</bold>     | <bold>Max Node Out-degree</bold>      |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 11 | [EMPTY]              | 0-3 Δ                               | 4-8 Δ                                | 9-18 Δ                                |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 12 | S2S                  | 31.7                                | 30.0                                 | 23.9                                  |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 13 | G2S-GIN              | 33.9 +6.9%                          | 32.1 +6.9%                           | 25.4 +6.2%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 14 | G2S-GAT              | 34.3 +8.0%                          | 32.0 +6.7%                           | 22.5 -6.0%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+\n| 15 | G2S-GGNN             | 35.0 +10.3%                         | 33.1 +10.4%                          | 22.2 -7.3%                            |\n+----+----------------------+-------------------------------------+--------------------------------------+---------------------------------------+",
        "label": "supports"
    },
    {
        "claim": "On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points.",
        "evidence": "Our hybrid model (CMOW/784) outperforms CBOW/784 by 2.5 points in the SICK-R task. On the TREC task, our hybrid model also outperforms the baselines by a large margin. CMOW and CBOW are on par with each other on most tasks, and they both perform worse than our hybrid model on SUBJ.",
        "table": "+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method    | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CBOW/784  | 90.0        | [BOLD] 79.2 | [BOLD] 74.0 | 87.1        | 71.6        | 85.6        | 78.9        | 78.5        | 42.1        | 61.0        | [BOLD] 78.1 |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW/784  | 87.5        | 73.4        | 70.6        | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | 77.2        | 74.7        | 37.9        | 56.5        | 76.2        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | Hybrid    | [BOLD] 90.2 | 78.7        | 73.7        | [BOLD] 87.3 | [BOLD] 72.7 | 87.6        | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8        |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | cmp. CBOW | +0.2%       | -0.6%       | -0.4%       | +0.2%       | +1.5%       | +2.3%       | +0.6%       | +1.4%       | +2.9%       | +3.9%       | -0.4%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | cmp. CMOW | +3.1%       | +7.2%       | +4.4%       | +0%         | +4.5%       | -0.5%       | +2.9%       | +6.7%       | +14.3       | +12.2%      | +2.1%       |\n+----+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Although the PRKGC+NS model receives supervision about human-generated NLDs, paths with the maximum score do not match human-generated NLDs to any significant extent.",
        "evidence": "This claim is significant because the maximum score does not match human-generated NLDs to any significant extent. Also, the PRKGC+NS model performs better than the PRKGC model in all evaluation metrics. This is because the PRKGC+NS model learns to generate paths that match human-generated NLDs at the same time. Paths with the maximum score do not match human-generated NLDs to any significant extent.",
        "table": "+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|    | Model         | Answerability Macro P/R/F   | # Answerable   |   Answer Prec. | Derivation Prec. RG-L (P/R/F)   |   Derivation Prec. BL-4 |\n+====+===============+=============================+================+================+=================================+=========================+\n|  0 | Shortest Path | 54.8/55.5/53.2              | 976            |            3.6 | 56.7/38.5/41.5                  |                    31.3 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  1 | PRKGC         | 52.6/51.5/50.7              | 1,021          |           45.2 | 40.7/60.7/44.7                  |                    30.9 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+\n|  2 | PRKGC+NS      | 53.6/54.1/52.1              | 980            |           45.4 | 42.2/61.6/46.1                  |                    33.4 |\n+----+---------------+-----------------------------+----------------+----------------+---------------------------------+-------------------------+",
        "label": "refutes"
    },
    {
        "claim": "On the other side, H-CMOW shows, among others, improvements at BShift.",
        "evidence": "Among these, H-cmow shows, among others, the best performance on noun, verb, and adjective tuples. Because they are nouns, they are easier to predict and thus have a much larger training set than the other classes. Also, they are nouns, which are often more frequent words than the other classes. While the other two tasks require relatively large vocabulary sizes, the word level tasks do not. So, the word level task is easier than the noun level task and the word level task is not directly related to the performance of the word-level tasks.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "Surprisingly, GDPL outperforms human in completing the task, and its average dialog turns are even lower than those of humans, though GDPL is superior in terms of match rate.",
        "evidence": "This claim is very surprising as a single model usually performs the best. Specifically, GDPL achieves a goal success rate of 86.5%, which is 5.9% higher than human, and 13.1% higher than per-discorrect ALDM. Its average dialog turns are even lower than those of humans. GDPL also achieves a better match rate than humans, and its average dialogs are also 5% higher than those of humans.",
        "table": "+----+----------------+----------------+-----------------+----------------+------------------+\n|    | Method         | Agenda Turns   | Agenda Inform   | Agenda Match   | Agenda Success   |\n+====+================+================+=================+================+==================+\n|  0 | GP-MBCM        | 2.99           | 19.04           | 44.29          | 28.9             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  1 | ACER           | 10.49          | 77.98           | 62.83          | 50.8             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  2 | PPO            | 9.83           | 83.34           | 69.09          | 59.1             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  3 | ALDM           | 12.47          | 81.20           | 62.60          | 61.2             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  4 | GDPL-sess      | [BOLD] 7.49    | 88.39           | 77.56          | 76.4             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  5 | GDPL-discr     | 7.86           | 93.21           | 80.43          | 80.5             |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  6 | GDPL           | 7.64           | [BOLD] 94.97    | [BOLD] 83.90   | [BOLD] 86.5      |\n+----+----------------+----------------+-----------------+----------------+------------------+\n|  7 | [ITALIC] Human | [ITALIC] 7.37  | [ITALIC] 66.89  | [ITALIC] 95.29 | [ITALIC] 75.0    |\n+----+----------------+----------------+-----------------+----------------+------------------+",
        "label": "refutes"
    },
    {
        "claim": "We find that the performance does not reach the best when iteration is set to 3.",
        "evidence": "We compare the performance between the one performing iteration and the others. We find that the performance reach the best when iteration is set to 3.",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "refutes"
    },
    {
        "claim": "[CONTINUE] We also observe that WMD-BIGRAMS slightly outperforms WMD-UNIGRAMS on 3 out of 4 language pairs.",
        "evidence": "The claim that WMD-bigrams is better than WMD-unigrams is supported by the experiments. Among all the language pairs, we observe that WMD-bigrams is slightly better than WMD-unigrams on 3 out of 4 language pairs.",
        "table": "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    | Metrics            | cs-en              | de-en              | fi-en              | lv-en              |\n+====+====================+====================+====================+====================+====================+\n|  0 | RUSE               | 0.624              | 0.644              | 0.750              | 0.697              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  1 | Hmd-F1 + BERT      | 0.655              | 0.681              | 0.821              | 0.712              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  2 | Hmd-Recall + BERT  | 0.651              | 0.658              | 0.788              | 0.681              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  3 | Hmd-Prec + BERT    | 0.624              | 0.669              | 0.817              | 0.707              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  4 | Wmd-unigram + BERT | 0.651              | 0.686              | <bold>0.823</bold> | 0.710              |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|  5 | Wmd-bigram + BERT  | <bold>0.665</bold> | <bold>0.688</bold> | 0.821              | <bold>0.712</bold> |\n+----+--------------------+--------------------+--------------------+--------------------+--------------------+",
        "label": "supports"
    },
    {
        "claim": "Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised [CONTINUE] downstream tasks [CONTINUE] On average over all downstream tasks, the relative improvement is 20.8%.",
        "evidence": "The task is to summarize the information from the claim section. Overall, we find that CMOW-R obtains better performance than CMOW-C on 10 out of 11 supervised tasks. The relative improvement is 20.8%. Further, we can find that on average, CMOW-R and CBOW-R are on par with each other.",
        "table": "+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Method   | SUBJ        | CR          | MR          | MPQA        | MRPC        | TREC        | SICK-E      | SST2        | SST5        | STS-B       | SICK-R      |\n+====+==========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | CMOW-C   | 85.9        | 72.1        | 69.4        | 87.0        | [BOLD] 71.9 | 85.4        | 74.2        | 73.8        | 37.6        | 54.6        | 71.3        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | CMOW-R   | [BOLD] 87.5 | [BOLD] 73.4 | [BOLD] 70.6 | [BOLD] 87.3 | 69.6        | [BOLD] 88.0 | [BOLD] 77.2 | [BOLD] 74.7 | [BOLD] 37.9 | [BOLD] 56.5 | [BOLD] 76.2 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | CBOW-C   | [BOLD] 90.0 | [BOLD] 79.3 | [BOLD] 74.6 | [BOLD] 87.5 | [BOLD] 72.9 | 85.0        | [BOLD] 80.0 | 78.4        | 41.0        | 60.5        | [BOLD] 79.2 |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | CBOW-R   | [BOLD] 90.0 | 79.2        | 74.0        | 87.1        | 71.6        | [BOLD] 85.6 | 78.9        | [BOLD] 78.5 | [BOLD] 42.1 | [BOLD] 61.0 | 78.1        |\n+----+----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased.",
        "evidence": "Similar to the observations on dim, a higher score indicates word content is better represented. In other words, the level of closeness between input and output space is increased, indicating that the proposed method can help words to be more specific.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "we see that analogical reasoning abilities of the learned embeddings are almost close to the distributed word representations.",
        "evidence": "We present the results in Table. It can be seen that the results obtained with our method are almost close to the distributed word representations. In other words, the sparse embeddings learned by our method are close to the raw word embeddings. We analyze that the sparse embeddings learned by our method have better semantic performance than the word2vec and GloVe embeddings.",
        "table": "+----+------------+----------+----------------+----------------+---------+\n|    | Methods    |   # dims |   Analg. (sem) |   Analg. (syn) |   Total |\n+====+============+==========+================+================+=========+\n|  0 | GloVe      |      300 |          78.94 |          64.12 |   70.99 |\n+----+------------+----------+----------------+----------------+---------+\n|  1 | Word2Vec   |      300 |          81.03 |          66.11 |   73.03 |\n+----+------------+----------+----------------+----------------+---------+\n|  2 | OIWE-IPG   |      300 |          19.99 |          23.44 |   21.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  3 | SOV        |     3000 |          64.09 |          46.26 |   54.53 |\n+----+------------+----------+----------------+----------------+---------+\n|  4 | SPINE      |     1000 |          17.07 |           8.68 |   12.57 |\n+----+------------+----------+----------------+----------------+---------+\n|  5 | Word2Sense |     2250 |          12.94 |          19.44 |    5.84 |\n+----+------------+----------+----------------+----------------+---------+\n|  6 | Proposed   |      300 |          79.96 |          63.52 |   71.15 |\n+----+------------+----------+----------------+----------------+---------+",
        "label": "not enough info"
    },
    {
        "claim": "However, models trained using linguistic features on the training data do not obtain significantly higher predictive accuracy.",
        "evidence": "This is consistent with the claim that neural networks are not effective for this task. Specifically, the best performing model is the LSTM according to [CONTINUE] the results obtained using all features of the model are comparable to that of the best performing feature-based model, i.e., word2vec clusters do not outperform LSTM in terms of accuracy. The best performing feature-based model is the bag-of-words model according to [CONTINUE] the results obtained with linguistic features of the model are slightly lower than those obtained with all features of the model. Word2vec clusters, which use syntactic dependency information, are the best performing feature-based model in terms of F1 and AUC. In comparison, our neural networks, which are deep neural networks, obtained significantly higher predictive accuracy than the feature-based models.",
        "table": "+----+--------------------------+--------------+-------------+--------------+\n|    | [BOLD] Model             | [BOLD] Acc   | [BOLD] F1   | [BOLD] AUC   |\n+====+==========================+==============+=============+==============+\n|  0 | Most Frequent Class      | 64.2         | 39.1        | 0.500        |\n+----+--------------------------+--------------+-------------+--------------+\n|  1 | Logistic Regression      | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n|  2 | Sentiment – MPQA         | 64.2         | 39.1        | 0.499        |\n+----+--------------------------+--------------+-------------+--------------+\n|  3 | Sentiment – NRC          | 63.9         | 42.2        | 0.599        |\n+----+--------------------------+--------------+-------------+--------------+\n|  4 | Sentiment – V&B          | 68.9         | 60.0        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  5 | Sentiment – VADER        | 66.0         | 54.2        | 0.654        |\n+----+--------------------------+--------------+-------------+--------------+\n|  6 | Sentiment – Stanford     | 68.0         | 55.6        | 0.696        |\n+----+--------------------------+--------------+-------------+--------------+\n|  7 | Complaint Specific (all) | 65.7         | 55.2        | 0.634        |\n+----+--------------------------+--------------+-------------+--------------+\n|  8 | Request                  | 64.2         | 39.1        | 0.583        |\n+----+--------------------------+--------------+-------------+--------------+\n|  9 | Intensifiers             | 64.5         | 47.3        | 0.639        |\n+----+--------------------------+--------------+-------------+--------------+\n| 10 | Downgraders              | 65.4         | 49.8        | 0.615        |\n+----+--------------------------+--------------+-------------+--------------+\n| 11 | Temporal References      | 64.2         | 43.7        | 0.535        |\n+----+--------------------------+--------------+-------------+--------------+\n| 12 | Pronoun Types            | 64.1         | 39.1        | 0.545        |\n+----+--------------------------+--------------+-------------+--------------+\n| 13 | POS Bigrams              | 72.2         | 66.8        | 0.756        |\n+----+--------------------------+--------------+-------------+--------------+\n| 14 | LIWC                     | 71.6         | 65.8        | 0.784        |\n+----+--------------------------+--------------+-------------+--------------+\n| 15 | Word2Vec Clusters        | 67.7         | 58.3        | 0.738        |\n+----+--------------------------+--------------+-------------+--------------+\n| 16 | Bag-of-Words             | 79.8         | 77.5        | 0.866        |\n+----+--------------------------+--------------+-------------+--------------+\n| 17 | All Features             | [BOLD] 80.5  | [BOLD] 78.0 | [BOLD] 0.873 |\n+----+--------------------------+--------------+-------------+--------------+\n| 18 | Neural Networks          | [EMPTY]      | [EMPTY]     | [EMPTY]      |\n+----+--------------------------+--------------+-------------+--------------+\n| 19 | MLP                      | 78.3         | 76.2        | 0.845        |\n+----+--------------------------+--------------+-------------+--------------+\n| 20 | LSTM                     | 80.2         | 77.0        | 0.864        |\n+----+--------------------------+--------------+-------------+--------------+",
        "label": "refutes"
    },
    {
        "claim": "RSI  “119.99”  requires  “RSI  <  120.00”  and RSI = `89.20` therefore does not require.",
        "evidence": "We believe the proposed method does not require human interpretability because it operates at the word level. We believe the performance measure is easily achieved using off-the-shelf memory cards as well.",
        "table": "+----+---------------------+----------------+---------------------+\n|    | [EMPTY]             | GloVe          | Imparted            |\n+====+=====================+================+=====================+\n|  0 | Participants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242 |\n+----+---------------------+----------------+---------------------+\n|  1 | Mean/Std            | 85/6.9         | 212/24.4            |\n+----+---------------------+----------------+---------------------+",
        "label": "not enough info"
    },
    {
        "claim": "from this Table, we can clearly see the effect of exploring hierarchical structure is more significant at higher recall rates, so we can improve performance via attention mechanism at higher recall rate.",
        "evidence": "From this table, we can see that exploring hierarchical structure is more significant at higher recall rates, so we can improve performance via attention mechanism at higher recall rate..",
        "table": "+----+--------------+-------+-------+-------+--------------+\n|    | Recall       |   0.1 |   0.2 |   0.3 | AUC          |\n+====+==============+=======+=======+=======+==============+\n|  0 | Rank+ExATT   | 0.584 | 0.535 | 0.487 | 0.392        |\n+----+--------------+-------+-------+-------+--------------+\n|  1 | PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204        |\n+----+--------------+-------+-------+-------+--------------+\n|  2 | PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396        |\n+----+--------------+-------+-------+-------+--------------+\n|  3 | Our Model    | 0.65  | 0.519 | 0.422 | [BOLD] 0.405 |\n+----+--------------+-------+-------+-------+--------------+",
        "label": "not enough info"
    },
    {
        "claim": "We find that the performance reach the best when iteration is set to 3.",
        "evidence": "We then evaluate the impact of iteration on the performance of the predictive model. We find that the performance reach the best when iteration is set to 3.",
        "table": "+----+-------------+-------+-------+-------+-------+\n|    | Recall      |   0.1 |   0.2 |   0.3 |   AUC |\n+====+=============+=======+=======+=======+=======+\n|  0 | Iteration=1 | 0.531 | 0.455 | 0.353 | 0.201 |\n+----+-------------+-------+-------+-------+-------+\n|  1 | Iteration=2 | 0.592 | 0.498 | 0.385 | 0.375 |\n+----+-------------+-------+-------+-------+-------+\n|  2 | Iteration=3 | 0.65  | 0.519 | 0.422 | 0.405 |\n+----+-------------+-------+-------+-------+-------+\n|  3 | Iteration=4 | 0.601 | 0.505 | 0.422 | 0.385 |\n+----+-------------+-------+-------+-------+-------+\n|  4 | Iteration=5 | 0.575 | 0.495 | 0.394 | 0.376 |\n+----+-------------+-------+-------+-------+-------+",
        "label": "supports"
    },
    {
        "claim": "we also removed the duplicate mentions identified by the lemmatisation-based method (reduced), and the effect was to boost cross-document results on the best of these sets (Joint+reduced) by a further 0.9% for all measures.",
        "evidence": "We removed the duplicate mentions identified by the lemmatisation-based method. We also boosted cross-document results on the best of the clusters by a further 0.9% for all measures.",
        "table": "+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|    | <bold>Model</bold>   |    R |   MUC P |   <italic>F</italic>1 |   B3 P |   CEAF-<italic>e</italic> P | CoNLL <italic>F</italic>1   |\n+====+======================+======+=========+=======================+========+=============================+=============================+\n|  0 | Cluster+Lemma        | 70.1 |    83   |                  60   |   84.9 |                        52.5 | 67.4                        |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  1 | Disjoint             | 65.3 |    80.8 |                  61.6 |   78.2 |                        58.3 | 70                          |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+\n|  2 | Joint                | 65.4 |    80.9 |                  63.3 |   76.4 |                        61.3 | <bold>71.2</bold>           |\n+----+----------------------+------+---------+-----------------------+--------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "compared to GloVe and Word2Vec, our sense-based distributed representations can be considered as an initial attempt to incorporate sense-level information.",
        "evidence": "Our Word2Sense system outperforms the GloVe system across all the evaluated datasets. It can be considered as an initial attempt to incorporate sense-level information. The results show that incorporating sense information has significantly better performance than oiwe-ipg and spine embeddings.",
        "table": "+----+---------+------------+------------+-------+---------+--------------+------------+\n|    |   GloVe |   Word2Vec |   OIWE-IPG |   SOV |   SPINE |   Word2Sense |   Proposed |\n+====+=========+============+============+=======+=========+==============+============+\n|  0 |   77.34 |      77.91 |      74.27 | 78.43 |   74.13 |        81.21 |      78.26 |\n+----+---------+------------+------------+-------+---------+--------------+------------+",
        "label": "not enough info"
    },
    {
        "claim": "BERT cosine performs the best.",
        "evidence": "From the claims in the table we can see that BERT performs the best compared to the other methods. Specifically, BERT-Cosine achieves .312 Pearson r over all the metrics, which is .9 more than Infersent-Cosine and .5 more than BERT-SU4. It is clear that BERT-Cosine achieves the best performance compared to all the other methods.",
        "table": "+----+------------------+--------------+--------------+-------------+-------------+\n|    | Metric           | [ITALIC] ρ   | [ITALIC] r   | G-Pre       | G-Rec       |\n+====+==================+==============+==============+=============+=============+\n|  0 | ROUGE-1          | .290         | .304         | .392        | .428        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  1 | ROUGE-2          | .259         | .278         | .408        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  2 | ROUGE-L          | .274         | .297         | .390        | .426        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  3 | ROUGE-SU4        | .282         | .279         | .404        | .440        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  4 | BLEU-1           | .256         | .281         | .409        | .448        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  5 | BLEU-2           | .301         | .312         | .411        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  6 | BLEU-3           | .317         | .312         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  7 | BLEU-4           | .311         | .307         | .409        | .446        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  8 | BLEU-5           | .308         | .303         | .420        | .459        |\n+----+------------------+--------------+--------------+-------------+-------------+\n|  9 | METEOR           | .305         | .285         | .409        | .444        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 10 | InferSent-Cosine | [BOLD] .329  | [BOLD] .339  | .417        | .460        |\n+----+------------------+--------------+--------------+-------------+-------------+\n| 11 | BERT-Cosine      | .312         | .335         | [BOLD] .440 | [BOLD] .484 |\n+----+------------------+--------------+--------------+-------------+-------------+",
        "label": "not enough info"
    },
    {
        "claim": "For a training set of 0.9M training examples, the proposed method reaches comparable classification performance to a BiLSTM approach.",
        "evidence": "It shows that, for a training set of 0.9M training examples, the proposed method reaches comparable classification performance to the BiLSTM approach..",
        "table": "+----+---------------+----------------------+-----------------+-------------------+\n|    | [EMPTY]       |   [BOLD] Punctuation |   [BOLD] BiLSTM |   [BOLD] Proposed |\n+====+===============+======================+=================+===================+\n|  0 | In-scope (F)  |                 0.66 |            0.88 |              0.85 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  1 | Out-scope (F) |                 0.87 |            0.97 |              0.97 |\n+----+---------------+----------------------+-----------------+-------------------+\n|  2 | PCS           |                 0.52 |            0.72 |              0.72 |\n+----+---------------+----------------------+-----------------+-------------------+",
        "label": "not enough info"
    },
    {
        "claim": "our evaluation F1-score (Macro) is 82.28%, which is slightly lower than those reported in [23] (87.5%).",
        "evidence": "we find that the proposed CNN-LSTMOur-neg-Ant model performs better than the baselines for both positive and negative sentiment.",
        "table": "+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|    | Classifier           | Positive Sentiment Precision   | Positive Sentiment Recall   | Positive Sentiment Fscore   |\n+====+======================+================================+=============================+=============================+\n|  0 | SVM-w/o neg.         | 0.57                           | 0.72                        | 0.64                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  1 | SVM-Punct. neg.      | 0.58                           | 0.70                        | 0.63                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  2 | SVM-our-neg.         | 0.58                           | 0.73                        | 0.65                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  3 | CNN                  | 0.63                           | 0.83                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  4 | CNN-LSTM             | 0.71                           | 0.72                        | 0.72                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  5 | CNN-LSTM-Our-neg-Ant | [BOLD] 0.78                    | [BOLD] 0.77                 | [BOLD] 0.78                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  6 | [EMPTY]              | Negative Sentiment             | Negative Sentiment          | Negative Sentiment          |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  7 | [EMPTY]              | Precision                      | Recall                      | Fscore                      |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  8 | SVM-w/o neg.         | 0.78                           | 0.86                        | 0.82                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n|  9 | SVM-Punct. neg.      | 0.78                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 10 | SVM-Our neg.         | 0.80                           | 0.87                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 11 | CNN                  | 0.88                           | 0.72                        | 0.79                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 12 | CNN-LSTM.            | 0.83                           | 0.83                        | 0.83                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 13 | CNN-LSTM-our-neg-Ant | [BOLD] 0.87                    | [BOLD] 0.87                 | [BOLD] 0.87                 |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 14 | [EMPTY]              | Train                          | [EMPTY]                     | Test                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 15 | Positive tweets      | 5121                           | [EMPTY]                     | 1320                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+\n| 16 | Negative tweets      | 9094                           | [EMPTY]                     | 2244                        |\n+----+----------------------+--------------------------------+-----------------------------+-----------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "Lemma-based targets do not significantly outperform type-based targets in terms of F-measure in all cases.",
        "evidence": "The type-based targets do not significantly outperform type-based targets in terms of F-measure in all cases. [CONTINUE] Lemmatized targets significantly outperform type-based targets in terms of F-measure in all cases. Lemma-based targets are significantly better than type-based targets on WN-N and VN for all extraction contexts. POS-based targets significantly outperform lemmatized targets on WN-V and VN for all extraction contexts.",
        "table": "+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|    | [EMPTY]      | WN-N P            | WN-N R            | WN-N F            | WN-V P            | WN-V R            | WN-V F            | VN P              | VN R              | VN F              |\n+====+==============+===================+===================+===================+===================+===================+===================+===================+===================+===================+\n|  0 | Context: w2  | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       | Context: w2       |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  1 | type         | .700              | .654              | .676              | .535              | .474              | .503              | .327              | .309              | .318              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  2 | x+POS        | .699              | .651              | .674              | .544              | .472              | .505              | .339              | .312              | .325              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  3 | lemma        | .706              | .660              | .682              | .576              | .520              | .547              | .384              | .360              | .371              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  4 | x+POS        | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  5 | Context: dep | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      | Context: dep      |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  6 | type         | .712              | .661              | .686              | .545              | .457              | .497              | .324              | .296              | .310              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  7 | x+POS        | .715              | .659              | .686              | .560              | .464              | .508              | .349              | .320              | .334              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  8 | lemma        | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591              | .512              | .548              | .408              | .371              | .388              |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|  9 | x+POS        | .722              | .666              | .693              | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> |\n+----+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+",
        "label": "refutes"
    },
    {
        "claim": "for example, for [cue:was] the 61.8% of the outcome categories are produced by instances whose premise begins with [cue:was].",
        "evidence": "In this section, we will analyze the output of our model on the various output categories during inference. We observe that about 61.8% of the outcomes categories are produced by instances whose premise begins with [cue:was]. The majority of the outcomes also follow a similar pattern, with other cues falling in between. For example, in the ‘In’ category, about 55.3% of the outcomes are produced by instances whose premise begins with [cue:was].",
        "table": "+----+-------+--------+---------+--------+\n|    | Cue   |   App. |   Prod. |   Cov. |\n+====+=======+========+=========+========+\n|  0 | in    |     47 |    55.3 |    9.4 |\n+----+-------+--------+---------+--------+\n|  1 | was   |     55 |    61.8 |   11   |\n+----+-------+--------+---------+--------+\n|  2 | to    |     82 |    40.2 |   16.4 |\n+----+-------+--------+---------+--------+\n|  3 | the   |     85 |    38.8 |   17   |\n+----+-------+--------+---------+--------+\n|  4 | a     |    106 |    57.5 |   21.2 |\n+----+-------+--------+---------+--------+",
        "label": "not enough info"
    },
    {
        "claim": "The hybrid model yields scores close to or even above the better model of the two on all tasks.",
        "evidence": "The results clearly indicate that the hybrid model is either comparable to or slightly better than the better model of the two methods on all tasks. This is not surprising, given that the words that are captured by word clusters in the CBOW and CMOW models are significantly longer than the words that can be captured by the H-MOW and CHiME-3 models, respectively. This is also true for all of the tasks, where the CBOW and CHiME-3 models are closer to each other than they are to the hybrid model.",
        "table": "+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|    | Dim   | Method    | Depth       | BShift      | SubjNum     | Tense       | CoordInv    | Length      | ObjNum      | TopConst    | SOMO        | WC          |\n+====+=======+===========+=============+=============+=============+=============+=============+=============+=============+=============+=============+=============+\n|  0 | 400   | CBOW/400  | 32.5        | 50.2        | 78.9        | 78.7        | 53.6        | 73.6        | 79.0        | 69.6        | 48.9        | 86.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  1 | 400   | CMOW/400  | [BOLD] 34.4 | 68.8        | 80.1        | [BOLD] 79.9 | [BOLD] 59.8 | 81.9        | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  2 | 400   | H-CBOW    | 31.2        | 50.2        | 77.2        | 78.8        | 52.6        | 77.5        | 76.1        | 66.1        | 49.2        | [BOLD] 87.2 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  3 | 400   | H-CMOW    | 32.3        | [BOLD] 70.8 | [BOLD] 81.3 | 76.0        | 59.6        | [BOLD] 82.3 | 77.4        | 70.0        | 50.2        | 38.2        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  4 | 784   | CBOW/784  | 33.0        | 49.6        | 79.3        | 78.4        | 53.6        | 74.5        | 78.6        | 72.0        | 49.6        | [BOLD] 89.5 |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  5 | 784   | CMOW/784  | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2        | [BOLD] 61.8 | 82.8        | [BOLD] 79.7 | 74.2        | [BOLD] 50.7 | 72.9        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  6 | 800   | Hybrid    | 35.0        | [BOLD] 70.8 | 81.7        | [BOLD] 81.0 | 59.4        | [BOLD] 84.4 | 79.0        | [BOLD] 74.3 | 49.3        | 87.6        |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  7 | -     | cmp. CBOW | +6.1%       | +42.7%      | +3%         | +3.3%       | +10.8%      | +13.3%      | +0.5%       | +3.2%       | -0.6%       | -2.1%       |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n|  8 | -     | cmp. CMOW | -0.3%       | +-0%        | -0.4%       | +1%         | -3.9%       | +1.9%       | -0.9%       | +0.1%       | -2.8%       | +20.9%      |\n+----+-------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+",
        "label": "supports"
    },
    {
        "claim": "[CONTINUE] MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap.",
        "evidence": "We can see that our τMIL-ND (model 2) does better than MIL (model 1) in all the measures. Particularly, our MIL (model 2) outperforms MIL (model 1) by about 1.5% in In-E+ and 3.3% in InSent+ F1. The MIL-ND superior performance on τM is statistically significant (p-value <0.01) in all the measures.",
        "table": "+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|    | System                     | All P        | All R        | All F1             | In  [ITALIC] E+ P   | In  [ITALIC] E+ R   | In  [ITALIC] E+ F1   |\n+====+============================+==============+==============+====================+=====================+=====================+======================+\n|  0 | Name matching              | 15.03        | 15.03        | 15.03              | 29.13               | 29.13               | 29.13                |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  1 | MIL (model 1)              | 35.87        | 35.87        | 35.87 ±0.72        | 69.38               | 69.38               | 69.38 ±1.29          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  2 | MIL-ND (model 2)           | 37.42        | [BOLD] 37.42 | 37.42 ±0.35        | 72.50               | [BOLD] 72.50        | [BOLD] 72.50 ±0.68   |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  3 | [ITALIC] τMIL-ND (model 2) | [BOLD] 38.91 | 36.73        | [BOLD] 37.78 ±0.26 | [BOLD] 73.19        | 71.15               | 72.16 ±0.48          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+\n|  4 | Supervised learning        | 42.90        | 42.90        | 42.90 ±0.59        | 83.12               | 83.12               | 83.12 ±1.15          |\n+----+----------------------------+--------------+--------------+--------------------+---------------------+---------------------+----------------------+",
        "label": "supports"
    },
    {
        "claim": "the results in Table 1 clearly show that the action number threshold, being the simplest, achieves the worst performance on almost all metrics.",
        "evidence": "The results in Table 1 show almost perfect correlation between the action number threshold and the slot number w/o and the slot number w/. [CONTINUE] Next, we evaluate the results of our multi-decoder network with the single-action baseline and the 10-action generation. For the single-action baseline, we simply use the threshold set to 1. The reason is that the higher the value of the action number threshold, the better the performance. [CONTINUE] HDSA has the worse performance, [CONTINUE] HDSA also outperforms our approach significantly for the 10-action generation.",
        "table": "+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|    | Model & Decoding Scheme    | Act # w/o               | Act # w/                | Slot # w/o              | Slot # w/               |\n+====+============================+=========================+=========================+=========================+=========================+\n|  0 | Single-Action Baselines    | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  1 | DAMD + greedy              | [BOLD] 1.00             | [BOLD] 1.00             | 1.95                    | [BOLD] 2.51             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  2 | HDSA + fixed threshold     | [BOLD] 1.00             | [BOLD] 1.00             | 2.07                    | [BOLD] 2.40             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  3 | 5-Action Generation        | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     | 5-Action Generation     |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  4 | DAMD + beam search         | 2.67                    | [BOLD] 2.87             | 3.36                    | [BOLD] 4.39             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  5 | DAMD + diverse beam search | 2.68                    | [BOLD] 2.88             | 3.41                    | [BOLD] 4.50             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  6 | DAMD + top-k sampling      | 3.08                    | [BOLD] 3.43             | 3.61                    | [BOLD] 4.91             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  7 | DAMD + top-p sampling      | 3.08                    | [BOLD] 3.40             | 3.79                    | [BOLD] 5.20             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  8 | HDSA + sampled threshold   | 1.32                    | [BOLD] 1.50             | 3.08                    | [BOLD] 3.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n|  9 | 10-Action Generation       | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    | 10-Action Generation    |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 10 | DAMD + beam search         | 3.06                    | [BOLD] 3.39             | 4.06                    | [BOLD] 5.29             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 11 | DAMD + diverse beam search | 3.05                    | [BOLD] 3.39             | 4.05                    | [BOLD] 5.31             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 12 | DAMD + top-k sampling      | 3.59                    | [BOLD] 4.12             | 4.21                    | [BOLD] 5.77             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 13 | DAMD + top-p sampling      | 3.53                    | [BOLD] 4.02             | 4.41                    | [BOLD] 6.17             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n| 14 | HDSA + sampled threshold   | 1.54                    | [BOLD] 1.83             | 3.42                    | [BOLD] 3.92             |\n+----+----------------------------+-------------------------+-------------------------+-------------------------+-------------------------+",
        "label": "not enough info"
    },
    {
        "claim": "the data augmentation strategy significantly improves the human evaluation performance (Wilcoxon signed-rank test, p-value",
        "evidence": "We also perform a Wilcoxon signed-rank test to test whether the improvements of our method over the baselines are statistically significant (p-value=0.05, t-test). The results show that the data augmentation strategy significantly improves the diversity of generated responses. With data augmentation, the pairwise BLEU score has been significantly improved on all three categories (good, OK, and Invalid).",
        "table": "+----+----------+-------------+-------------+--------------+--------------+--------------+\n|    | Model    | Diversity   | App         | Good%        | OK%          | Invalid%     |\n+====+==========+=============+=============+==============+==============+==============+\n|  0 | DAMD     | 3.12        | 2.50        | 56.5%        | [BOLD] 37.4% | 6.1%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  1 | DAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1%        | 9.9%         |\n+----+----------+-------------+-------------+--------------+--------------+--------------+\n|  2 | HDSA (+) | 2.14        | 2.47        | 57.5%        | 32.5%        | [BOLD] 10.0% |\n+----+----------+-------------+-------------+--------------+--------------+--------------+",
        "label": "not enough info"
    }
]